\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools}
\usepackage{graphicx}
\usepackage{geometry}
    \geometry{
        a4paper,
        left = 40mm,
        top = 20mm,
        right = 40mm,
        bottom = 30mm
    }
\setlength{\parindent}{0pt}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\title{Partial Differential Equations 540}
\author{Taught by: Dr. Peter Sternberg \\ Written by:Thanic Nur Samin}
\date{\vspace{-5ex}}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\begin{document}

\maketitle

Class 1: 01/08

\section*{Introduction}

Let \(u=u(x,t)\) where \(x=(x_1, x_2, x_3) \in \mathbb{R}^3\) (or any \(\mathbb{R} ^n\) ) and \(t\in [0,\infty)\) 

Here, \(x\) denotes position, \(t\) denotes time.

\(u\) denotes a kind of \underline{density}, which is \(\frac{\text{amount of stuff}}{\text{volume}}\) for some measure of stuff.

It is usually a good habit to keep track of the units.

Now, \(u(x,t)\) denotes density.

Let \(\Omega\) be an arbitrary subset in the domain.

Then, the amount of stuff in \(\Omega \) at time \(t\) is given by

\[
    \int _\Omega u(x,t) \, \mathrm{d} x
\]

And the rate of change of stuff in \(\Omega \) is given by

\[
    \frac{\mathrm{d}}{\mathrm{d}t} \int _\Omega u(x,t) \, \mathrm{d} x
\]

\[
    =\int_{\Omega }^{} \frac{\mathrm{d}}{\mathrm{d}t} u(x,t) \,\mathrm{d}x = \int_{\Omega }^{} u_t(x,t) \,\mathrm{d}x 
\]

We can look at the rate of change in other ways as well. It mainly depends on two things.

Firstly, it depends on the stuff that comes out of or goes in through the boundary of \(\Omega \), namely \(\partial \Omega \). This is the flux accross \(\partial \Omega \).

\underline{Flux accros \(\partial \Omega \) }:

[Insert the flux diagram here]

\(\vec{F} =(F^{(1)}(x_1, x_2, x_3, t),F^{(2)}, F^{(3)})\) 

\(\vec{F} :\mathbb{R} ^3 \times [0,\infty) \to \mathbb{R}^3\) 

This \(\vec{F} \) denotes the amount of stuff that leaves.

Let \(\vec{N} \) be the normal vector. \(\vec{F} \cdot \vec{N} >0 \iff \) stuff leaves \(\Omega \).

\(\vec{F} \cdot \vec{N} \) has the unit of \(\frac{\text{Amount}/\text{Area}}{\text{time}}\) 

This contributes,

\[
    -\int_{\partial \Omega }^{} \vec{F} \cdot \vec{N}  \,\mathrm{d}S = -\int_{\Omega }^{} \operatorname{div} \vec{F} \,\mathrm{d}x 
\]

Where we have used divergence theorem. Recall that, \(\displaystyle \operatorname{div} \vec{F} = \nabla \cdot \vec{F} = \frac{\partial F^{(1)}}{\partial x_1} + \frac{\partial F^{(2)}}{\partial x_2} + \frac{\partial F^{(3)}}{\partial x_3} \) 

Second contribution comes from sources and sinks inside \(\Omega \). We calculate that by \(f = f(x,t)\). This has unit \(\frac{\text{amount}/\text{volume} }{\text{time}}\). \(f(x,t)\) is scalar valued.

So, this contributes

\[
    \int_{\Omega }^{} f(x,t) \,\mathrm{d}x 
\]

This tells us,

\[
    \int_{\Omega }^{} u_t(x,t) \,\mathrm{d}x = - \int_{\Omega }^{} \operatorname{div} \vec{F} \,\mathrm{d}x + \int_{\Omega }^{} f(x,t) \,\mathrm{d}x 
\]

\[
    \implies \int_{\Omega }^{} (u_t + \operatorname{div} \vec{F} -f) \,\mathrm{d}x = 0
\]

Since this is true for arbitrary \(\Omega \), we have,

\[
    u_t + \operatorname{div} \vec{F} = f 
\]

Which is a differential equation. This is a very fundamental differential equation, since our `stuff' can be anything.

We see some examples:

\begin{itemize}
    \item Transport (Convection). Given velocity \(\vec{v} (x,t)\), \(u\) is transported (or ``convected'') by \(\vec{v} \). Then we have \(\vec{F} (x,t)=u(x,t)\vec{v} (x,t)\). This gives us the differential equation \(u_{t} +\operatorname{div}(u \vec{v}) = f \) where \(\vec{v} ,f\) are given. This is a first order PDE.
    \item Diffusion Process: Stuff moves from high concentration area to low concentration area. This gives us, \(\vec{F} = - k\nabla u=-k \mathcal{D} u\) where \(k>0\) is the diffusion coefficient, a constant. Recall that \(\nabla u = (u_{x_1}, u_{x_2}, u_{x_3})\). So we have the PDE \(u_{t} +\operatorname{div}(-k\nabla u) = f\). We can write this using the laplacian: \(u_{t} = k \Delta u + f\) or \(u_{t} = k\nabla^2 u\). This is the heat equation or the diffusion equation. If \(u\) is independent of time, in other words \(u\) is at equilibrium, then \(u_{t} =0\) and so we have the equation \(-k \Delta u=f\). This is called Poisson's equation. If \(f \equiv 0\) this implies \(\Delta u=0\) which is called Laplace's equation. Solutions of Laplace's equations are called harmonic.
    \item When \(\vec{F} \) depends only on \(u\), or in other words \(\vec{F} =\vec{F} (u)\) then we have the PDE \(u_{t} +\operatorname{div} \vec{F} (u) = f\). This is a conservation law. 
\end{itemize}

Note the use of the word order.

\begin{definition}
    Order of a PDE is the highest derivative in the PDE.
\end{definition}

\begin{definition}
    Laplacian: The laplacian of \(u\) is \(\Delta u\) or \(\nabla^2 u\) and it equals \(u_{x_1 x_1} + u_{x_2 x_2} + u_{x_3 x_3}\).
\end{definition}

Recall that gradiant of \(u\) is given by \(\nabla u = \left( \frac{\partial u}{\partial x_1} , \frac{\partial u}{\partial x_2} , \frac{\partial u}{\partial x_3}   \right) \) and so divergence of gradiant gives us the laplacian.

\hrule
\hfil

Class 2: 01/10

\section*{Multi-Index Notation}

\begin{definition}
    Multi Index: \(\alpha \) is an \(n\) multi-index if \(\alpha \) is an \(n\)-tuple of non-negative integers. So \(\alpha =(\alpha_1, \dots, \alpha_n )\).

    Now, given \(u:\mathbb{R} ^n\to \mathbb{R} \), given \(\alpha \), we define \(\vert \alpha  \vert = \displaystyle \sum_{j=1}^{n} \alpha _{j} \) 

    We finally define \(\mathcal{D} ^\alpha u\).

    \[
        \mathcal{D} ^\alpha u=\frac{\partial^{\vert \alpha  \vert } u}{\partial^{\alpha_1}x_1 \dots \partial^{\alpha _{n}}x_{n} } 
    \]

\end{definition}

Example: Suppose \(n=3\) and \(\alpha =(2,0,3)\).

Then \(\vert \alpha  \vert = 5\) and \(\mathcal{D} ^\alpha u=\dfrac{\partial ^5 u}{\partial ^2 x_1 \partial ^3 x_3} = u_{x_1 x_1 x_3 x_3 x_3}\) 

\section*{Linear/Nonlinear PDE}

\begin{definition}
    A PDE is \(N^{th}\) order linear if it can be written in the form:

    \[
        \sum_{\vert \alpha  \vert \leq N} a_\alpha (x)\mathcal{D} ^\alpha u = f(x)
    \]

    if \(f\equiv 0\) the PDE is homogeneous.

    Examples: \(\Delta u=0,\Delta u=f,u_{t} =k \Delta u, u_{tt} - c^2 \Delta u=f\) [wave equation].
    
\end{definition}

Otherwise, the PDE is non-linear. But there are ``degrees'' of nonlinearity.

\begin{definition}
    A PDE is \(N^{th} \) order semilinear if it can be written in the form

    \[
        \sum_{\vert \alpha  \vert = N} a_\alpha (x)\mathcal{D} ^\alpha u + a_0(x,u,\mathcal{D} \vec{u} \dots ,\mathcal{D} ^{N-1}u) = 0
    \]

\end{definition}

Examples: \(\Delta u=u^3,u_{t} =\Delta u+\vert \nabla u \vert ^2\) 

\begin{definition}
    A PDE is \(N^{th} \) order quasi-linear if it can be written in the form

    \[
        \sum_{\vert \alpha  \vert = N } a_\alpha (x,u,\mathcal{D} u, \dots , \mathcal{D} ^{N-1}u) \mathcal{D} ^\alpha u + a_0(x,u,\mathcal{D} u,\cdots,\mathcal{D} ^{N-1}u) = 0
    \]
\end{definition}

Example: The minimal surface equation.

\[
    \nabla \cdot \left( \frac{\nabla u}{\sqrt{1+\vert \nabla u \vert ^2} } \right) =0
\]

\[
    \sum_{i,j=1}^{n} \left( \frac{(1+\vert \nabla u \vert ^2) \delta_{ij} - u_{x_i}u_{x_j}}{(1+\vert \nabla u \vert ^2)^{\frac{3}{2}}} \right) u_{x_i x_j} = 0   
\]

\begin{definition}
    Otherwise the PDE is fully nonlinear. The order is the highest order derivative that appears.
\end{definition}

Examples: \(\vert \nabla u \vert = h(x)\) [Eikonal equation], \(\det(\mathcal{D} ^2 u) = h(x)\) where \(\mathcal{D} ^2 u\) is the Hessian Matrix [Monge-Ampere equation]

In 2d, this is \(\det \begin{pmatrix}
    u_{x_1 x_1} &  u_{x_1 x_2} \\
    u_{x_1 x_2} &  u_{x_2 x_2} \\
\end{pmatrix} = h\) 

\section*{Scalar PDE vs Systems}

Suppose \(u:\mathbb{R} ^n \to \mathbb{R} ^k\) and we have the PDE \(\Delta u = \nabla_u V(u)\) where \(V:\mathbb{R} ^k \to \mathbb{R} \) 

Here, \(\Delta u = (\Delta u_1, \dots ,\Delta u_k)\) 

Example: Navier-Stokes PDE

\[
    u_{t} +(u\cdot \nabla)u - \Delta u = \nabla p
\]

\[
    \operatorname{div}(u) = 0
\]

Here, \(u:\mathbb{R} ^3 \times [0,T)\to \mathbb{R} ^3\) and \((u\cdot\nabla)u = (u\cdot\nabla u_1, u\cdot\nabla u_2, u\cdot\nabla u_3)\) 

\section*{Solving the Transport Equation}

Recall the transport equation

\[
    u_{t} + \nabla \cdot (u \vec{v} ) = 0
\]

Where \(\vec{v} \) is the given velocity. We consider the special case where \(\vec{v} \) is constant.

Note that \(\nabla \cdot (u \vec{v} )= \nabla u \cdot \vec{v} + u\nabla \cdot \vec{v} \) . If \(\vec{v} = \vec{v_0}\) we have \(\nabla \cdot (u \vec{v} ) = \nabla u \cdot \vec{v_0} \) 

This gives us the equation of ``simple transport'', which is first order linear. We specify \(u(x,0)=g(x)\), so \(g:\mathbb{R} ^n \to \mathbb{R} \).

\underline{Idea: Rephrase PDE as a directional derivative, aka an ODE}

Remember that \(u=u(x,t)\)

So our equation is essentially \( (u_{x_1}, u_{x_2},\dots , u_{x_n}, u_t)\cdot (v_0^{(1)},\dots ,v_0^{(n)}, 1) = 0 \) 

So, rate of change along the diretion \((\vec{v_0} , 1)\) is \(0\).

Therefore, \(u\) is constant along lines in \(\mathbb{R} ^{n+1}\) with tangent \((\vec{v_0} ,1)\).

[Insert Picture if line here]

Parametric description of these lines:

For any \(x_0\in\mathbb{R} ^n\) we have, \((x,t)=\gamma_{x_0}(t)=(\vec{x_0} ,0) + t(\vec{v_0} ,1)\)  

From the rate of change argument, for \((x,t)\) in this line, \(u(x,t)=u(x_0,0)=g(x_0)\).

\(x=x_0 + tv_0\) which implies \(x_0=x-tv_0\). Therefore, the solution is \(u(x,t)=g(x-tv_0)\) 

We can check the solution using the chain rule.

\[
    \displaystyle  u_t = \sum_{j=1}^{n} g_{x_j}(\vec{x} -t \vec{v_0} )(-v_0^{(j)}) = - \vec{v_0} \cdot \nabla g(\vec{x} -t \vec{v_0} = - \vec{v_0} \cdot \nabla u(x,t))
\] 

\[
    \implies u_{t} + \vec{v_0} \cdot \nabla u = 0 
\]

This is the simplest of \underline{Method of Characteristics}, which is convering a PDE into a system of ODEs.

\hrule
\hfil

Class 3: 01/12

\section*{Laplace's Equation and Poisson's Equation}

\(\Delta u = u_{x_1 x_1}+\dots +u_{x_n x_n} = 0\)

\(\Delta u = f\) 

There are lots of harmonic functions.

\begin{itemize}
    \item Any linear function \(\sum_{i=1}^{n} c_{i} x_{i} \) 
    \item When \(n=2\), the Real and Imaginary parts of analytic functions. If \(f(z)\) is analytic, we can set \(f(z) = \Re f(z) + i \Im f(z) = u(x,y) + i v(x,y)\). Cauchy-Riemann equations give us \(u_{x} = v_{y} ,u_{y} =-v_{x} \) so by differentiating them we get \(u_{xx} = v_{xy}, u_{yy} = - v_{xy}\). Therefore \(\Delta v = 0\). Example: \(e^z=e^{x+iy} = e^x \cos y + i e^x \sin y \)
    \item We can use seperation of variables to find harmonic: \(u(x,y) = f(x) g(y)\), \(\Delta u = 0 \iff f^{\prime\prime} (x) g(y) + f(x) g^{\prime\prime} (y) =0 \) thus we have \(  \frac{f^{\prime\prime}(x)}{f(x)} = - \frac{g^{\prime\prime}(y)}{g(y)} \). Since this is a function of \(x\) and \(y\) seperately it must be constant, so we have \(  \frac{f^{\prime\prime}(x)}{f(x)} = - \frac{g^{\prime\prime}(y)}{g(y)} = \lambda\). If we have \(\lambda >0\) we see \(f(x) = e^{\pm \sqrt{\lambda } x}\) and \(g(y) = \cos(\sqrt{-\lambda} y)\) or \(\sin (\sqrt{-\lambda } y)\)
    \item Ansatz (what even is this): we can look for \underline{radial} solutions to Laplace's equation. Let \(x = (x_1, \dots, x_n )\), we look for \(u(x) = v(\vert x \vert )\) where \(r = \vert x \vert = \sqrt{x_1^2 + \dots + x_n^2}\) where \(v:\mathbb{R} ^+ \to \mathbb{R} \). We use chain rule. For fixed \(i\in [n]\) we have, \(u_{x_i} = v^{\prime} (r) r_{x_{i}} \implies u_{x_i x_i} = v^{\prime\prime} (r) (r_{x_i})^2 + v^{\prime} (r) r_{x_i x_i}\). Note that \(r_{x_i} = \frac{1}{2|x|}\cdot 2x_i = \frac{x_i}{\vert x \vert }\) and \(r_{x_i x_i} = \frac{1}{|x|} + x_i \frac{\partial }{\partial x_i} \left( \frac{1}{\vert x \vert } \right) = \frac{1}{\vert x \vert } + x_i \left( - \frac{1}{\vert x \vert ^2} \right) \frac{x_i}{|x|} = \frac{1}{\vert x \vert } - \frac{x_i^2}{\vert x \vert ^3} \). Therefore, \(\Delta u = \sum_{i=1}^{N} \left( v^{\prime\prime} (r) \frac{x_1^2}{|x|^2} + v^{\prime} (r)\left( \frac{1}{\vert x \vert }-\frac{x_i^2}{|x|^3} \right)  \right) = v^{\prime\prime} (r) + v^{\prime} (r) \left( \frac{n}{\vert x \vert } - \frac{1}{\vert x \vert } \right)  \). Therefore, \(\Delta u = v^{\prime\prime} (r) + \frac{n-1}{r}v^{\prime} (r)\). If we try to solve \(v^{\prime\prime} +\frac{n-1}{r} v^{\prime} = 0\), we can take \(w=v^{\prime} \) to get \(w^{\prime} + \frac{n-1}{r}w = 0\) which is a first order linear ODE. We can seperate and get \(\int \frac{w^{\prime}}{w} = \int \frac{1-n}{r} \implies \ln w = (1-n) \ln r + C \implies w = C r^{1-n} \) so \(v^{\prime} = C r^{1-n} \implies v = C r^{2-n} \) when \(n \geq 3\), when \(n=2\) we have \(v = C \ln r\). Note that while these are solutions, we have a singularity at \(r=0\). So, they are harmonic everywhere except the origin where they are undefined.    
\end{itemize}

Note that solutions to Laplace's equation can give us more solutions to Poisson's equation, since if \(\Delta u_1 = f, \Delta u_2 = 0 \implies \Delta (u_1 + u_2) = f\) as well.

\section*{Well-Posed Problem}

A list of requirement, first phrased by Hadamard. There are lots of `bad' problems that are good.

\begin{itemize}
    \item There exists a solution [Existence]
    \item There cannot be \(2\) solutions [Uniqueness]
    \item Continuous dependence on data [we need some other condition to not have infinitely many solutions. Continuous dependence means when we shift some data then the result also shifts, and it happens continuously]
\end{itemize}

For \(\Delta u = 0\) or \(\Delta = f\), we can fix a domain \(\Omega \subset \mathbb{R} ^n\) where \(\Omega \) is bounded, open, connected (if disconnected we can look at as different problems)

[Insert Picture of \(\Omega \), a blob ]

Consider boundary conditions, certain conditions on the boundary.

\begin{itemize}
    \item \(u = g\) on \(\partial \Omega \) for some given \(g:\partial \Omega \to \mathbb{R} \). These are called [Dirichlet boundary conditions]
    \item \(\Delta u\cdot v = g\), \(g:\partial \Omega \to \mathbb{R}\). So some directional derivative is given. These are called [Neumann boundary conditions]. We're basically fixing the flux or something similar.
\end{itemize}

We can have many others.

\begin{theorem}
    Assume \(u \in \mathcal{C}^2(\Omega)\cap \mathcal{C}^1(\overline{\Omega} )\) 
\end{theorem}. Then there exists at most one solution to \(\Delta u = f\) in \(\Omega\) and \(u=g\) on \(\partial \Omega \) 

\begin{proof}   
    Suppose \(u_1,u_2\) are solutions to the problem. Let \(v \coloneqq u_1 - u_2\). We want to show that \(v\equiv 0\).

    Note that, \(\Delta v = \Delta u_1 - \Delta u_2 = f-f = 0\) in \(\Omega\)

    On \(\partial \Omega : v = g - g = 0\).
    
    So we have,
    
    \[
        \int_{\Omega}^{} \operatorname{div}(v \nabla v)  \,\mathrm{d}x 
    \] 

    \[
        \int_{\Omega }^{} \nabla v\cdot \nabla v \,\mathrm{d}x + \int_{\Omega }^{} v \Delta v \,\mathrm{d}x  
    \]

    So, by divergence theorem.

    \[
        \int_{\Omega }^{} \nabla v\cdot \nabla v + v \Delta v dx\ \,\mathrm{d}x = \int_{\partial \Omega }^{} v \nabla v \cdot v \,\mathrm{d}S  
    \]
    
    Thus,
    
    \[
        \int_{\Omega }^{\infty} \vert \nabla v \vert ^2 \,\mathrm{d}x = 0
    \]

    Which gives us \(v \equiv 0\) 

\end{proof} 

\hrule
\hfil

Class 04: 01/17

Focus on the thirteen for HW problem 1*

\begin{theorem}

    Uniqueness for Neumann Boundary Condition (B.C.)

    \(\Delta u=f\) in \(\Omega \subset \mathbb{R} ^n\)  

    \(\nabla u \cdot \upsilon =g\) on \(\partial \Omega \)  

    \(\Omega\) bounded \(\partial \Omega \) smooth

    \(f:\Omega \to \mathbb{R} ,g:\partial \Omega \to \mathbb{R} \) given.

    Given any 2 solutions \(u_1\) and \(u_2\) in \(\mathcal{C} ^2(\Omega )\cap \mathcal{C} ^1(\overline{\Omega } )\)   then \(u_1 - u_2\) is a constant.

\end{theorem}

\begin{proof}
    Same as for Dirichlet condition. Let \(v=U_2 - u_1\). Then \(\Delta v=0\) in \(\Omega \) and \(\nabla v\cdot \upsilon=0\)
    
    Then, again,

    \[
        \int _\Omega v \Delta v = 0 =-\int_\Omega |\nabla v|^2 + \int _\Omega \operatorname{div}(v \nabla v) =-\int _\Omega |\nabla v|^2+\int _{\partial \Omega } v\nabla v\cdot \upsilon dS
    \]

    Which gives us \(\nabla v=0 \) 

\end{proof}

This method of proof is called the energy method.

\section*{Volume, Surface Measure}

\(B(0,1)=\left\{ x\in\mathbb{R} ^n:\vert x \vert <1 \right\} \) 

\(S^{n-1} = \left( x\in\mathbb{R} ^n:\vert x \vert =1 \right) \) 

Suppose we want to calculate the volume of \(B(0,R)\)

This is \(\displaystyle \int_{\left\{ x:\vert x \vert <R \right\} }^{} 1  \,\mathrm{d}x  \)

Set \(y=\frac{x}{R}\) 

So the integral is

\[
    R^n \int_{\left\{ y:\vert y \vert <1 \right\} }^{} 1 \,\mathrm{d}y = R^n \vert B(0,1) \vert  
\]

Since \(R^n dy=dx \)

We define:

\(\alpha(n)\coloneqq \vert B(0,1) \vert \) volume of unit ball.

\(\omega (n)\coloneqq \vert S^{n-1}  \vert \) 

\(\vert \partial B(0,R) \vert =R^{n-1} \partial B(0,1)=\omega (n)R^{n-1} \) 

We have,

\[
    \alpha (n)=\int_{B(0,1)}^{} 1 \,\mathrm{d}x =\int_{0}^{1} \int_{\left\{ x:\vert x \vert =r \right\} }^{}  \,\mathrm{d}S  \,\mathrm{d}r 
\]

\[
    =\int_{0}^{1} \omega (n) r^{n-1}  \,\mathrm{d}r 
\]

So, \(\alpha (n)=\omega (n)\dfrac{r^n}{n}\bigg\vert^1_0\) 

So, \(n \alpha (n)=\omega (n)\) 

Recall: ``Radial Laplacian''

For \(u=u(r), r=|x|\) 

\(\Delta u=u^{\prime\prime} (r)+\frac{n-1}{r}u^{\prime} (r)\) 

We found:

in 2d, \(u=C\ln r\)

In nd, \(u=\frac{C}{r^{n-2} }\) 

There is something wrong in the sense that, they are harmonic, which means they solve Laplace Equation \(\Delta u=0\) but they have a singularity at \(0\)

\begin{definition}
    Fundamental Solution to Laplace's Equation: (One of the goals this week is to convince this theorem earns it's name, it's really fundamental)

    \[
        \Phi(x) = \begin{dcases}
            -\frac{1}{2\pi}\ln \vert x \vert , &\text{ if } n=2 ;\\
            \frac{1}{n(n-2)\alpha (n)}\frac{1}{\vert x \vert ^{n-2}}, &\text{ if } n\geq 3 ;
        \end{dcases}
    \]

\end{definition}

We look at \(3\) limits. We integrate these solutions around singularity, and shrink them.

Given \(g\) is smooth, and assuming \(n\geq 3\) 

\[
    \lim_{\epsilon  \to 0} \left\vert \int_{B(x_0,\epsilon)}^{} \Phi(y - x_0)g(y) \,\mathrm{d}y \right\vert
\]

\[
    =\lim_{\epsilon  \to 0} \left\vert \int_{B(x_0,\epsilon )}^{} \frac{1}{\omega (n)(n-2)}\frac{1}{\vert y - x_0 \vert } \,\mathrm{d}y \right\vert 
\]

\[
    =\lim_{\epsilon \to 0} \left\vert \frac{1}{\omega (n)(n-2)} \int_{0}^{\epsilon} \int_{\partial B(x_0,r)}^{} \frac{1}{\vert y - x_0 \vert ^{n-2}}g(y) \,\mathrm{d}S_y  \,\mathrm{d}r \right\vert
\]

\[
    \leq \frac{\max \vert g \vert}{\omega (n)(n-2)} \lim_{\epsilon  \to 0} \int_{0}^{\epsilon } \frac{1}{\epsilon ^{n-2}} \omega_n r^{n-1}  \,\mathrm{d}r 
\]

\[
    \leq C \lim_{\epsilon  \to 0} \int_{0}^{\epsilon } r \,\mathrm{d}r = 0
\]

Second limit:

\[
    \lim_{\epsilon  \to 0} \left\vert \int_{\partial B(x_0,\eta )}^{} \Phi(y-x_0)g(y) \,\mathrm{d}y  \right\vert  
\]

\[
    \lim_{\epsilon  \to 0} \max \vert g \vert \int_{\partial B(x_0,\epsilon )}^{} \frac{1}{\omega (n)(n-2)}\frac{1}{r^{n-2}} \,\mathrm{d}S 
\]

\[
    \leq \lim_{\epsilon \to 0} C \frac{1}{\epsilon ^{n-2}} \omega _n \epsilon^{n-1}
\]

Third limit:

\[
    \lim_{\epsilon  \to 0} \int_{\partial B(x_0,\epsilon )}^{} g(y)\nabla\Phi(y-x_0)\cdot \upsilon  \,\mathrm{d}S 
\]

We have, \(\Phi(y-x_0)=\frac{1}{\omega (n)(n-2)}\frac{1}{\vert y-x_0 \vert ^{n-2}}\) 

So, \(\nabla\Phi(y-x_0)\cdot\upsilon = \frac{\partial }{\partial r} \left( \frac{1}{\omega (n)(n-2)} r^{2-n}  \right) \) where \(r=\vert y-x_0 \vert \) 

\(=-\frac{1}{\omega (n)}r^{1-n}\) 

So, the initial limit is equal to

\[
    \lim_{\epsilon  \to 0} \int_{\partial B(x_0,\epsilon )}^{} g(y) \epsilon ^{1-n} \,\mathrm{d}S = -\lim_{\epsilon  \to 0} \frac{1}{\omega (n)\epsilon ^{n-1}}\int_{\partial B(x_0,\epsilon )}^{} g(y) \,\mathrm{d}S_y  
\]

\(=-g(x_0)\) 

The reason we get \(-g(x_0)\) (Just a neat negative sign, nothing weird!) was because of our choice of constants.

\hfil
\hrule

Class 05: 01/19

Fundamental solutions to Poisson:

\[
    \Phi(x) = \begin{dcases}
        -\frac{1}{2\pi}\ln \vert x \vert , &\text{ if }n=2  ;\\
        \frac{1}{\omega_n(n-2)}\frac{1}{\vert x \vert ^{n-2}}, &\text{ if }  n\geq 3;\\
    \end{dcases}
\]

Today we introduce \underline{Green's Identity} which is the equivalent of Integration By Parts.

\begin{theorem}
    Divergence Theorem:
    \[
        \int_{\Omega }^{} \nabla \cdot \vec{F} dx \,\mathrm{d}x = \int_{\partial \Omega }^{} \vec{F} \cdot \upsilon  \,\mathrm{d}S 
    \]

    Where \(\vec{F} : \Omega \subset \mathbb{R} ^n \to \mathbb{R} ^n\) 

\end{theorem}

Now, take \(u,v:\overline{\Omega } \to \mathbb{R} \) 

Then,

\[
    \int_{\Omega }^{} \operatorname{div}(u \nabla v) \,\mathrm{d}x = \int_{\Omega}^{} \left[u \Delta v + \nabla u \cdot \nabla v\right]  \,\mathrm{d}x = \int_{\Omega }^{} u \nabla v \cdot \upsilon \,\mathrm{d}S 
\]

\[
    \int_{\Omega }^{} \operatorname{div}(v \nabla u) \,\mathrm{d}x = \int_{\Omega}^{} \left[ v \Delta u + \nabla u \cdot \nabla v \right] \,\mathrm{d}x = \int_{\Omega }^{} v \nabla u \cdot \upsilon \,\mathrm{d}S 
\]

Subtracting,

\[
    \int_{\Omega }^{} (u \Delta v - v \Delta u) \,\mathrm{d}x = \int_{\Omega }^{} (u \nabla v \cdot \upsilon - v \nabla u \cdot \upsilon ) \,\mathrm{d}S  
\]

\section*{Newtonian Potential}

Given \(f:\mathbb{R} ^n \to \mathbb{R} \) consider for \(x,y\in \mathbb{R} ^n\), \(\Phi(x-y)f(y)\).

For \(y\) fixed, when \(x \neq y\) this is a function of \(f\) and it is harmonic.

Question: What if we integrate over all \(y\)? This is kind of like taking the sum of a lot of harmonic functions.

\begin{proposition}
    Assume \(f\in C^2_C (\mathbb{R} ^n)\) [twice continuously differentiable and compactly supported].

    Define \(w:\mathbb{R} ^n \to \mathbb{R} \) via

    \[
        w(x) \coloneqq \int_{\mathbb{R} ^n}^{} \Phi(x-y)f(y) \,\mathrm{d}y 
    \]

    We call \(w\) the ``Newtonian Potential of \(f\)''

    Then, \(-\Delta w(x) = f(x)\) 

\end{proposition}

This is a way of building, lets say, building a solution of the heat equation.

Note that, \(w +\) any harmonic function solves \(- \Delta w = f\) 

\begin{proof}
    First of all, we need to take derivatives of \(w\). So, since we are feeling reckless today,

    \[
        \Delta w = \int_{\mathbb{R} ^n}^{} \Delta _x \Phi(x-y) f(y) \,\mathrm{d}y = \int_{\mathbb{R} ^n}^{} 0\cdot f(y) \,\mathrm{d}y = 0 
    \]

    This is nonsense. Problem is, we have a singularity of \(\Phi\) at \(0\) which makes it so that we can't take the derivative inside the integral.

    Note that, the given integral is a convolution.

    \[
        \int_{\mathbb{R}^n}^{} \Phi(x-y)f(y) \,\mathrm{d}y = \Phi * f = f * \Phi 
    \]

    Then we rewrite \(w\) as:

    \[
        w(x) = \int_{\mathbb{R} ^n}^{} \Phi(y) f(x-y) \,\mathrm{d}y 
    \]

    Claim: It's legal to differentiate (twice) under the integral.

    Fix \(i\in \left\{ 1,\cdots, n \right\} \) and let \(\vec{e_i}\) be standard basis vector. Then,
    
    \[
        \frac{w(x+\vec{he_i})-w(x)}{h} = \int_{\mathbb{R}^n}^{} \Phi(y) \left[ \frac{f(x+\vec{he_i} -y)-f(x-y)}{h} \right]  \,\mathrm{d}y 
    \]

    We want to take the limit \(h\to 0\). Look at the stuff inside [] seperately. Applying Mean Value Theorem:
    
    \[
        \frac{f(x+\vec{he_i} -y)-f(x-y)}{h} = f_{x_i}(x+\tilde{h}\vec{e_i} -y) \text{ for some \(\tilde{h}\in (0,h)\)} 
    \]

    Since \(f\) and the first derivative are compactly supported, \(f_{x_i}\) is continuous on a compact set and hence uniformly continuous. Which means, \(\forall \epsilon >0, \exists \delta \) so that \(\forall z_1,z_2 \in \) compact set, then \(\vert f_{x_i}(z_1) - f_{x_i} (z_2)      \vert < \epsilon \) provided \(\vert z_1 - z_2 \vert < \delta \) 

    Then, \(\vert f_{x_i}(x+\tilde{h}\vec{e_i} -y) - f_{x_i}(x-y) \vert < \epsilon \) 

    Then,

    \[
        \left\vert \frac{w(x+\vec{he_i} )-w(x)}{h} - \int_{\mathbb{R} ^n}^{} \Phi(y)f_{x_i}(x-y) \,\mathrm{d}y \right\vert
    \]

    \[
        \leq\int_{\mathbb{R}^n}^{} \vert \Phi(y) \vert \vert f_{x_i}(x+\tilde{h}\vec{e_i} -y)-f_{x_i}(x-y) \vert  \,\mathrm{d}y 
    \]

    \[
        \leq \epsilon \int_{B(0,R_x)}^{} \vert \Phi(y) \vert  \,\mathrm{d}y  \text{ Provided \(\vert h \vert < \delta\) for some radius \(R_x > 0\) } 
    \]

    Note that,

    \[
        \int_{B(0,R_x)}^{} \vert \Phi(y) \vert  \,\mathrm{d}y = C \int_{0}^{R_x} \int_{\partial B(0,R_x)} \frac{1}{r^{n-2} }\omega _n r^{n-1} \,\mathrm{d}S \, \mathrm{d}r
    \]

    \[
        \simeq C R_x^2 < C_1
    \]

    From the fact that \(f\) is compactly supported.

    So, original integral is \(\leq C_1 \epsilon \) 

    So we can swap integral and derivative.

    Same argument works for second derivatives.

    So,

    \[
        w_{x_i x_i}(x) = \int_{\mathbb{R} ^n}^{} \Phi(y) f_{x_i x_i} (x-y) \,\mathrm{d}y 
    \]  


    \[
        \sum_{i=1}^{N} w_{x_i x_i} = \Delta w = \int_{\mathbb{R} ^n}^{} \Phi(y) \Delta f(x-y) \,\mathrm{d}y 
    \]

    \[
        = \int_{B(0,\epsilon )}^{} \Phi(y) \Delta f(x-y) \,\mathrm{d}x + \int_{\mathbb{R} ^n \setminus B(0,\eta)}^{} \Phi(y) \Delta f(x-y) \,\mathrm{d}x 
    \]

    \[
        = I_1^{\epsilon } + I_2^{\epsilon} 
    \]

    We have, \(I_1^{\epsilon} \to 0\) as \(\epsilon \to 0\) from wednesday lecture.

    For \(I_2\) we use Green's Identity.

    \[
        I_2^\epsilon = \int_{ \Omega_x \left\{ y:x-y\in \text{support of \(f\)} \right\} \setminus B(0,\epsilon) }^{\infty} \Phi(y) \Delta f(x-y)  \,\mathrm{d}y 
    \]

    \[
        =\int_{\Omega_x \setminus B(0,\eta)}^{} f(x-y)\Delta \Phi(y) \,\mathrm{d}y - \int_{\partial \Omega_x}^{} \left[ f(x-y)\nabla \Phi \cdot \upsilon - \Phi \nabla f(x-y)\cdot \upsilon\right] \,\mathrm{d}S
    \]

    \[
        - \int_{\partial B(0,\epsilon)}^{} \left[ f(x-y)\nabla\Phi\cdot \upsilon - \Phi(y)\nabla f(x-y)\cdot \upsilon \right] \,\mathrm{d}S 
    \]

    First one cancels because \(\Phi\) is harmonic.

    In the second integral, the terms cancel.

    In the third integral, note that as \(\epsilon \to 0\) the first term goes to \(-f(x)\). To see this, note that the inner normal becomes negative the outer normal.
    
    And the second term approaches \(0\) as \(\epsilon \to 0\)  

\end{proof}

\hfil
\hrule

Class 06: 01/22

Last time:

If \(f\in C^2_c(\mathbb{R}^n)\) [twice continuously differentiable and compactly supported] then \(w(x)=\int_{\mathbb{R} ^n}^{} \Phi(x-y)f(y) \,\mathrm{d}y \) solves \(-\Delta w=f\) in \(\mathbb{R}^n\) where \(\Phi\) is one of the fundamental solution to Laplace's equation.

In fact, though it's hard to prove, we only need \(f\) to be H\"older Continuous.

\begin{definition}
    \(f\) is H\"older continuous on a domain \(\Omega \subset \mathbb{R} ^n\) with exponent \(\alpha \in (0,1)\) if \(f\) is continuous and

    \[
        \sup _{ x,y\in \Omega, x\neq y } \frac{\vert f(x) - f(y) \vert }{\vert x-y \vert^\alpha } < \infty
    \]
\end{definition}

This means \(f\in C^{0,\alpha}\) 

Example: \(f:\mathbb{R} \to \mathbb{R}\) , \(f(x)=\sqrt{\vert x \vert } \) 

This is not differentiable, not even Lipschitz continuous, but is holder continuous with \(\alpha = \frac{1}{2}\) 

We say \(f\in C^{k,\alpha}\) if \(f\in C^k\) and \(D^\beta f\in C^{0,\alpha}\) for all multi index \(\beta \) so that \(\vert \beta  \vert = k\) 

We can also have \(\alpha =1\) but we just say \(f\) is Lipschitz then.

\begin{theorem}
    If \(f\in C^{0,\alpha}(\overline{\Omega} )\) then \(\forall \Omega ^{\prime} \subset \subset \Omega \) [This means \(\overline{\Omega ^{\prime} } \subset \Omega \) ] we have \(w\in C^{2,\alpha }(\Omega ^{\prime} )\) and \(-\Delta w = f\) in \(\Omega\).
\end{theorem}

We are not going to prove this. But, \(f\) continuous doesn't work. This theorem requires several pages of analysis.

Read Gilberg-Trudinger for more.

Note that this is a solution with no boundary conditions specified, but we can add any harmonic function to fit boundary conditions.

\section*{Properties of Harmonic Functions}

Harmonic functions are very special functions.

\begin{theorem}
    Let's take an open set \(\Omega \subset \mathbb{R} ^n\). Then a function \(u\in C^2(\Omega )\) is harmonic if and only if \(\forall x\in \Omega ,r>0\) such that \(B(x,r) \subset \Omega\) then \(u(x)=\dashint_{\partial B(x,r)} u(y)\,\mathrm{d} y\) 
\end{theorem}

This is called the mean value property (MVP).

Note that \(\dashint_{\partial B(x,r)}=\frac{1}{\omega _n r^{n-1}}\int_{\partial B(x,r)}\) 

And \(\dashint_{B(x,r)}=\frac{1}{\alpha (n) r^n}\int_{B(x,r)}\) 

We also have \(n \alpha (n)=\omega (n)\) 

\begin{proof}

    Fix \(x\in \Omega\). Then, \(\forall r>0\) so that \(B(x,r)\subset \Omega\) we define
    
    \[
        \phi(r)\coloneqq \frac{1}{\omega (n)r^{n-1} }\int_{\partial B(x,r)}^{} u(y) \,\mathrm{d}S_y 
    \]

    \(S_y\) means we're integrating on the surface of the sphere w.r.t. \(y\). \(S_y\) is the infinitesimal surface area.

    Note, we want to prove that \(\phi\) is constant.

    So, we want \(\phi^{\prime} =0\) 

    So we change limits to get \(r\) out of there.

    Define \(y=x+rz\) where \(r\in \partial B(0,1)\) 

    Then \(\mathrm{d} S_y = r^{n-1} \mathrm{d} S_z\)
    
    \[
        \phi (r)=\frac{1}{\omega (n)}\int_{\partial B(0,1)}^{} u(x+rz) \,\mathrm{d}S_z
    \] 

    \[
        \phi^{\prime} (r) = \frac{1}{\omega (n)} \int_{\partial B(0,1)}^{} \nabla_y u(x+rz)\cdot z \,\mathrm{d}S_z 
    \]

    Back to \(y\):
    
    \[
        \phi^{\prime} (r)=\frac{1}{\omega (n)r^{n-1}}\int_{\partial B(x,r)}^{} \nabla_y u(y)\cdot \frac{y-x}{r} \,\mathrm{d}S_y 
    \]

    Note that \(n=\frac{y-x}{r}\) is the unit normal.

    So we can apply divergence theorem.

    \[
        \phi^{\prime} (r) = \frac{1}{\omega(n)r^{n-1}}\int_{B(x,r)}^{} \nabla\cdot\nabla_y u(y) \,\mathrm{d}y  
    \]

    \[
        \frac{1}{\omega(n) r^{n-1}}\int_{B(x,r)}^{} \Delta u(y) \,\mathrm{d}y 
    \]

    Now, if \(\Delta u \equiv 0\) then \(\phi^{\prime} \equiv 0\) and let \(r\to 0\) to see that \(u(x)=\) the average.

    For the other direction, if \(u\) is not harmonic at \(x\neq 0\) then \(\Delta u(y)>0\) or the other sign for some ball centered at \(x\) and thus \(\phi\) is not constant in that ball. Thus MVP fails.

\end{proof}

\section*{Maximum/Minimum Principle}

In assignment we proved maximum principle. Let \(u\in C^2(\Omega ) \cap C^0(\\overline{ \Omega })\) be harmonic in \(\Omega \) for some \(\Omega \subset \mathbb{R} ^n\) , bounded and connected.

Then, \(\max_{\overline{\Omega} }u(x)=\max _{\partial \Omega} u(x)\) and the same from minimum as well [from homework]

Further, if \(\exists x_0\in \Omega \) so that \(u(x_0)=\max_{\overline{\Omega} }u(x)\) [or minimum] then \(u(x)\) is constant. This is the STRONG maximum/minimum principle.

\begin{proof}
    Let \(M\) be the max. Let \(\Omega_1 \coloneqq \left\{ x\in \Omega : u(x)=M \right\} \) 

    Either \(\Omega _1\) is empty or it is \(\Omega\) 

    Topology we want to prove \(\Omega _1\) is both open and closed relative to \(\Omega\) 

    1: \(\Omega _1\) is relatively closed in \(\Omega\) : if \(\left\{ x_j \right\} \in \Omega \) and \(x_j \to \overline{x} \in \Omega\) then \(\overline{x} \in \Omega _1\)
    
    Why? \(\lim_{j \to \infty} u(x_j)=M \implies u(\overline{x} )=M \) 

    2: \(\Omega_1\) is open: Let \(\overline{x} \in \Omega_1\). We claim that there is a ball \(B(\overline{x},r)\) around \(\overline{x}\) which is in \(\Omega_1\)

    Proof: Suppose not. Then for every \(x \in B(x,r) \)

    we have \(u(x)\leq u(\overline{x})\) 

    Now, using MVP,

    \[
        0 = \int_{\partial B(\overline{x} , r)}^{} (u(x)-u(\overline{x} ))  
    \]

    Which we can't do unles \(u(x)\equiv u(\overline{x} )\) 

\end{proof}

\hfil
\hrule

Class 07: 01/24
 
Last time we talked about the maximum principle for harmonic function and also talked about mean value property.

Strong maximum principle implies either function is constant or the max happens in the boundary. Note that we work on each connected component, if \(f\) is \(1\) on \(A\) and \(2\) on \(B\) that isn't constant but harmonic and this is stupid

(Another) Uniqueness for Dirichlet Boundary Condition:

Assume \(\Delta u=f\) in \(\Omega \) 

Also \(u = g\) in \(\partial \Omega \) 

Where \(\Omega \subset \mathbb{R} ^n\), bounded, open.

Then there is at most one solution \(u\in C^2(\Omega )\cap C(\overline{\Omega} )\) 

We have aleady seen one solution using the Energy Method: Hey there's two solutions, consider their difference.

If \(u_1,u_2\) are both solutions, let \(v\coloneqq u_1 - u_2 \) then \(\Delta v = 0\) and \(v=0\) on \(\partial \Omega\). Note we can now just use maximum and minimum principle to deduce that \(v\equiv 0\) 

\(\max_{\overline{\Omega} }v = \max _{\partial \Omega }v = 0\) 

Same for min so \(v\) is identically \(0\) 

Also remember about `Well Posed Problem' in PDE. One condition is `a solution exists' which we didn't establish, another `it is unique' which we established, and finally `continuous dependence'. The last can be established using maximum principle.

\begin{proposition}
    Suppose for \(j=1,2\) we have \(\Delta u_j = 0\) in \(\Omega \) and \(u_j = g_j\) on \(\partial \Omega\)  where \(g_1,g_2\) are given functions. Then,

    \[
        \max_{\overline{\Omega } }\vert u_1 - u_2 \vert = \max _{\partial \Omega}\vert g_1 - g_2 \vert 
    \]

    This gives us continuous dependence.

\end{proposition}

\begin{proof}
    Again let \(v = u_1 - u_2\) 

    Then \(\max _{\overline{\Omega }} v=\max_{\partial \Omega} (g_1 - g_2)\), same for min.
    
    This gives us the answer.

\end{proof}

Detour: Let \(A_{ij}(x)\) be a matrix depdending on \(x\) that is positive definite. Then \(\displaystyle \sum_{1\leq i,j\leq n} a_{ij}(x) u_{x_i x_j} \) is called `elliptic'. We have maximum/minimum principle for this too.

Last time we saw: for \(u\in C^2\), we had

\(u\text{ harmonic} \iff u\text{ satisfies the MVP [Mean Value Property]}  \) 

Today we are going to erase the \(C^2\). Assume \(u\) is continuous in some domain \(\Omega \) [\(u\in C(\Omega)\) ] and \(\Omega\) satisfies MVP for every \(B(x,r) \subset \Omega\). Then,

i: \(u\) is harmonic

ii: \(u\) is infinitely differentiable [\(u\in C^{\infty} (\Omega) \) ] 

\begin{proof}
    We will make use of mollifiers.

    Mollifiers are used for analysis in general when you have lousy function and you want to approximate it by something nicer.

    Use a mollification of \(u\). We use the symmetric mollifier:

    Let \(\eta : [0,\infty) \to [0,\infty)\) such that: \(\eta \in C^2(\infty)\) and \(\eta(r)\equiv 0\) for any \(r > 1\). Furthermore, we have \(\int_{\mathbb{R}^n}^{} \eta (\vert x \vert ) \,\mathrm{d}x  = 1\).
    
    Let for any \( \epsilon > 0\) :

    \(\eta_{\epsilon} (x)=\epsilon^{-n}\eta \left( \frac{\vert x \vert}{\epsilon} \right) \) 

    Note that:

    \(\int_{\mathbb{R} ^n}^{} \eta _\epsilon (x) \,\mathrm{d}x = \epsilon ^{-n} \int_{\mathbb{R} ^n}^{} \eta \left( \frac{\vert x \vert}{\epsilon} \right)  \,\mathrm{d}x \) 

    Let \(y=\frac{x}{\epsilon}\) to see this integral become \(\int_{\mathbb{R} ^n}^{} \eta (\vert y \vert ) \,\mathrm{d}y = 1\) 

    For the proof: Define:

    \[
        u_{\epsilon} (x) = \int_{\Omega}^{} \eta _\epsilon (x-y)u(y) \,\mathrm{d}y 
    \]

    For \(x\in \Omega_{\epsilon}=\{ x:\operatorname{dist}(x,\partial \Omega) > \epsilon  \}  \) 

    Note that, \(u_{\epsilon} \in C^{\infty} (\Omega _\epsilon)\) 

    Prove this similar to yesterday.

    See similar facts in Appendix C. Today we just use the previous thing.

    We assume the MVP.

    Fix \(\epsilon\) and \(x\in \Omega_{\epsilon} \) 

    \(\displaystyle u_{\epsilon} (x)=\int_{\Omega}^{} \eta _\epsilon (x-y)u(y) \,\mathrm{d}y \) 

    \(\displaystyle =\epsilon^{-n}\int_{\Omega}^{} \eta \left( \frac{\vert x-y \vert }{\epsilon} \right) u(y) \,\mathrm{d}y \) 

    \(\displaystyle = \epsilon ^{-n} \int_{B(x,\epsilon)}^{} \eta \left( \frac{\vert x-y \vert }{\epsilon} \right) u(y) \,\mathrm{d}y  \) 

    \(\displaystyle = \epsilon ^{-n} \int_{0}^{\epsilon} \int_{\partial B(x,r)}^{} \eta \left( \frac{r}{\epsilon} \right) u(y) \,\mathrm{d}S_y  \,\mathrm{d}r \) 

    \(\displaystyle = \epsilon ^{-n} \int_{0}^{\epsilon} \eta \left( \frac{r}{\epsilon} \right)   \int_{y\in \partial B(x,r)}^{} u(y) \,\mathrm{d}S_y \,\mathrm{d}r \) 

    We can use Mean Value Property

    \(\displaystyle = \epsilon ^{-n} \int_{0}^{\epsilon} \eta \left( \frac{r}{\epsilon} \right)   u(x)\omega(n)r^{n-1} \,\mathrm{d}r \) 


    Set \(s=\frac{r}{\epsilon}\) 
    
    that gives us
    
    \(\displaystyle = u(x) \int_{B(0,1)}^{} \eta (\vert x \vert ) \,\mathrm{d}x \) 

    \(=u(x)\) 

    Note that, \(u_{\epsilon}(x)=u(x) \) when \(\epsilon \) is small enough that gives us the regularity class. 

\end{proof}

\begin{proposition}
    Derivative Estimates for Harmonic Functions:

    Let \(u\) be harmonic on an open set \(\Omega \subset \mathbb{R} ^n\) then for every \(n\) and every non-negative integer \(k\) there exists \(C_{n,k}\) such that if \(B(x,r) \subset \Omega\) then

    \(\displaystyle \vert D^\alpha u(x) \vert \leq \frac{C_{n,k}}{r^{n+k} } \int_{B(x,r)}^{} \vert u(y) \vert  \,\mathrm{d}y \) for every mutli-index \(\alpha \) such that \(\vert \alpha  \vert = k\) 

\end{proposition}

To be proven on friday.

One can use these estimates to show that \(u\) harmonic implies \(u\) is analytic, meaning for every \(x_0\), in some neighborhood the multivariable taylor series converges.

\[
    u(x) = \sum \frac{D^\alpha u(x_0)(x-x_0)^\alpha}{\alpha !}
\]

for \(\vert x - x_0 \vert \) small.

Another application to be shown on friday.

One can also use this to prove Liouville's Theorem: if \(u:\mathbb{R}^n \to \mathbb{R} \) is harmonic and bounded then it must be constant.

\hfil
\hrule

Class 08: 01/26

\begin{theorem}
    Suppose \(u\) is harmonic in \(\Omega \subset \mathbb{R} ^n\), open. Then there exists constants \(C_{k,n}\) such that \(\forall B(x_0,r) \subset \Omega \) we have:
    
    \[
        \vert D^\alpha u(x_0) \vert \leq \frac{C_{k,n}}{r^{n+k}} \vert \vert u \vert  \vert _{L^1(B(x_0,r))}
    \]

    For all multi-index \(\alpha \) such that \(\vert \alpha  \vert = k\)

    Notation: for \(p<\infty \) we have \(\vert \vert u \vert  \vert _{L^p(S)} = (\int _S \vert u \vert ^p)^\frac{1}{p}\)
    
    And \(\vert \vert u \vert  \vert _{L^{\infty} (S)} = \sup _{x\in S}\vert u(x) \vert \) 

\end{theorem}

This theorem is powerful because it gives us information about any order derivative just from \(u\). This allows us to prove that harmonic functions are analytic by majorizing it with a converging infinite series.

\begin{proof}
    
    We use induction.

    Let's start with \(k=0\), aka we don't take any derivative.

    Take a ball centered at \(x_0\) with radius \(r\) inside \(\Omega \) and use MVP.

    \[
        u(x_0)=\frac{1}{\alpha (n)r^n}\int_{B(x_0,r)}^{} u(x) \,\mathrm{d}x 
    \]

    Note that instead of taking MVP over surface we take it over whole ball.

    Thus,

    \[
        \vert u(x_0) \vert \leq \frac{1}{\alpha(n)r^n} \vert \vert u \vert  \vert _{L^1(B(x_0,r))}
    \]

    Now suppose \(k=1\).

    This is a common technique. If a function is regular enough, we can take the derivative of the whole PDE!

    Recall that \(u\in C^{\infty}\). This is because if we mollify it we're supposed to get something better but we just get the function itself.

    Differentiate Laplace's Equation w.r.t. \(x_i,i\in \{ 1,2,\cdots,n \} \) 

    That gives us: \(\Delta (u_{x_i})=0\) 

    Thus, derivative of harmonic function is harmonic. We can take the MVP to \(u_{x_i}\) on \(B(x_0,\frac{r}{2})\) 

    \[
        u_{x_i}(x_0)=\frac{1}{\alpha (n)(\frac{r}{2})^n} \int_{B(x_0,\frac{r}{2})}^{} u_{x_i}(x) \,\mathrm{d}x = \frac{2^n}{\alpha (n)r^n} \int_{B(x_0,\frac{r}{2})}^{} u_{x_i}(x) \,\mathrm{d}x
    \]

    Unwritten rule of PDE: we always integrate by parts at least once. We use Divergence theorem.

    \[
        =\frac{2^n}{\alpha (n)r^n} \int_{B(x_0,\frac{r}{2})}^{} \operatorname{div}(0,\cdots,u,\cdots,0) \,\mathrm{d}x 
    \]

    \[
        =\frac{2^n}{\alpha(n)r^n} \int_{\partial B(x_0,\frac{r}{2})}^{} u \upsilon^{(i)} \,\mathrm{d}S 
    \]

    \[
        \implies \vert u_{x_i}(x_0) \vert \leq \frac{2^n}{\alpha(n)r^n} \vert \vert u \vert  \vert _{L^{\infty}(\partial B(x_0,\frac{r}{2}))}\cdot n \alpha (n)(\frac{r}{2})^{n-1} 
    \]

    \[
        =\frac{2n}{r} \vert \vert u \vert  \vert _{L^{\infty}(\partial B(x_0,\frac{r}{2}))}
    \]

    Draw ball of r/2 in ball of r picture here for better understanding.

    \[
        \leq \frac{2n}{r} \frac{1}{\alpha(n)(\frac{r}{2})^n} \vert \vert u \vert  \vert_{L^1(B(\cdot,\frac{r}{2}))}
    \]

    \[
        \leq \frac{2^{n+1}}{r^{n+1} } \frac{n}{\alpha (n)} \vert \vert u \vert  \vert _{L^1(B(x_0,r))}
    \]

    we can continue by induction.

\end{proof}

Note that, in this way, we can get:

\[
    C_{k,n} = \frac{(nk 2^{n+1})^k}{\alpha (n)}
\]

\begin{theorem}
    Liouville's Theorem:

    Let \(u\) be harmonic on \(\mathbb{R}^n\) [Entire solution of Laplace's Equation] and suppose \(u\) is bounded. Then \(u\) is constant. 

\end{theorem}

\begin{proof}
    Take \(x_0\in\mathbb{R} ^n\) and let \(r>0\) be any positive number.

    \[
        \vert u_{x_i}(x_0) \vert \leq \frac{C_{1,n}}{r^{n+1}} \int_{B(x_0,r)}^{} \vert u(x) \vert  \,\mathrm{d}x \leq \frac{C_{1,n}}{r^{n+1}} M \alpha (n) r^n = \frac{D}{r}
    \] 

    Since the solution is entire, it is true for every \(r\) so any partial derivative is \(0\) at any point.

\end{proof}

\begin{proposition}
    Suppose \(u\) is a \underline{bounded} solution to \(- \Delta u=f\) in \(\mathbb{R} ^n\) for \(f\in C_c^2(\mathbb{R}^n)\) and \(n\geq 3\). Then,

    \[
        u(x)=\int_{\mathbb{R}^n}^{} \Phi(x-y)f(y) \,\mathrm{d}y  + C
    \]

    Notation: \(u\in C^2\) means \(u, u_{x_i}, u_{x_i x_j}\) are continuous.\(u \in C_c^2\) means \(u\) is \(C_2\) and compactly supported.
    
    Note the `bounded', so \(C\) comes from Liouvlille's theorem.

\end{proposition}

\begin{proof}
    Let \(\tilde{u}\coloneqq \int_{\mathbb{R}^n}^{} \Phi(x-y)f(y) \,\mathrm{d}y \)
    
    Then \(u-\tilde{u}\) is harmonic.

    \(u\) is bounded.

    Recall: \(\vert \Phi(x) \vert \leq \frac{C}{\vert x \vert ^{n-2}}\) for \(n \geq 3\)   

    \[
        \tilde{u}(x)=\int_{y\in B(x,1)}^{} \Phi(x-y)f(y) \,\mathrm{d}y + \int_{y\notin B(x,1)}^{} \Phi(x-y)f(y) \,\mathrm{d}y = I_1 + I_2
    \]

    Now, \(I_2 \leq  C \int_{y\in B(x,1)}^{} f(y) \,\mathrm{d}y <\infty \) since \(f\) has compact support.
    
    Also, \(\vert I_1 \vert \leq \vert f \vert _{L^{\infty}} \int_{B(x,1)}^{} \Phi(x-y) \,\mathrm{d}y = \vert f \vert _{L^{\infty}} \int_{0}^{1} \int_{\partial B(x,r)}^{} \frac{C}{r^{n-2} } \,\mathrm{d}S  \,\mathrm{d}r \)
    
    \(= C \vert f \vert_{L^{\infty}} \int_{0}^{1} \frac{\omega(n)r^{n-1}}{r^{n-2}} \,\mathrm{d}r < \infty  \) 

    So \(\tilde{u}\) is bounded as well. So we can use Liouville to deduce that \(u-\tilde{u}\) is harmonic and entire and thus constant.
    
\end{proof}

\begin{theorem}
    Harnack Inequality: [These exist for all sorts of elliptic PDEs] Take an open connected set \(\Omega \subset \mathbb{R} ^n\) and take \(u:\Omega \to \mathbb{R} ^n\) be harmonic and \underline{non-negative}. Then, for every subdomain \(\Omega ^{\prime} \subset \subset \Omega\) [sits compactly within \(\Omega \)] there exists a constant \(C(n,\Omega ,\Omega ^{\prime})\) such that:

    \[
        \sup_{\Omega ^{\prime} } u \leq C \inf _{\Omega ^{\prime} } u
    \]

    Crucially, it doesn't depend on \(u\). This is useful for proving reguarity etc. It's useful for homework!

\end{theorem}

\hfil
\hrule

Class 09: 01/29

Review:

\[
    \Phi(x)=\begin{dcases}
        -\frac{1}{2\pi }\ln \vert x \vert , &\text{ if } n=2 ;\\
        \frac{1}{n(n-2)\alpha (n)}\frac{1}{\vert x \vert ^{n-2}}, &\text{ if } n \geq 3 ;
    \end{dcases}
\]

Suppose \(h\) is continuous. We have three limits:

\begin{enumerate}
    \item \(\displaystyle \lim_{\epsilon  \to 0} \int_{B(x,\epsilon )}^{} \Phi(x-y)h(y) \,\mathrm{d}y = 0\) 
    \item \(\displaystyle \lim_{\epsilon \to 0} \int_{\partial B(x,\epsilon )}^{} \Phi(x-y)h(y) \,\mathrm{d}y = 0\)
    \item \(\displaystyle \lim_{\epsilon  \to 0} \int_{\partial B(x,\epsilon )}^{} h(y)\nabla_y \Phi(x-y)\cdot \upsilon _y  \,\mathrm{d}S = -h(x)\)  
\end{enumerate}

\begin{theorem}
    Harnack Inequality: [These exist for all sorts of elliptic PDEs] Take an open connected set \(\Omega \subset \mathbb{R} ^n\) and take \(u:\Omega \to \mathbb{R} ^n\) be harmonic and \underline{non-negative}. Then, for every subdomain \(\Omega ^{\prime} \subset \subset \Omega\) [sits compactly within \(\Omega \)] there exists a constant \(C(n,\Omega ,\Omega ^{\prime})\) such that:

    \[
        \sup_{\Omega ^{\prime} } u \leq C \inf _{\Omega ^{\prime} } u
    \]

    Crucially, it doesn't depend on \(u\). This is useful for proving reguarity etc. It's useful for homework!

\end{theorem}

\begin{proof}

    Note that, this is equivalent to: for any \(y_1,y_2\in \Omega ^{\prime} \) , we can find \(C\) so that \(u(y_1) \leq C u(y_2)\) 

    Case 1: Let's suppose for some \(x_0 \in \Omega\) and some \(r > 0\) so that \(B(x_0, 4r) \subset \Omega\) and let \(y_1, y_2 \in B(x_0,r)\) 

    Using the fact that \(u\) is harmonic: by Mean Value Property:

    \[
        u(y_1) = \frac{1}{\alpha (n)r ^ n} \int_{B(y_1,r)}^{} u(y) \,\mathrm{d}y
    \]

    Use the non-negativity of \(u\) 

    \[
        u(y_1) \leq \frac{1}{\alpha (n) r ^ n} \int_{B(x_0, 2r)}^{} u(y) \,\mathrm{d}y 
    \]

    The ball of radius \(2r\) from \(y_1\) contains the ball of radius \(r\) from \(y_2\) and in tern the ball of radius \(3r\) from \(y_2\) contains everything.

    \[
        u(y_1) \leq \frac{3^n}{\alpha (n) (3r)^n} \int_{B(y_2, 3r)}^{} u(y) \,\mathrm{d}y = 3^n u(y_2)
    \]

    The last equality is due to the MVP.

    Therefore, for any \( y_1,y_2\in B(x_0,r)=\Omega ^{\prime} \) so that \(B(x_0,4r)\in \Omega\), we have 

    \[
        u(y_1) \leq 3^n u(y_2)
    \]

    We thus have Harnack's inequality.

    Case 2: Consider general \(\Omega ^{\prime} \subset \subset \Omega \) and pick \(r\) such that \(\operatorname{dist}(\Omega ^{\prime} , \partial \Omega ) > 4r\). Consider \(y_1,y_2\in \Omega ^{\prime} \).

    [insert picture]

    Connect \(y_1,y_2\) with a path in \(\Omega ^{\prime} \), we can cover this path with at most \(m\) balls of radius \(r\) depending on \(n,\Omega ,\Omega ^{\prime} \) 

    In the path from \(y_1\) to \(y_2\) take \(z_1,z_2,\cdots z_{m-1}\) so that they are in the intersections of the balls.

    Then we get \(u(y_1) \leq 3^n u(z_1) \leq 3^{2n} u(z_2) \leq \dots \leq 3^{nm} u(y_2)  \) by case 1.

    So \(u(y_1) \leq C u(y_2)\) where \(C\) depends only on \(n, \Omega , \Omega^{\prime} \) 

\end{proof}

Harnack is used on 1 or 2 homework problems.

\section*{Green's Functions for Laplace's Equation}

Recall: If \(u\) and \(v\) are smooth [all you need is \(C^2\) on some nice bounded domain \(\Omega \) and \(C^1\) on the boundary \(C^2(\Omega)\cap C^1(\overline{\Omega} )\) ] on some \(\Omega \subset \mathbb{R} ^n\) :

\[
    \int_{\Omega}^{} (u \Delta v - v \Delta u) \,\mathrm{d}y = \int_{\partial \Omega }^{} (u\nabla v \cdot \upsilon - v \nabla u \cdot \upsilon) \,\mathrm{d}S 
\]

Let \(\Omega _\eta \coloneqq \Omega \setminus B(x,\epsilon)\) for some \(x\in \Omega \) [a punctured domain] and we take \(v(y) = \Phi (x-y)\). Then,

\[
    \int_{\Omega _\epsilon }^{} (u \Delta _y \Phi(x-y) - \Phi(x-y) \Delta u(y)) \,\mathrm{d}y 
\]

\[
    =\int_{\partial \Omega }^{} (u \nabla_y \Phi(x-y)\cdot \upsilon _y - \Phi(x-y)\nabla_y u\cdot \upsilon_y) \,\mathrm{d}S
\]

\[
    + \int_{\partial B(x,\epsilon) }^{} (u \nabla_y \Phi(x-y)\cdot \upsilon _y - \Phi(x-y)\nabla_y u\cdot \upsilon_y) \,\mathrm{d}S 
\]

In the first integral, \(\Delta_y \Phi(x-y) = 0\) since it is harmonic.

If we let \(\epsilon \to 0\) we have:

\[
    -\int_{\Omega }^{} \Phi(x-y)\Delta u(y) \,\mathrm{d}y = \int_{\partial \Omega}^{} u \nabla_y \Phi \cdot \upsilon_y - \Phi \nabla u_y \cdot \upsilon_y \,\mathrm{d}S + u(x)
\]

We can rewrite everything and get:

\[
    u(x) = - \int_{\Omega}^{} \Phi(x-y) \Delta u(y) \,\mathrm{d}y 
\]

\[
    + \int_{\partial \Omega }^{} \left( \Phi(x-y) \nabla u_y \cdot \upsilon - u(y) \nabla_y \Phi(x-y)\cdot \upsilon _y \right)  \,\mathrm{d}S 
\]

Suppose we seek a solution to:

\[
    - \Delta u = f \text{ in } \Omega 
\]

\[
    u = g \text{ in } \partial \Omega 
\]

Then, if we have the solution, the following must be true:

\[
    u(x) = \int_{\Omega}^{} \Phi(x-y)f(y) \,\mathrm{d}y 
\]

\[
    + \int_{\partial \Omega }^{} \left( \Phi(x-y)\nabla u(y)\cdot \upsilon - g(y)\nabla_y \Phi(x-y)\cdot \upsilon_y \right)  \,\mathrm{d}S 
\]

This is a solution, but we don't know \(\nabla u\cdot \upsilon\) on \(\partial \Omega\) 

Strategy to fix this problem: Add to \(\Phi(x-y)\) a function \(v(x,y)\). The function \(v\) is called the `corrector'.

We want:

\begin{itemize}
    \item For each \(x\in \Omega, \Delta_y v(x,y)=0\)
    \item For each \(x\in \Omega \) and \(y\in \partial \Omega \) , \(\Phi(x-y)+v(x,y)=0\) 
\end{itemize}

Repeat the previous calculation with \(\Phi(x-y)\) replaced by \(G(x,y)\) given by \(G(x-y)=\Phi(x-y)+v(x,y)\) 

This gives us:

\[
    u(x)=\int_{\Omega}^{} G(x,y)f(y) \,\mathrm{d}y - \int_{\partial \Omega }^{} g(y)\nabla_y G(x,y)\cdot \upsilon _y \,\mathrm{d}S 
\]

\(G\) is called the \underline{Green's Function}.

Note that finding \(v\) is just solving the Laplace's Equation with a boundary condition. 

Thus, to solve Poisson with a Boundary condition we need to solve Laplace with a boundary Condition

\hfil
\hrule

Class 10: 01/31

Review:

1. We had \underline{Green's Identity:}

\[
    \int_\Omega u_1 \Delta u_2 - u_2 \Delta u_1 \, \mathrm{d} y = \int_{\partial \Omega } u_1 \nabla u_2 \cdot \upsilon - u_2 \nabla u_1 \cdot \upsilon \, \mathrm{d} S_y
\]

2. Also: suppose \(-\Delta u = f\) in \(\Omega \)

\(u = g\) on \(\partial \Omega \) 

Take \(u_1 = u, u_2 = \Phi(x-y)\) 

Work with \(\lim_{\epsilon  \to 0} \int_{\Omega \setminus B(x,\epsilon )}^{}  \) 

\[
    \implies u(x) = \int_{\Omega}^{} \Phi(x-y)f(y) \,\mathrm{d}y + \int_{\partial \Omega}^{} \Phi(x-y)\nabla u(y)\cdot \upsilon - g(y)\nabla_y \Phi(x-y)\cdot \upsilon  \,\mathrm{d}S_y 
\]

3. Replace \(\Phi(x-y)\) by \(G(x,y)=\Phi(x-y)+v(x,y)\) 

Where \(v\) is smooth for all \(x,y\in \Omega \) 

\(\nabla_y v = 0\) 

\(\Phi(x-y)+v(x,y)=0\) when \(x\in \Omega ,y\in \partial \Omega\) 

\[
    \implies u(x)=\int_{\Omega}^{} G(x,y)f(y) \,\mathrm{d}y - \int_{\partial \Omega }^{} g(y)\nabla_y G(x,y)\cdot \upsilon _y \,\mathrm{d}S 
\]

\begin{proposition}
    If we can find a Green's Function \(G\) then we have \(G(x,y)=G(y,x)\) for all \(x,y\in \Omega \) [see Evans.]
\end{proposition}

\underline{Green's Function for the Upper Half Space}, \(\mathbb{R} ^n_+\) 

Consider \(y\in \partial \mathbb{R} ^n_+\) and fix \(x\in\mathbb{R} _n^+\). How do we find \(v\)?

Consider the line \(x-y\). Take a version of the fundamental solution that has singularity in the reflection of \(x\) through the boundry, \(x^r\). Then we have \(\vert x-y \vert = \vert x^r - y \vert \) 

Take \(v(x,y)=-\Phi(x^r - y)\)

And thus \(G(x,y)=\Phi(x-y) - \Phi(x^r - y)\) 

To calculate the solution, we need \(-\nabla_y G(x,y)\cdot \upsilon _y\) 

Note, in \(\mathbb{R} ^n_+\) the outer normal derivative is \(-\frac{\partial}{\partial y_n} \) and so negative that is just \(\frac{\partial }{\partial y_n} \) and so we have:

\[
    \frac{\partial G(x,y)}{\partial y_n} \bigg|_{y_n=0}
\]

Let \(n \geq 3\) then we have: \(G(x,y)=\) 

\[
    \frac{1}{n(n-2)\alpha (n)}\left\{ \frac{1}{[(x_1 - y_1)^2 + \dots + (x_n - y_n)^2]^{\frac{n-2}{2}}} - \frac{1}{[(x_1 - y_1)^2 + \dots + (x_n + y_n)^2]^{\frac{n-2}{2}}} \right\} 
\]

Taking partial derivative with respect to \(y_n\) and setting \(y_n = 0\) we get:

\[
    \frac{1}{n(n-2)\alpha (n)}\frac{2-n}{2}\left\{ \frac{-2(x_n - y_n)}{[\dots]^{\frac{n}{2}}} - \frac{2(x_n + y_n)}{[\dots]^{\frac{n}{2}}} \right\} 
\]

\[
    =\frac{2}{n \alpha (n)} \frac{x_n}{\vert x - y \vert ^n} = K(x,y)
\]

It's called the Poisson Kernel for \(\mathbb{R}^n_+\). Also true for \(n=2\).

\begin{theorem}
    Assume \(g\) is continuous and bounded on \(\partial \mathbb{R} ^n _+\).
    
    Let \(u(x)=\int_{\partial \mathbb{R} ^n_+}^{} K(x,y)g(y) \,\mathrm{d}y^{\prime}  \) where \(\mathrm{d} y^{\prime} =\mathrm{d} y_1 \dots \mathrm{d} y_{n-1} \) 

    For \(x\in\mathbb{R} ^n_+\). Then,

    \begin{enumerate}
        \item \(C \in C^{\infty} (\mathbb{R} ^n_+)\cap L^{\infty} (\mathbb{R} ^n_+)\) 
        \item \(\Delta u = 0\) in \(\mathbb{R} ^n_+\)
        \item \(\lim_{x \to x_0, x\in \mathbb{R} ^n_+} = g(x_0) \) for all \(x_0\in \partial \mathbb{R} ^n_+\)   
    \end{enumerate}

\end{theorem}

Proof is pretty simple but it hinges on some \underline{key properties} of \(K\).

\begin{enumerate}
    \item \(\forall y\in \partial \mathbb{R} _+^n, \forall x\in \mathbb{R} ^n\) \(K\) is \(C^{\infty}\). However \(K\) is not smooth for \(x,y\in\mathbb{R} ^n_+\) 
    \item \(\int_{\partial \mathbb{R} ^n_+}^{} K(x,y) \,\mathrm{d}y^{\prime} = 1\) for all \(x\in\mathbb{R} ^n_+\)
    \item\(\forall y\in \partial \mathbb{R} ^n_+,\forall x\in\mathbb{R} ^n_+, \Delta_x K(x,y)=0\) 

\end{enumerate}

Suppose \(n=2\) then,

\[
    K[x,y]=\frac{1}{\pi }\int_{-\infty}^{\infty} \frac{x_2}{[(x_1 - y_1)^2 + x_2^2]} \,\mathrm{d}y_1 
\]

\[
    =\frac{1}{\pi }\frac{1}{x_2}\int_{-\infty}^{\infty} \frac{1}{(\frac{x_1 - y_1}{x_2})^2 + 1} \,\mathrm{d}y_1 
\]

Let \(z = \frac{x_1 - y_1}{x_2} \) then \(\mathrm{d} z = =\frac{1}{x_2}\,\mathrm{d}y_1 \)

So the integral is:

\[
    \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{1}{z^2 + 1 } \,\mathrm{d}x = 1
\]

For property 3: we show that \(\Delta _x K(x,y)=0\). We can either compute directly or use the fact that \(G(x,y)=G(y,x)\)

For all \(x\) we have \(y \mapsto G(x,y)\) is harmonic for \(x \neq y\) which means \(x \mapsto G(x,y)\) is harmonic for \(x \neq y\) which means \(x \mapsto \frac{\partial }{\partial y_n} G(x,y)\) is harmonic for \(x\neq y\) and thus \(K\) is harmonic since it is derivative at \(y_n=0\) 

\hfil
\hrule

Class 11: 02/02

\begin{theorem}
    Assume \(g : \partial \mathbb{R} ^n_+ \to \mathbb{R} \) is continuous and bounded. Let \(u(x)\coloneqq \int_{\partial \mathbb{R} ^n_+}^{} K(x,y)g(y) \,\mathrm{d}y^{\prime}  \) where \(\mathrm{d} y^{\prime} = \mathrm{d} y_1 \dots \mathrm{d} y_{n-1}\) and \(K(x,y)=\frac{2}{n \alpha (n)} \frac{x_n}{\vert x-y \vert ^n}\). Then,

    \begin{enumerate}
        \item \(u \in C^{\infty}(\mathbb{R} _+^n)\cap L^{\infty}(\mathbb{R} ^n_+)\) 
        \item \(\Delta u = 0\) in \(\mathbb{R} ^n_+\) 
        \item \(\lim_{x \to x_0,x\in \mathbb{R}^n_+} u(x)=g(x_0)\) for all \(x_0\in \partial \mathbb{R} ^n_+\) 
    \end{enumerate}

\end{theorem}

Properties of the Poisson Kernel \(K\) :

\(\forall y\in \partial \mathbb{R}^n_+,\forall x\in \mathbb{R} ^n_+\) we have \(K\) is \(C^{\infty}\) 

\(\forall x\in \mathbb{R} ^n_+, \int_{\partial \mathbb{R} ^n_+}^{} K(x,y) \,\mathrm{d}y^{\prime} =1 \) 

\(\forall y\in \partial \mathbb{R} ^n_+,\forall x\in \mathbb{R} ^n_+\) we have \(\Delta _x K(x,y)=0\) 

[insert picture of graph of \(K\) for fixed \(x\)]

For fixed \(x\) as \(y\) gets bigger \(K\) decays. Peak is when \(x\) is close to \(y\).

\begin{proof}
    Part 1: we need infinitely differentiable and bounded.

    For bounded:

    \(\vert u(x) \vert \leq \int_{\partial \mathbb{R}^n_+}^{} K(x,y)\vert g(y) \vert  \,\mathrm{d}y \) 

    \(\leq \vert g \vert _{L^{\infty}} \int_{\partial \mathbb{R} ^n_+}^{} K(x,y) \,\mathrm{d}y \) 

    \(= \vert g \vert _{L^{\infty}}\) 

    This is kind of a maximal prinicple.

    Then we need differentiable. This good because \(K\) decays when \(x\) is far from \(y\) 

    Fix \(x\) 

    \(\vert K(x,y) \vert \leq \frac{C}{1 + \vert y \vert ^n}\) for large \(y\).
    
    Now, \(\int_{\mathbb{R} ^{n-1}}^{} \frac{1}{1+\vert y \vert ^n} \,\mathrm{d}y = \int_{0}^{\infty} \int_{\partial B(0,r)}^{} \frac{1}{1+r^n} \,\mathrm{d}S  \,\mathrm{d}r \) 

    Note that the balls are in \(\mathbb{R}^{n-1}\) 

    \(=\int_{0}^{\infty} \frac{1}{1+r^n} \omega (n)r^{n-2} \,\mathrm{d}r \) 

    Outside of a set this is like integrating \(\frac{1}{r^2}\) from \(1\) to \(\infty\) 

    Basically, the integral is nicely convergent.

    Thus, it is legal to differentiate \(u\) under the integral sign.

    For 2: Valid using the fact that \(\Delta _x K(x,y)=0\) 

    So, \(\Delta u = \int_{\partial \mathbb{R} ^n_+}^{} \Delta _x K(x,y)g(y) \,\mathrm{d}y = 0 \) 

    The hard part is 3.

    This is where we use the continuity of \(g\). Let \(\epsilon > 0\) so there's some \(\delta \) so that \(\vert g(y) - g(x_0) \vert < \epsilon\) if \(\vert y - x_0 \vert < \delta\) 

    For \(x\) in the north consider \(u(x)-g(x_0)\). This equals:

    \[
        u(x)-g(x_0)=\int_{\partial \mathbb{R} ^n_+}^{} K(x,y)g(y) \,\mathrm{d}y^{\prime} -g(x_0) 
    \]

    \[
        u(x)-g(x_0)=\int_{\partial \mathbb{R} ^n_+}^{} K(x,y)[g(y)-g(x_0)] \,\mathrm{d}y^{\prime}
    \]

    \[
        \vert u(x) - g(x_0) \vert \leq \int_{\{ y : \vert y - x_0 \vert < \delta \} }^{} K(x,y) \vert g(y) - g(x_0) \vert \,\mathrm{d}y^{\prime}
    \]
        
    \[
        + \int_{\{ y : \vert y - x_0 \vert \geq \delta \} }^{} K(x,y) \vert g(y) - g(x_0) \vert \,\mathrm{d}y^{\prime} = I_1 + I_2 
    \]

    Now, \(I_1 \leq \epsilon \int K \leq \epsilon\) 

    \(I_2 \leq 2 \vert g \vert _{L^{\infty}} \int_{ \{ y : \vert y - x_0 \vert \geq \delta \}} \, K(x,y) \mathrm{d} y^{\prime}  \) 

    For \(\vert x - y_0 \vert < \delta / 2\),

    [draw pic of ball of radius \(\delta / 2\) around \(x_0\) and \(x\) lives somewhere in the ball. Draw a \(\delta\) ball around \(x_0\) then \(y\) lies outside that ball. So, \(\vert x - y \vert \geq \frac{1}{2} \vert x_0 - y \vert \) ]

    \(\vert y - x_0 \vert \leq \vert y - x \vert + \vert x - x_0 \vert \leq \vert y - x \vert + \delta / 2 \leq \vert y - x \vert + \frac{1}{2} \vert y - x_0 \vert \) 

    So,

    \[
        I_2 \leq 2 \vert g \vert _{L^{\infty}} \frac{2}{n \alpha (n)} x_n \int_{\{ y : \vert y - x_0 \vert \geq \delta  \} }^{} \frac{1}{\vert x - y \vert^n} \,\mathrm{d}y^{\prime}
    \] 

    \[
        \leq \frac{2 \vert g \vert _{L^{\infty}}}{n \alpha (n)}\cdot 2^n x_n \int_{\{ \vert y-x_0 \vert \geq d \} }^{} \frac{1}{\vert y - x_0 \vert ^n} \,\mathrm{d}y^{\prime} 
    \]

    \[
        \leq C x_n \int_{\delta}^{\infty} \int_{\partial B(x_0,r)}^{} \frac{1}{r^n} \,\mathrm{d}S  \,\mathrm{d}r 
    \]

    \[
        =\tilde C x_n \cdot \frac{1}{\delta}
    \]

    Given \(\epsilon \) the \(\delta \) is fixed. So, we can send \(x_n\) to \(0\) and see that \(I_2\) goes to \(0\).

\end{proof}

That solves Poisson over upper half space.

Now we talk about Green's Function for a ball in \(\mathbb{R}^n\) 

We have Green's Identity:

\[
    u(x) = -\int_{B(0,R)}^{} \Phi(x-y) \Delta u(y) \,\mathrm{d}y + \int_{\partial B(0,R)}^{} \Phi(x-y)\nabla u(y)\cdot \upsilon - u(y)\nabla\Phi(x-y)\cdot \upsilon \,\mathrm{d}S 
\]

Suppose we want to solve:

\(- \Delta u = f\) or \(\Delta u = 0\) in \(B(0,R)\) 

\(u = g\) on \(\partial B(0,R)\) 

In Greens Identity,

\[
    u(x) = \int_{B(0,R)}^{} \Phi(x-y) f(y) \,\mathrm{d}y + \int_{\partial B(0,R)}^{} \Phi(x-y)\nabla u(y)\cdot \upsilon - g(y)\nabla\Phi(x-y)\cdot \upsilon \,\mathrm{d}S 
\]

We seek:

\[
    G(x,y)=\Phi(x-y) + v(x,y)
\]

\(v\) is smooth and harmonic \(\forall x,y\in B(0,R)\) 

\(v(x,y)=-\Phi(x-y)\) \(\forall x\in B(0,R),\forall y\in \partial B(0,R)\) 

How to find \(v\) ? ``Reflect'' a given \(x\in B(0,R)\) 

\[
    x^{\star} \coloneqq \frac{R^2}{\vert x \vert ^2}x
\]

Claim: \(\frac{\vert x^{\star} - y \vert}{\vert x - y \vert }\) is independent of \(y\). This equals \(\frac{R}{\vert X \vert }\) 

\[
    \vert x^{\star} -y \vert ^2 = \left\vert \frac{R^2}{\vert x \vert ^2}x - y \right\vert ^2 = \frac{R^2}{\vert x \vert ^2} \left\vert \frac{R}{\vert x \vert }x - \frac{\vert x \vert }{R}y \right\vert^2 = \frac{R^2}{\vert x \vert ^2}\left\vert \frac{\vert y \vert }{\vert x \vert }x - \frac{\vert x \vert }{\vert y \vert}y \right\vert^2
\]

\[
    = \frac{R^2}{\vert X \vert ^2}\vert x-y \vert ^2
\]

\hfil
\hrule

Class 12: 02/05

Recap: if we want a solution to Poisson's Equation with Boundary Condition [here boundary is a ball]: we need:

\[
    u(x)=\int_{B(0,R)}^{} \Phi(x-y)f(y) \,\mathrm{d}y 
\]

\[
    + \int_{\partial B(0,R)}^{} \left[ \Phi(x-y)\nabla u\cdot\nu - g(y)\nabla\Phi\cdot\nu \right]  \,\mathrm{d}S 
\]

if \(-\Delta u = f\) in \(B(0,R)\) and \(u=g\) on \(\partial B(0,R)\) 

This is not a solution, since we can't find $\nabla u$.

For solving, we replace \(\Phi(x-y)\) by \(G(x,y)=\Phi(x-y)+v(,y)\) 

So we want \(v\) so that for all \(x\in B(0,R)\) and for all \(y\in \partial B(0,R)\) we have \(\Delta_x v(x,y)=0\) and \(v(x,y)=-\Phi(x-y)\).

For all \(x\in B(0,R)\) find it's `reflection' [actually inversion] \(\tilde{x}=\frac{R^2}{\vert x \vert ^2}x\) 

Then we have \(\vert \tilde x - y \vert = \frac{R}{\vert x \vert }\vert x-y \vert \) for all \(x\in B(0,R)\setminus \{ 0 \} \) and \(y\in \partial B(0,R)\).

\[
    \Phi(\tilde{x}-y)=\frac{1}{n(n-2)\alpha(n)}\frac{1}{\vert \tilde{x}-y \vert ^{n-2}}=\frac{1}{n(n-2)\alpha (n)}\left( \frac{\vert x \vert }{R} \right)^{n-2} \frac{1}{\vert x-y \vert ^{n-2}}
\]

Then, we define \(v(x,y)\) as following:

\[
    v(x,y)=-\left( \frac{R}{\vert x \vert } \right) ^{n-2}\Phi(\tilde x - y)
\]

So, our Green's Function is:

\[
    G(x,y)=\Phi(x-y)-\left( \frac{R}{\vert x \vert } \right)^{n-2}\Phi(\tilde x - y) 
\]

We need to find \(\nabla_y G\cdot\nu\) on \(\partial B(0,R)\). Note that \(\nu = \frac{y}{R}\) 

Then, \(\displaystyle\nabla_y\Phi(x-y)=\frac{1}{n(n-2)\alpha (n)}\frac{2-n}{\vert x-y \vert ^{n-1}}\frac{-(x-y)}{\vert x-y \vert }\) 

\(=\displaystyle \frac{1}{n \alpha (n)} \frac{x-y}{\vert x-y \vert ^n}\) 

Similarly, \(\displaystyle \nabla_y \left( \left( \frac{R}{\vert x \vert } \right) ^{n-2} \Phi(\tilde x - y) \right) = \left( \frac{R}{\vert x \vert } \right) ^{n-2} \frac{1}{n \alpha (n)}\frac{\tilde x - y}{\vert \tilde x - y \vert ^n}\)

Thus, \(\nabla_y G\cdot\nu\) their subtraction. After simplifying,

\[
    \nabla_y G\cdot\nu = \frac{1}{n \alpha (n) R} \frac{\vert x \vert ^2 - R^2}{\vert x - y \vert ^n}
\]

If \(u\) solves \(\Delta u = 0\) on \(B(0,R)\) with \(u=g\) on \(\partial B(0,R)\) , then,

\[
    u(x)= \frac{1}{n \alpha (n)R}\int_{\partial B(0,R)}^{} \frac{R^2-\vert x \vert ^2 }{\vert x - y \vert ^n} \,\mathrm{d}S_y 
\]

Let \(\displaystyle K(x,y)\coloneqq \frac{1}{n \alpha (n) R} \frac{R^2 - \vert x \vert ^2}{\vert x - y \vert ^n}\) 

This is the Poisson Kernel for the ball.

\begin{theorem}
    Let \(g\) be continuous on \(\partial B(0,R)\). Then the function \(u(x)\) given by

    \[
        u(x)\coloneqq \int_{\partial B(0,R)}^{} K(x,y)g(y) \,\mathrm{d}S_y 
    \]

    Solves the following:
    
    \begin{itemize}
        \item \(\Delta u=0\) in \(B(0,R)\)
        \item \(u\in C^{\infty} (B(0,R))\)  
        \item \(\lim_{x \to x_0} u(x)=g(x_0)\) for all \(x_0\in \partial B(0,R)\) 
    \end{itemize}

\end{theorem}

Properties of the Poisson Kernel \(K\):

For every \(x\in B(0,R)\) and for every \(y\in \partial B(0,R)\),

\begin{enumerate}
    \item \(K(x,y)\) is \(C^{\infty}\) in \(x\) and is harmonic \(\Delta _x K(x,y)=0\) 
    \item \(\int_{\partial B(0,R)}^{} K(x,y) \,\mathrm{d}S_y= 1 \) for all \(x\in B(0,R)\)  
\end{enumerate}

For \(2\), consider \(g\equiv 1\).

The unique solution to \(\Delta u = 0\) in \(B(0,R)\) and \(u=1\) in \(\partial B(0,R)\) is \(u\equiv 1\) 

Putting this in the theorem,

\(u(x)=1=\int_{\partial B(0,R)}^{} K(x,y) \,\mathrm{d}S_y  \) 

Which gives us property \(2\).

For proof of all other properties, they're essentially identical to the upper half plane.

Perron's Method gives us the solution to \(\Delta u = 0\) in \(\Omega \), \(u=g\) on \(\partial \Omega \) for \(g:\partial \Omega \to \mathbb{R} \) continuous and any ``reasonable'' bounded domain \(\Omega\)

By ``reasonable'' we mean domains that follow the `exterior ball condition', every point on the exterior has a ball contained in the exterior. For counterexample, consider a region with a cusp.

\section*{The Heat Equation}

This will be different from Heat Equation.

We will use Fourier Transform!

(see Evans 4.something for fourier transform)

\begin{definition}[Fourier Transform]
    Given a function \(f:\mathbb{R} ^n \to \mathbb{R} \) we define the fourier transform of \(f\) 
    
    \[
        \hat{f} (y) \coloneqq \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R}^n}^{} e^{-ix\cdot y} f(x) \,\mathrm{d}x 
    \]
    
\end{definition}

So, \(\hat{f} :\mathbb{R}^n \to \mathbb{C}\). If \(f\) is even, \(\hat{f} :\mathbb{R}^n \to \mathbb{R}\) 

\begin{definition}[Inverse Fourier Transform]
    We define the inverse fourier transform of \(f\) as

    \[
        \check{f}(x)\coloneqq \frac{1}{(2\pi)^{\frac{n}{2}}} \int_{\mathbb{R}^n}^{} e^{ix\cdot y}f(y) \,\mathrm{d}y 
    \]

\end{definition}

\begin{definition}[Schwartz Class]
    \[
        \mathcal{S} \coloneqq \{ f\in C^{\infty}(\mathbb{R}^n): \sup_{x\in\mathbb{R}^n} \left\vert x^\beta D^\alpha f(x) \right\vert < \infty \text{ for all multi-indices \(\alpha \) and \(\beta\) } \} 
    \]    

    In words, any derivative of \(f\) decays faster than any polynomial.

    Example: \(C^\infty_0(\mathbb{R}^n),e^{-\vert x \vert ^2}\) 

\end{definition}

\hfil
\hrule

Class 13: 02/07

Review of Fourier Transforms:

\[
    \hat{u}(y) = \frac{1}{(2\pi )^\frac{n}{2}} \int_{\mathbb{R} ^n}^{} e^{-ix\cdot y} u(x) \,\mathrm{d}x 
\]

Inverse transform:

\[
    \check{u}(x) = \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} e^{ix\cdot y} u(y) \,\mathrm{d}y 
\]

Next week: \(\check{\hat{u}}=u\) 

\(\mathcal{S} =\) Schwartz class.

\begin{definition}[Schwartz Class]
    \[
        \mathcal{S} \coloneqq \{ f\in C^{\infty}(\mathbb{R}^n): \sup_{x\in\mathbb{R}^n} \left\vert x^\beta D^\alpha f(x) \right\vert < \infty \text{ for all multi-indices \(\alpha \) and \(\beta\) } \} 
    \]    

    In words, any derivative of \(f\) decays faster than any polynomial.

    Example: \(C^\infty_c(\mathbb{R}^n),e^{-\vert x \vert ^2}\) 

\end{definition}

Note: Fourier Transform maps \(\mathcal{S}\) to \(\mathcal{S}\) 

\(u\in \mathcal{S} \implies \hat{u}\in C^{\infty}\) 

\[
    y^\beta D^\alpha \hat{u}(y)
\]

\[
    =\frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} y^\beta D_y^\alpha (e^{-ix\cdot y}u(x)) \,\mathrm{d}x 
\]

\[
    =\frac{1}{2\pi ^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} y^\beta (-ix)^\alpha e^{-ix\cdot y}u(x) \,\mathrm{d}x 
\]

\[
    =\frac{1}{(2\pi)^\frac{n}{2}}\int_{\mathbb{R} ^n}^{} (-i)^\beta (-ix)^\alpha D_x^\beta (e^{ix\cdot y})u(x) \,\mathrm{d}x 
\]

Use IBP several times to get \(u(x)\) inside the integral

\[
    \leq C \int_{\mathbb{R} ^n}^{} \left\vert e^{ix\cdot y}D^\beta (x^\alpha u(x)) \right\vert  \,\mathrm{d}x < \infty
\]

Note:

If \(u\in L^1(\mathbb{R}^n)\) then \(\int_{\mathbb{R} ^n}^{} \vert u \vert  \,\mathrm{d}x < \infty\) 

Then \(\vert \hat{u}(y) \vert \leq \frac{1}{(2\pi )^\frac{n}{2}}\int_{}^{} \vert u \vert  \,\mathrm{d}x \) 

In other words \(\hat{u}\in L^{\infty}\)

\begin{proposition}
    [Plancherel]

    If \(f\in L^2(\mathbb{R} ^n)\) then,

    \[
        \int_{\mathbb{R} ^n}^{} \vert f \vert ^2 \,\mathrm{d}x = \int_{\mathbb{R} ^n}^{} \vert \hat{f} \vert ^2 \,\mathrm{d}y 
    \]

\end{proposition}

How to make rigorous sense of \(\hat{f}\) if \(f\in L^2\)?

Answer: approximate with Schwartz class functions!

Take \(\{ f_j \} \in \mathcal{S}\) so that \(f_j \to f\) in \(L^2\) 

\(\int \vert f_j - f_k \vert ^2 \to 0 \implies \int \vert \hat{f}_j - \hat{f}_k \vert ^2\to 0\) 

Define \(\hat{f}\) as the limit of this cauchy sequence.

Now we develop some properties of fourier transforms that are useful for PDEs.

\section*{Fourier Transform and Convolutions}

\begin{definition}
    Assume \(f\) and \(g\) are in \(L^2(\mathbb{R} ^n)\) and define:

    \[
        (f\ast g)(x) = \int_{\mathbb{R} ^n}^{} f(x-z)g(z) \,\mathrm{d}z 
    \]

    Note: By H\"older's inequality, \((f\ast g)(x) \leq (\int f(x-z)^2)^{\frac{1}{2}}(\int g(z)^2)^{\frac{1}{2}}\) 
\end{definition}

Recall: H\"older's inequality: for \(1 < p < \infty \) given \(\frac{1}{p} + \frac{1}{q} = 1\)

\[
    \int fg\,\mathrm{d}x \leq \left(\int f^p\right)^\frac{1}{p}\left(\int g^q\right)^\frac{1}{q} 
\]

Take \(f,g\in L^1(\mathbb{R}^n)\cap L^2(\mathbb{R}^n)\). Then,

\[
    \widehat{f \ast g}(y) = \frac{1}{(2\pi)^\frac{n}{2}} \int_{\mathbb{R} ^n}^{} e^{-ix\cdot y}\int_{\mathbb{R} ^n}^{} f(x-z)g(z) \,\mathrm{d}z  \,\mathrm{d}x 
\]

Use Fubini:

\[
    =\frac{1}{(2\pi )^\frac{n}{2}} \int_{\mathbb{R}^n}^{} e^{-i(x-z)\cdot y} \int_{\mathbb{R} ^n}^{} e^{-iy\cdot z} f(x-z)g(z) \,\mathrm{d}x  \,\mathrm{d}z 
\]

Change variables. Let \(s = x-z, \mathrm{d}s=\mathrm{d}x  \) 

\[
    = \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} e^{-is\cdot y} \int_{\mathbb{R} ^n}^{} e^{-iy\cdot z} f(s)g(z) \,\mathrm{d}s  \,\mathrm{d}z 
\]

Therefore,

\[
    \widehat{f\ast g}(y) = (2\pi)^{\frac{n}{2}}\widehat{f}(y)\widehat{g}(y)
\]

When looking at the heat equation on friday, we'll use fourier transform on PDE.

What does PDE have to do wit Fourier?

We look at fourier transform of a derivative.

\begin{proposition}
    Let \(u\in \mathcal{S}\). For any multi-index \(\alpha\),

    \[
        \widehat{D^\alpha u}(y) = (iy)^\alpha \widehat{u}(y)
    \]

\end{proposition}

\begin{proof}
    \[
        \widehat{D^\alpha u}(y) = \frac{1}{(2\pi )^\frac{n}{2}}\int_{\mathbb{R} ^n}^{} e^{-ix\cdot y} D^\alpha u(x) \,\mathrm{d}x 
    \]

    Integrate by Parts \(\vert \alpha \vert \) times,

    \[
        = \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} e^{-ix\cdot y} (iy)^\alpha u(x) \,\mathrm{d}x 
    \]

    \[
        = (iy)^{\alpha} \widehat{u}(y)
    \]

\end{proof}

Today we finish with a key example.

\begin{proposition}
    Let \(a > 0\) constant and let \(u(x)\coloneqq e^{-a \vert x \vert ^2},x\in\mathbb{R} ^n\). Note that this is in \(\mathcal{S}\) 

    Then,

    \[
        \hat{u}(y)=\frac{1}{(2a)^{\frac{n}{2}}}e^{-\frac{\vert y \vert ^2}{4a}}
    \]

    So fourier transform of a Gaussian is a Gaussian.

    We prove this using ODEs.

\end{proposition}

\begin{proof}
    Case 1: \(n = 1\) 

    Then \(u^{\prime} (x)=-2a x e^{-ax^2}\) 

    Thus \(\widehat{u^{\prime} }(y)=iy\widehat{u}(y)\) [from the formula we proved before].

    Using the definition of Fourier Transform,

    \[
        \widehat{u^{\prime} } = \frac{1}{\sqrt{2\pi} }\int_{\mathbb{R}}^{} e^{-ixy}(-2axe^{-ax^2}) \,\mathrm{d}x 
    \]

    \[
        =-\sqrt{\frac{2}{\pi }} a \int_{\mathbb{R}}^{} e^{-ixy}xe^{-ax^2} \,\mathrm{d}x 
    \]

    Set this aside and consider \(\frac{\mathrm{d}}{\mathrm{d}y} \hat{u}(y)\)
    
    \[
        \frac{\mathrm{d}}{\mathrm{d}y} \hat{u}(y) = \frac{1}{\sqrt{2\pi } } \frac{\mathrm{d}}{\mathrm{d}y} \int_{\mathbb{R} }^{} e^{-ixy}e^{ax^2} \,\mathrm{d}x 
    \]

    \[
        =\frac{1}{\sqrt{2\pi } }\int_{\mathbb{R}}^{} -i x e^{-ixy - ax^2} \,\mathrm{d}x 
    \]

    \[
        = - i \cdot \frac{1}{\sqrt{2\pi }} \int_{\mathbb{R} }^{} e^{-ixy}xe^{-ax^2} \,\mathrm{d}x 
    \]

    Note that the stuff inside the integrand is the same. Therefore,

    \[
        \frac{\mathrm{d}}{\mathrm{d}y} \widehat{u}(y) = - \frac{y}{2a}\widehat{u}(y)
    \]

    Thus, \(\widehat{u}(y)\) is the solution of a first order linear ODE.

    Thus,

    \[
        \widehat{u}(y) = C e^{- \frac{y^2}{4a}}
    \]

    To get \(C\) consider

    \[
        C = \widehat{u}(0) = \frac{1}{\sqrt{2\pi}}\int_{\mathbb{R}}^{} e^{-ax^2} \,\mathrm{d}x 
    \]

    Thus, \(C = \frac{1}{\sqrt{2a}}\) 

\end{proof}

\hfil
\hrule

Class 14: 02/09

\begin{proposition}
    For \(u(x) =e^{-a \vert x \vert ^2}\),

    \[
        \widehat{u}(y) = \frac{1}{(2a)^{\frac{n}{2}}}e^{- \vert y \vert ^2 / 4a}
    \]

\end{proposition}

Last time:

Case 1: \(n = 1\) 

Recall: we created an ODE and solved it.

General case, \(n \geq 1\)?

We don't have to do any extra work!

\begin{proof}

If we look at the \(n\)-dimensional fourier transform,

\[
    \hat{u}(y) = \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} e^{-ix\cdot y}e^{-a \vert x \vert ^2} \,\mathrm{d}x 
\]

\[
    = \frac{1}{\sqrt{2\pi }}\cdots \frac{1}{\sqrt{2\pi } }\int_{\mathbb{R} }^{} \dots \int_{\mathbb{R} }^{} e^{-i x_1 y_1}\cdots e^{-i x_n y_n}e^{-a x_1^2}\cdots e^{-ax_n^2} \,\mathrm{d}x_1 \cdots \,\mathrm{d}x_n 
\]

\[
    = \left( \int_{\mathbb{R}}^{} e^{-ix_1 y_1}e^{-ax_1^2} \,\mathrm{d}x_1  \right) \cdots
    \left( \int_{\mathbb{R}}^{} e^{-ix_n y_n}e^{-ax_n^2} \,\mathrm{d}x_n  \right)
\]

\[
    = \frac{1}{\sqrt{2a}}\cdots \frac{1}{\sqrt{2a}} e^{-y_1^2 / 4a}\cdots e^{-y_n^2 / 4a}
\]

\end{proof}

\begin{proposition}
    If \(f,g\) are \(L^1\cap L^2\) then,

    \[
        \widehat{f * g}(y) = (2\pi)^{n / 2}\widehat f(y) \widehat g(y)
    \]
\end{proposition}

\begin{proposition}
    If \(f\in \mathcal{S} \) [Schwartz Class], for all multi-index \(\alpha\) we have

    \[
        \widehat{D^\alpha f}(y) = (iy)^\alpha \widehat{f}(y)
    \]
\end{proposition}

\section*{Heat (Diffusion) Equation}

Recall beginning of the course.

Let \(\kappa > 0\). Heat dissipates. So, without any sources or sinks, the homogeneous version of the heat equation is:

\[
    u_t = \kappa \Delta u
\]

Note that this is a time dependent problem, \(u\) is a function of \(x\in \mathbb{R} ^n\) [spatial variable] and \(t \geq 0\) [time variable].

We're looking at the `cauchy problem', which means we're looking at the whole space \(\mathbb{R}^n\), [not some bounded domain \(\Omega\)]

At time \(0\) we're going to specify \(u(x,0)=f(x)\), where \(f : \mathbb{R} ^n \to \mathbb{R}\). How nice does \(f\) have to be? We're just going to use fourier transformation without caring, then we'll come back and see what we did makes sense under which context.

Basically, we're formally computing the fourier transform of the PDE.

\[
    u_t = \kappa \Delta u
\]

\[
    \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R} ^n}^{} e^{-i x\cdot y} u_t(x,t) \,\mathrm{d}x = \kappa \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R}^n}^{} e^{-ix\cdot y} (u_{x_1 x_1} + \dots + u_{x_n x_n}) \,\mathrm{d}x 
\]

Note the derivative on the left w.r.t.\ \(t\) so we're not supposed to use the derivative formula yet. We just take \(\frac{\mathrm{d}}{\mathrm{d}t} \) outside.

\[
    \frac{\partial }{\partial t} \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R}^n}^{} e^{-ix\cdot y}u(x,t) \,\mathrm{d}x = \kappa \left[ (iy_1)^2 + \dots +(iy_n)^2 \right] \widehat u(y,t)
\]

\[
    \frac{\partial}{\partial t} \widehat u(y,t) = - \kappa \vert y \vert \widehat u(y,t)
\]

We could also fourier transform the initial condition.

\[
    u(x,0)=f(x) \implies \widehat u(y,0) = \widehat f(t)
\]

Note that \(y\) is a parameter, and our fouriered ODE is a differential equation on \(t\), we can treat \(y\) as a constant.

So our solution is:

\[
    \widehat{u}(y,t) = C(y) e^{-\kappa \vert y \vert ^2 t} 
\]

\[
    \widehat{u}(y,0) = \widehat f(y) = C(y)
\]

\[
    \boxed{ \widehat u(y,t) = \widehat f(y) e^{\kappa \vert y \vert ^2 t} }
\]

We could fourier invert it, but that's not very enlightening. However, \(e^{\kappa \vert y \vert ^2 t}\) is a Gaussian on \(y\) so we can recognize this as product of fourier transforms and original differential equation has a convolution as a solution.

We want to identify \(g(x,t)\) such that \(\widehat \Phi(y,t)= e^{-\kappa \vert y \vert ^2 t}\).

Equating coefficients,

\[
    \boxed{g(x,t) = e^{- \vert x \vert ^2 / 4 \kappa t} \implies \widehat g(y,t) = (2 \kappa t)^{\frac{n}{2}} e^{-\kappa \vert y \vert ^2 t} }
\]

Thus, \(\widehat u = \frac{1}{(2 \kappa t)^{\frac{n}{2}}}\widehat f\widehat g=\frac{1}{(4\kappa\pi t)^{\frac{n}{2}}}\widehat{f * g}\)

Therefore, \(u = \frac{1}{(4\kappa\pi t)^{\frac{n}{2}}} f * g\)

Thus, \(u(x,t) = \frac{1}{(4\pi \kappa t)^{\frac{n}{2}}} \int_{\mathbb{R}^n}^{} e^{-\vert x - y \vert ^2 / 4 \kappa t}f(y)  \,\mathrm{d}y \) 

Denote \(\Phi(x,t) \coloneqq \frac{e^{-\vert x \vert ^2 / 4 \kappa t}}{(4 \pi \kappa t)^{n / 2}}\) 

We call this the heat kernel.

\begin{theorem}
    Let \(f\) be bounded on \(\mathbb{R}^n\) 

    \[
        u(x,t)\coloneqq \begin{dcases}
            \Phi * f , &\text{ if } t > 0 ;\\
            f(x), &\text{ if } t = 0 ;
        \end{dcases}
    \]

    Then for \(t > 0\) and \(\forall x\in\mathbb{R}^n\),

    \[
        u \in C^{\infty}, u_t = \kappa \Delta u
    \]

    Furthermore if \(f\) is continuous, then,

    \[
        \boxed{\lim_{x \to x_0, t \to 0^+} u(x,t) = f(x_0)}
    \]

\end{theorem}

\begin{proof}
    Properties of \(\Phi\) :

    \begin{enumerate}
        \item \(\Phi\in C^{\infty}\) for \(t > 0\) for all \(x\in \mathbb{R}^n\) 
        \item \(\Phi_t = \kappa \Delta \Phi\) for \(t > 0\).
        The heat kernel decays exponentially so it is Schwartz Class.

        Then \(\widehat\Phi_t = - \kappa \vert y \vert ^2 \Phi\) 

        Inverting the fourier transform,

        \[
            \Phi_t = \kappa \Delta \Phi
        \]

        \item \(\int_{\mathbb{R} ^n}^{} \Phi(x,t) \,\mathrm{d}x = 1\). \(\forall t > 0\), consider \(\frac{1}{(4\pi \kappa t)^\frac{n}{2}}\int_{\mathbb{R} ^n}^{} e^{- \vert x \vert ^2 / 4 \kappa t} \,\mathrm{d}x =\)
        
        \( \frac{1}{(\pi)^{n / 2}} \int_{\mathbb{R}^n}^{} e^{- \vert z \vert ^2} \,\mathrm{d}z = \frac{1}{(\pi)^\frac{n}{2}}\pi^\frac{n}{2} = 1 \) 

    \end{enumerate}

    To begin the argument,

    \[
        u(x,t) - f(x_0) = \int_{\mathbb{R}^n}^{} \Phi(x-y)f(y) \,\mathrm{d}y - f(x_0) 
    \]

    \[
        = \int_{\mathbb{R}^n}^{} \Phi(x-y) \left[ f(y) - f(x_0) \right]  \,\mathrm{d}y 
    \]

    \[
        = \int_{y : \vert y - x_0 \vert < \delta}^{} \Phi(x-y) [f(y) - f(x_0)] \,\mathrm{d}y + \int_{y : \vert y - x_0 \vert \geq \delta }^{} \Phi(x-y)[f(y)-f(x_0)] \,\mathrm{d}y 
    \]

    For first integral argue continuity, for second integral argue going to \(0\). [This is an important problem solving technique.]

\end{proof}

Properties of the solution:

\begin{enumerate}
    \item For \(t > 0\) we have \(u\in C^{\infty}\) in \(x\) and \(t\). We basically have infinite smoothing.
    \item We have somewhat of a maximal principle.
    
    \[
        \vert u(x,t) \vert \leq \int_{\mathbb{R}^n}^{} \Phi(x-y,t) \vert f(y) \vert  \,\mathrm{d}y \leq \sup \vert f(x) \vert \int_{\mathbb{R}^n}^{} \Phi(x-y,t) \,\mathrm{d}y = \sup_{\mathbb{R}^n} \vert f \vert 
    \]

\end{enumerate}

\hfil
\hrule

Class 15: 02/12

\section*{Heat Kernel}

Heat Kernel is:

\[
    \Phi(x,t) = \frac{1}{(4\pi \kappa t)^\frac{n}{2}}e^{-\frac{\vert x \vert ^2}{4 \kappa t}}
\]

Solution to \(u_t = \kappa \Delta u, x\in\mathbb{R}^n,t > 0, u(x,0)=f(x)\) for \(x\in\mathbb{R}^n\) is given by:

\[
    u(x,t) = \begin{dcases}
        \int_{\mathbb{R}^n}^{} \Phi(x-y,t)f(y) \,\mathrm{d}y  &\text{ if } t > 0 ;\\
        f(x) &\text{ if } t = 0 ;
    \end{dcases}
\]

This is the solution to the Cauchy Problem [solving over \(\mathbb{R}^n\) instead of some \(\Omega\)]

Last class we proved the fact that this is indeed continuous, when we do \(t \to 0\) the solution does converge to \(f(x)\) 

We also have infinite differentiability for \(t > 0\) 

We have `infinite propagation speed': Suppose \(f\) is compactly supported [0 outside some compact set] and non-negative. Then, for \(t > 0\), \(u\) is non-zero everywhere.

Visualization: consider an infinite bar. Suppose \(f\equiv 0\) outside \((0,1)\), \(n=1\) . For \(t > 0\) we have \(u(x,t)=\int_\mathbb{R} \Phi(x-y,t)f(y)dy > 0\) for all \(x\in\mathbb{R}\). This is disturbing in terms of physics, because heat is travelling with infinite speed.

Another observation: If \(\int_{\mathbb{R}^n} f(x)dx < \infty\) then for any \(t > 0\),

\[
    \int_{\mathbb{R}^n}^{} u(x,t) \,\mathrm{d}x = \int_{\mathbb{R}^n}^{} \int_{\mathbb{R}^n}^{} \Phi(x-y,t)f(y) \,\mathrm{d}y  \,\mathrm{d}x = \int_{\mathbb{R}^n}^{} \int_{\mathbb{R}^n}^{} \Phi(x-y,t)f(y) \,\mathrm{d}x  \,\mathrm{d}y 
\]

Since integral of \(\Phi\) is \(1\), this is just \(\int_{\mathbb{R}^n}^{} f(y) \,\mathrm{d}y \). So, the total heat stays the same. So, there's an energy conservation.

Note: Mathematicians can mean any integral of the solution as energy. Here we're talking about the physical energy.

Most annoyingly, the solution to the heat equation is not unique! It is the only one satisfying \(\vert u(x,t) \vert \leq A e^{\alpha \vert x \vert ^2}\) for \(x\in\mathbb{R}^n, 0\leq t \leq T\). However, there exists a solution to \(\tilde u_t = \kappa \Delta \tilde u\) with initial condition \(\tilde u(x,0)=0\) such that \(\tilde u\) violates such a bound.

\section*{Inhomogeneous Heat Equation}

The equation is \(u_t = \Delta u + F(x,t)\) for \(x\in \mathbb{R}^n,t>0\) with \(u(x,0)=0\) [so we have a heat source].

[If initial condition is \(u(x,0)=f\), simply add the solution to the homogeneous heat equation.]

\underline{Duhenel's Principle:} This is essentially a recipe where we build the solution to an inhomogeneous solution out of homogeneous solutions.

\(\forall s > 0,\) define \(U(x,t,s)\) solve \(U_t = \Delta U\) for \(t > s\) and \(U(x,s,s)=F(x,s)\). Then, by the solution to the homogeneous heat equation,

\[
    U(x,t,s) = \int_{\mathbb{R}^n}^{} \Phi(x-y,t-s)F(y,s) \,\mathrm{d}y 
\]

\underline{Claim}: Define \(u(x,t)=\displaystyle \int_{0}^{t} U(x,t,s) \,\mathrm{d}s \) then \(u\) solves the inhomogeneous heat problem.

Assume \(F\) is \(C^2\) in \(x\) and \(C^1\) in \(t\), and also assume \(F\) is compactly supported [in both \(x\) and \(t\)].

First we're going to change variable. Let \(z\coloneqq x - y\) and \(\tau = t - s\). Then,

\[
    u(t) = \int_{0}^{t} \int_{\mathbb{R}^n}^{} \Phi(z,t) F(x-z,t-\tau) \,\mathrm{d}z \,\mathrm{d}\tau 
\]

Now we compute derivatives.

\[
    u_t = \int_{0}^{t} \int_{\mathbb{R}^n}^{} \Phi(z,t)F_t(x-z,t-\tau) \,\mathrm{d}z  \,\mathrm{d}\tau + \int_{\mathbb{R}^n}^{} \Phi(z,t)F(x-z,0) \,\mathrm{d}z 
\]

\[
    \Delta u = \int_{0}^{t} \int_{\mathbb{R}^n}^{} \Phi(z,\tau)\Delta_x F(x-z,t-\tau) \,\mathrm{d}z  \,\mathrm{d}\tau 
\]

Fix \(\epsilon > 0\).

\[
    u_t - \Delta u = \int_{0}^{\epsilon} \int_{\mathbb{R}^n}^{} \Phi(z,t) \left\{ F_t(x-z,t-\tau) - \Delta_x F(x-z,t-\tau) \right\}  \,\mathrm{d}z \,\mathrm{d}\tau 
\]

\[
    +\int_{\epsilon}^{t} \int_{\mathbb{R}^n}^{} \Phi(z,t) \left\{ F_t(x-z,t-\tau) - \Delta_x F(x-z,t-\tau) \right\}  \,\mathrm{d}z \,\mathrm{d}x 
\]

\[
    +\int_{\mathbb{R}^n}^{} \Phi(z,t)F(x-z,0) \,\mathrm{d}z = I_{\epsilon } + J_{\epsilon} + \int_{\mathbb{R}^n}^{} \Phi(z,t)F(x-z,0) \,\mathrm{d}z 
\]

Note that \(I_{\epsilon} \) is bounded. \(\vert I_{\epsilon}  \vert \leq \max (\vert F_t \vert + \vert \Delta F \vert )\int_{0}^{\epsilon} \int_{\mathbb{R}^n}^{} \Phi(z,\tau) \,\mathrm{d}z  \,\mathrm{d}\tau\).

Since the Poisson kernel integrates to \(1\) this is less than \(\max\cdot \epsilon\) so \(I_{\epsilon} \to 0\).

For \(J_{\epsilon}\), switch \(\frac{\partial}{\partial \tau} \) for \(\frac{\partial}{\partial \tau} \) and switch \(\frac{\partial }{\partial z} \) for \(-\frac{\partial}{\partial x} \). Rewrite \(J_{\epsilon} \) :

\[
    J_{\epsilon} = \int_{\epsilon}^{t} \int_{\mathbb{R}^n}^{} \Phi(z,\tau) \left\{ - F_\tau(x-z,t-\tau)-\Delta_z F(x-z,t-\tau) \right\}  \,\mathrm{d}z  \,\mathrm{d}\tau 
\]

We can do integration by parts with \(\tau\). We also simplify the integral using \(\int_\Omega v\Delta w - w\Delta v = \int_{\partial \Omega} v\nabla w\cdot\nu - w\nabla v\cdot\nu\).

This lets us write the integral as:

\[
    = - \int_{\mathbb{R}^n}^{} \Phi(z,t)F(x-z,t-\tau) \,\mathrm{d}z \bigg|_{\tau = \epsilon}^{\tau = t} + \int_{\epsilon}^{t} \int_{\mathbb{R}^n}^{} \Phi_\tau (z,\tau) F(x-z,t-\tau) \,\mathrm{d}z  \,\mathrm{d}\tau 
\]

\[
    - \int_{\epsilon}^{t} \int_{\mathbb{R}^n}^{} \Delta_z \Phi(z,\tau)F(x-z,t-\tau) \,\mathrm{d}z  \,\mathrm{d}\tau 
\]

The latter two integrals cancel each othersince \(\Phi\) is the solution of the heat equation.

Thus, we have:

\[
    J_{\epsilon} = - \int_{\mathbb{R}^n}^{} \Phi(z,t)F(x-z,0) \,\mathrm{d}z + \int_{\mathbb{R}^n}^{} \Phi(z,\epsilon) F(x-z,t-\epsilon) \,\mathrm{d}z 
\]

The first integral is \(0\) and as we let \(\epsilon \to 0\) we see that the second integral \(\to F(x,t)\).

\hrulefill

Class 16: 02/14

Last time: Duhenel's Principle. We used this to come up with the solution to the inhomogeneous differential equation \(u_t = \kappa \Delta u + F(x,t), u(x,0)=0\) for \(x\in \mathbb{R}^n, t > 0\).

Our solution was:

\[
    u(x,t) = \int_{0}^{t} \int_{\mathbb{R}^n}^{} \Phi(x-y,t-\tau)F(y,\tau) \,\mathrm{d}y  \,\mathrm{d}\tau 
\]

\[
    u(x,t) = \int_{0}^{t} \int_{\mathbb{R}^n}^{} \frac{1}{[4\pi\kappa(t-\tau)]^{n / 2}}e ^ {- \frac{\vert x-y \vert ^2}{4 \kappa (t-\tau)} }F(y,\tau) \,\mathrm{d}y  \,\mathrm{d}\tau 
\]

If we want to solve \(u_t = \kappa \Delta u + F(x,t)\) with initial condition \(u(x,0)=f(x)\),

Add a homogeneous solution:

\[
    u(x,t) = \int_{\mathbb{R}^n}^{} \Phi(x-y,t)f(y) \,\mathrm{d}y + u(x,t) + \int_{0}^{t} \int_{\mathbb{R}^n}^{} \Phi(x-y,t-\tau)F(y,\tau) \,\mathrm{d}y  \,\mathrm{d}\tau 
\]

One can use this solution as a starting point for solving non-linear heat equation.

What is a nonlinear heat equation?

\[
    u_t = \Delta u + h(u)
\]

Where \(h(u)\) is non-linear, with initial condition \(u(x,0) = f(x)\)

Example \(h:\) it can be a power function.

How to solve it?

Start with some initial `guess' \(u_1\).

Solve \(u_t = \Delta u + h(u_1)\) where \(u(x,0)=f(x)\).

Taking \(h(u_1)\) to be \(F(x,t)\), we find a solution to \(u_2\).

Plug and solve: gives us \(u_3\).

Thus we generate a sequence of linear solutions \(\{ u_k \} \).

We need this sequence to converge, or at least a subsequence to converge. If \(u_{k_j}\to u^{\star}\) then \(u^{\star}\) is our solution.

This converts solving our nonlinear PDE to solving a linear PDE and finding a fixed point.

\begin{theorem}
    Assume \(u\) solves \(u_t = \Delta u\) for \(x\in \mathbb{R}^n\) for \(0 < t \leq T\), and we have the initial condition \(u(x,0) = g(x)\), where \(g\) is continuous and bounded on \(\mathbb{R}^n\) 

    If \(\vert u(x,t) \vert \leq M e^{a \vert x \vert ^2}\) for some \(M,a\) for all \(x\in\mathbb{R}^n, t\in [0,T]\), Then,

    \[
        \sup_{x\in\mathbb{R}^n, t\in [0,T)} u(x,t) = \sup_{x\in\mathbb{R}^n} g(x)
    \]

    In particular, if \(g \equiv 0\) we have \(u \equiv 0\) 

\end{theorem}

To prove this, we will use the maximum principle for the heat equation on a bounded domain.

\begin{theorem}

    \underline{Weak Maximum Principle for the Heat Equation on a Bounded Domain}: We let \(\Omega \subset \mathbb{R}^n\) be a bounded, open, connected set. Assume \(u\) is \(C^2\) in \(x\), \(C^1\) in \(t\) on \(\Omega_T \coloneqq \Omega\times (0,T]\) and continuous in the closure \(\overline{\Omega_T}\).

    Assume \(u_t - \Delta u \leq 0\) in \(\Omega_T\). Then,

    \[
        \max_{\overline{\Omega_T} } u(x) = \max_{(\Gamma\times \{ 0 \}) \cup (\partial \Omega \times [0,T]) } u(x)
    \]

    Furthermore, if \(u_t - \Delta u \geq 0\) in \(\Omega_T\) then

    \[
        \min{\overline{\Omega_T} } u(x) = \min_{(\Gamma\times \{ 0 \}) \cup (\partial \Omega \times [0,T]) } u(x)
    \]

    \underline{Notation:} We write \(\partial_P \Omega_T \coloneqq (\Omega \times \{ 0 \} )\cup  (\partial \Omega \times [0,T])\). This is the parabolic boundary of \(\Omega_T\).

\end{theorem}

\begin{proof}
    Case 1: \(u_t - \Delta u < 0\) 

    \underline{Claim:} \(\max_{\overline{\Omega }_T} u(x,t)\) cannot occur on \(\overline{\Omega}_T \setminus \partial _P \Omega_T\) 
    
    We use contradiction. Suppose \(\exists x_0\in \Omega, t_0\in (0,T]\) such that \(\max u(x,t) = u(x_0,t_0)\).

    Thus, at \(x_0\), we have \(u_{x_i x_i}(x_0,t_0) \leq 0\) for every \(i\in [n]\) 
    
    Adding them, \(\Delta u(x_0,t_0) \leq 0\).

    Also, \(u_t (x_0,t_0)\) if \(t_0 < T\).

    If \(t_0 = T\), then \(u_t(x_0,t_0)=\lim_{h\to 0} \frac{u(x_0,T) - u(x_0,T - h)}{h} \) [one sided derivative]
    
    Then \(u_t(x_0,y_0) \geq 0\).

    So, we have \((\geq 0) - (< 0) = (< 0)\). So this is a contradiction.

    Case 2: \(u_t - \Delta u \leq 0\)

    Define \(u_\delta (x,t) = u(x,t) - \delta t\) for \(t > 0\).

    Then \(u_{\delta _t} - \Delta u_\delta = u_t - \Delta u - \delta < 0\)

    Apply Case 1 to see:

    \(\max u(x,t) = \max u_\delta (x,t) + \delta t \leq \max_{x,t\in \partial_P \Delta_T} u_\delta + \delta T \leq \max_{\partial_p \Delta_T} u + \delta T\).
    
    Let \(\delta \to 0\) to reach the conclusion.

    Same for the other sign.

\end{proof}

Now we begin proof on \(\mathbb{R}^n\) 

\begin{proof}
    Assume \(\vert u(x,t) \vert \leq  M e ^ {a \vert x \vert ^2}\) for some \(M,a\) for \(x\in\mathbb{R}^n, t\in [0,T]\).

    \underline{Case 1}: assume \(4aT < 1\) 
    
    Thus, \(\exists \epsilon > 0\), small enough such that \(4a (T+ \epsilon ) < 1\),
.
    Fix \(y\in\mathbb{R}^n\) and let \(\mu > 0\) be a positive parameter. Define:

    \[
        v^\mu (x,t) \coloneqq u(x,t) - \frac{\mu}{(\tau + \epsilon - t)^{n / 2}} e^\frac{\vert x - y \vert^2}{4 (T+\epsilon -t)}
    \]

    Note that \(v^\mu\) still solves the heat equation. To see this, write:

    \[
        v^\mu = u - \mu \frac{1}{(-1)^{n / 2}} \frac{1}{(1-)^{\frac{n}{2}}(t - (\tau + \epsilon)^{n/2})} e ^ {-\frac{\vert x - y \vert ^2}{4(t - (\tau + \epsilon))}}
    \]

    So this is basically an ugly looking version of the heat kernel. Thus it indeed solves the heat equation.

    \underline{Strategy (will finish on friday)}: Apply the weak maximal principle to \(v^\mu\) on some cylinder \(\{ x\in \mathbb{R}^n : \vert x - y \vert \leq , t\in [0,1] \} \)

    We're going to take \(r\) to be huge, so that \(u(x,t)\) is increasing faster than \(M e ^ {a \vert x \vert ^2}\) 

\end{proof}

\hrulefill

Class 17: 02/16

\underline{Weak Maximum Principle:}

Let \(u_t = \kappa \Delta u\) in \(\Omega \times (0,T]\) 

\(\implies \max_{\overline{\Omega_T}}=\max_{(\Omega \times \{ 0 \} ) \cup (\partial \Omega \times [0,T])}\) 

\begin{theorem}
    Assume \(u\in C^2_1(\mathbb{R}^n\times (0,T])\cap C(\mathbb{R}^n\times [0,T])\) solves \(u_t = \Delta u, u(x,0)=g(x)\) for \(g:\mathbb{R}^n \to \mathbb{R}\) continuous and bounded. Assume the expontential bound: \(\vert u(x,t) \vert \leq M e^{a \vert x \vert ^2}\). Then:

    \[
        \sup_{\mathbb{R}^n \times [0,T]} u(x,t) = \sup_{\mathbb{R}^n} g(x)
    \]
\end{theorem}

We started the proof yesterday.

\begin{proof}
    \underline{Case 1:} \(4aT < 1\) 

    \(\implies  \exists \epsilon >0\) such that \(4a(T+\epsilon) < 1\)
    
    Fix \(y\in \mathbb{R}^n, \mu >0\) 

    Define \(v^\mu(x,t)\) to be:

    \[
        u(x,t) - \frac{\mu}{(T + \epsilon - t)^\frac{n}{2}}e^{\frac{\vert x - y \vert ^2}{4(T + \epsilon - t)}}
    \]

    Then, \(v^\mu_t = \Delta v^\mu\) so this is indeed a solution to the heat equation.

    We want to apply the maximum principle here. But we need a bounded domain.

    Take the domain to be a `cylinder': \(B(y,r)\times [0,T]\). For fixed \(r\) we can apply the weak maximum principle. Thus,

    \[
        v^\mu(x,t) \leq \max_{B(y,r)\times \{ 0 \} \cup \partial B(y,r)\times [0,T]}
    \]

    \underline{Claim:} This max occurs on the `bottom' of the cylinder, not the sides. So it occurs on \(B(y,r)\times \{ 0 \} \) 

    Here is where the exponential bound comes to play.

    Take a look at the sides: for \(x\) such that \(\vert x - y \vert = r\),

    Then \(v^\mu(x,t) \leq Me^{a \vert x \vert ^2} - \frac{\mu}{(T + \epsilon  - t)^\frac{n}{2}} e^{\frac{r^2}{4(T+\epsilon -t)}}\) 

    Note that \(\vert x \vert \leq \vert y \vert + r\). Also note that \(\frac{1}{4(T+\epsilon)}> a =?\frac{1}{4(T+\epsilon)} = a + \gamma\) for some \(\gamma > 0\). Also, if we plug in \(t = 0\) we get smallest denominators and hence highest value.
    
    \[
        v^\mu(x,t) \leq Me^{a (\vert y \vert + r)^2} - \frac{\mu}{(T + \epsilon )^{n / 2}}e^{\frac{r^2}{4(T+\epsilon)}}
    \]

    \[
        v^\mu(x,t)\leq M e^{a(\vert y \vert + r)^2} - \frac{\mu}{(T+\epsilon )^\frac{n}{2}}e^{(a+ \gamma )r^2}
    \]

    \[
        \sim e^{ar^2} - \mu e^{(a+\gamma)r^2}
    \]

    So, for \(r\) big enough, we can make \(v^\mu < \sup_{\mathbb{R}^n} g(x)\) on \(\partial B(y,r)\times [0,T]\)  

    Thus, from the weak maximum principle, \(v^\mu(x,t) \leq \sup_{\mathbb{R}^n}v^\mu(x,0) \leq \sup_{\mathbb{R}^n} g(x)\) 

    For all \(x\in \mathbb{R}^n, t\in [0,T]\) 

\end{proof}

This also gives us a solution with condition \(u(x,0)\equiv 0\): since \(u,-u\) are all solution, the only possible solution that satisfies the exponential bound is the one identically 0.

\section*{2 Theorems on Fourier Transformation}

\begin{theorem}
    \underline{Fourier Inversion Formula:}

    \[
        \check{\hat{u}} = u
    \]

    Where:

    \[
        \hat{u}(y) = \frac{1}{(2\pi)^\frac{n}{2}}\int_{\mathbb{R}^n}^{} e^{-ix\cdot y}u(x) \,\mathrm{d}x 
    \]

    \[
        \check{u}(x) = \frac{1}{(2\pi)^\frac{n}{2}}\int_{\mathbb{R}^n}^{} e^{ix\cdot y}u(y) \,\mathrm{d}y 
    \]
\end{theorem}

\begin{theorem}
    [Plancherel]

    For \(u\in L^2(\mathbb{R}^n)\) 

    \[
        \lVert u \rVert _{L^2(\mathbb{R}^n)} = \lVert \hat{u} \rVert _{L^2(\mathbb{R}^n)}
    \]

    [Fourier transform can be defined as limit of fourier transforms of Schwartz Class]

\end{theorem}

\begin{proof}
    [Fourier Inversion]

    Let \(u\in \mathcal{S}\)
    
    Recall: For \(f(x) = e^{- a \vert x \vert ^2}\),
    
    \(\hat{f}() = \frac{1}{(2a)^{\frac{n}{2}}}e^{-\frac{1}{4a}\vert y \vert ^2}\) 

    Note that, \(\forall u,v\in L^2\) 
    
    \[
        \int_{\mathbb{R}^n}^{} u(y)\hat{v}(y) \,\mathrm{d}y = \int_{\mathbb{R}^n}^{} \hat{u}(y)v(y) \,\mathrm{d}y 
    \]

    Call this (1)

    We can prove this by putting the formula for fourier tranform and doing fubini integral swap.

    Fix any \(z\in\mathbb{R}^n\). Fix \(\epsilon > 0\) 

    Define \(v_{\epsilon}(x) \coloneqq e^{i x\cdot z - \epsilon \vert x \vert ^2} \) 

    \[
        \hat{v}_{\epsilon}(y) = \frac{1}{(2\pi)^{\frac{n}{2}}}\int_{\mathbb{R}^n}^{} e^{-ix\cdot y} e^{ix\cdot z}e^{-\epsilon \vert x \vert ^2} \,\mathrm{d}x  
    \]

    \[
        =\frac{1}{(2\pi )^\frac{n}{2}}\int_{\mathbb{R}^n}^{} e^{-ix\cdot (y-z)} e^{-\epsilon \vert x \vert ^2} \,\mathrm{d}x 
    \]

    This is the fourier transform of \(e^{-\epsilon \vert x \vert ^2}\) evaluated at \(y - z\)! So we can use the formula:

    \[
        = \frac{1}{(2\epsilon)^\frac{n}{2}}e^{-\frac{1}{4\epsilon}\vert y - z \vert ^2}
    \]

    Now we use equation (1) with \(u\) and \(v_{\epsilon} \) 

    \[
        \frac{1}{(2\epsilon)^\frac{n}{2}}\int_{\mathbb{R}^n}^{} u(y)e^{-\frac{1}{4\epsilon}\vert y - z \vert ^2}  \,\mathrm{d}y = \int_{\mathbb{R}^n}^{} \hat{u}(y)e^{iy\cdot z - \epsilon \vert y \vert ^2} \,\mathrm{d}y 
    \]

    \[
        \frac{1}{(2\pi)^\frac{n}{2}}\frac{1}{(2\epsilon)^\frac{n}{2}}\int_{\mathbb{R}^n}^{} u(y)e^{-\frac{1}{4\epsilon}\vert y - z \vert ^2}  \,\mathrm{d}y = \frac{1}{(2\pi)^\frac{n}{2}}\int_{\mathbb{R}^n}^{} \hat{u}(y)e^{iy\cdot z - \epsilon \vert y \vert ^2} \,\mathrm{d}y 
    \]

    Now let \(\epsilon  \to 0\). Since \(u\) is in Schwartz,

    \(RHS \to \frac{1}{(2\pi)^\frac{n}{2}}\int_{\mathbb{R}^n}^{} e^{iy\cdot z}\hat{u}(y) \,\mathrm{d}y = \check{\hat{u}}(z)\) 

    We want to show that \(LHS \to u(z)\) 

    Let \(K(y) = \frac{1}{\pi^\frac{n}{2}}e^{-\vert y \vert ^2}\). Then \(\int_{\mathbb{R}^n}^{} K(y) \,\mathrm{d}y = 1\). Then,

    \[
        K_{\epsilon}(y) \coloneqq \frac{1}{(2\sqrt{\epsilon } )^n}K(\frac{y}{2\sqrt{\epsilon} }) 
    \]

    So \(\int_{\mathbb{R}^n}^{} K_{\epsilon}(y)  \,\mathrm{d}y = 1\)
    
    \[
        \lim_{\epsilon\to 0} LHS = \lim_{\epsilon \to 0} \int_{\mathbb{R}^n}^{} K_{\epsilon}(y-z)u(y)  \,\mathrm{d}y = u(z)
    \]

    This proves the Fourier Inversion Formula.

\end{proof}

\section*{When Convolution Fails}

\underline{Laplace's Equation on an infinite strip}:

Let \(S\) be an infinite horizontal strip [0 to L]

\(\Delta u = 0\) in \(S\)

\(u(x_1,0) = f(x_1), -\infty < x_1 < \infty\) 

\(u_{x_2}(x_1,L) = 0, -\infty < x_1 < \infty\) 

Fourier transform in \(x_1\) 

\(u_{x_1 x_1}+u_{x_2 x_2} = 0\)

\[
    \implies (i y_1)^2 \hat{u}(y_1,x_2)+\hat{u}_{x_2 x_2} (y_1,x_2) = 0
\]

\[
    \implies \hat{u}_{x_2 x_2} - y_1^2 \hat{u}(y_1,x_2) = 0
\]

This is a ODE in \(x_2\).

\[
    \hat{u}(y_1,x_2)= A_1(y_1)\cosh(y_1 x_2) + A_2(y_1 x_2)
\]

More conveniently,

\[
    \hat{u}(y_1,x_2) = B_1(y_1)\cosh(y_1(x_2 - L)) + B_2(y_1)\sinh (y_1(x_2 - L))
\]

\(u_{x_2}(x_1,L)=0\) tells us \(B_2 \equiv 0\).

\[
    \hat{u}(y_1,x_2) = B_1(y_1)\cosh(y_1(x_2 - L))
\]

\[
    \hat{u}(y_1,0) = B_1(y_1)\cosh(-y_1 L) = \hat{f}(y_1)
\]

Thus, \(B_1(y_1) = \frac{\hat{f}(y_1)}{\cosh(y,L)}\)

So, \(\hat{u}(y_1,x_2) = \hat{f}(y_1)(\frac{\cosh y_1(x_2 - L )}{\cosh(y_1 L)})\) 

\hrulefill

Class 18: 02/19

\underline{Laplace's Equation in a Strip}

We have \(\Delta u = 0\) when \(-\infty < x_1 < \infty \) and \(0 < x_2 < L\) 

Boundary conditions: \(u(x_1,0)=f(x_1),u_{x_2}(x_1,L)=0\) when \(-\infty < x_1 < \infty\).

We took Fourier Transform, and got \(\widehat{u}_{x_2 x_2}(y_1,x_2)-y_1^2\widehat{u}(y_1,x_2)=0\) which gave us \(\widehat{u}(y_1,x_2)=\widehat{f}(y_1)\frac{\cosh(y_1(x_2 - L ))}{\cosh(y_1 L)}\)

We \underline{don't get a convolution} so we need to use inverse fourier [also use cosh is even]:

\[
    u(x_1,x_2)=\int_{-\infty}^{\infty} e^{ix_1 y_1}\widehat{f}(y_1)\boxed{\frac{\cosh(y_1 (L - x_2))}{\cosh(y_1 L)}} \,\mathrm{d}y_1 
\]

The boxed part is often called \underline{fourier multiplier}.

Recall that \(\cosh t = \frac{e^t + e^{-t}}{2}\) 

So, for \(y_1 \gg 1\) we have:

\[
    \frac{\cosh(y_1(L-x_2))}{\cosh(y_1 L)}\sim \frac{e^{y_1(L - x_2)}}{e^{y_1 L}}\sim e^{-y_1 x_2}
\]

So we can actually get a nice approximate solution.

\section*{Eigenvalues and Eigenfunctions of the Laplacian}

Consider the following \underline{variational} problem:

\[
    \inf_{u\in A} \frac{\int_\Omega \vert \nabla u \vert ^2}{\int_\Omega u^2}
\]

Where \(\Omega \subset \mathbb{R}^n\) is a bounded domain

\(A = \{ u\in L^2(\Omega), \nabla u\in L^2(\Omega), u=0 \text{ on }\partial \Omega , u\not\equiv 0  \} \) 

Question: is the infimum ever achieved? Yes! But we just assume it, not prove.

Assume: this infimum is achieved by some smooth function \(u_1\in A\). Let \(\lambda_1\) be the infimum.

\[
    \lambda_1 \coloneqq \dfrac{\int_\Omega \vert \nabla  \vert ^ 2 }{\int_\Omega u^2}
\]

Also, give the fraction a name, \(E(u)\) 

\underline{Claim:} \(u_1\) solves a PDE.

We use calculus. 

Define: \(f(t)=E(u_1 + tv)\) where \(v\) is \underline{any} fixed element of \(A\).

Note that \(u_1 + tv\in A\) for all \(t\in\mathbb{R}\).

Then \(f(0)\leq f(t)\)  for all \(t\). Thus \(f^{\prime} (0)=0\) 

We compute \(f^{\prime} (t)\).

\[
    \frac{\mathrm{d}}{\mathrm{d}t} \frac{\int \vert ^{\prime} \nabla u_1 + t \nabla v \vert ^2 \, \mathrm{d}x}{\int (u + tv)^2\,\mathrm{d}x }
\]

\[
    =\dfrac{(\int (u_1 + tv)^2)(\int 2 \nabla u_1 \cdot \nabla v + 2t \vert \nabla v \vert ^2)-(\int \vert \nabla u_1 + t \nabla v \vert ^2)(\int 2 u_1 v + 2tv^2)}{\left( \int (u_1 + tv)^2\,\mathrm{d}x  \right) }
\]

When we plug in \(0\) we should get \(0\)

\[
    \dfrac{(\int u_1^2)(\int \nabla u_1\cdot \nabla v)-(\int \vert \nabla u_1 \vert ^2)(\int u_1 v)}{(\int u_1^2)^2} = 0
\]

\[
    \implies \int \nabla u_1 \cdot \nabla v - \boxed{\frac{\int \vert \nabla u_1 \vert ^2}{\int u_1^2}}(\int u_1 v)
\]

The thing inside the box is just \(\lambda_1\) 

\[
    \int_\Omega (\nabla u_1\cdot \nabla v - \lambda_1 u_1 v)\,\mathrm{d}x = 0 
\]

By applying integration by parts, we see that:

\[
    \int_\Omega (- \Delta u_1 - \lambda_1 u_1)v\,\mathrm{d} x = 0
\]

Since \(v\) is arbitrary, the only way this is possible is:

\[
    -\Delta u_1 = \lambda_1 u_1
\]

Note that the function \(E(u)\coloneqq \frac{\int \vert \nabla u \vert ^2}{\int u^2}\) is called the Rayleigh quotient.

\(u_1\) is the first eigenfunction of laplacian with Dirichlet boundary condition. \(\lambda_1\) is the first eigenvalue.

One can characterize the \(k\)-th eigenvalue for \(k=1,2,\cdots\) by:

\(\inf E(u)\) over all \(u\)  such that \(u=0\) on \(\partial \Omega \) and \(\int u u_j \mathrm{d}x = 0\) [\(u\) is perpendicular to all previous eigenfunctions] for all \(j < k\).

We have a sequence \(\lambda_k\) so that \(\lambda_k\to \infty\) and \(\{ u_k \} \) are dense in \(L^2(\Omega)\) meaning there exists constants \(\{ c_k \}_{k=1}^\infty\) such that: \(\vert f(x) = \sum_{k=1}^N c_k u_j(x)  \vert \) goes to \(0\) in \(L^2\) meaning integral of square goes to \(0\).

One implication is, for any smooth enough function \(v\) such that \(v=0\) on \(\partial \Omega \) we have the inequality:

\[
    \frac{\int_\Omega \vert \Delta v \vert ^2}{\int_\Omega v^2}\geq \lambda_1
\]

Rewriting we get \underline{Poincar\'e inequality}.

\[
    \int_\Omega v^2 \leq \frac{1}{\lambda_1}\int_\Omega \vert \Delta v \vert ^2
\]

\section*{Heat Equation on a Bounded Domain}

Suppose \(u_t = \Delta u\) for \(x\in \Omega , t > 0\) 

And consider the Dirichlet Boundary condition: \(u = 0\) for \(x\in \partial \Omega \) and \(t > 0\). Suppose \(u(x,0)=f(x)\) for \(x\in \Omega\) 

In \(\mathbb{R}^n\) we used convolution. That will not work here.

One approach is `seperation of variables'.

Seek a solution:

\(\boxed{u(x,t)=G(x)T(t)}\).

Plugging it in,

\[
    G(x)T^{\prime} (t) = T(t)\Delta G(x) \implies \frac{T^{\prime} (t)}{T(t)} = \frac{\Delta G(x)}{G(x)}
\]

Thus, if such a solution exists, we must have a constant \(-\lambda\) that is equal to the quotient.

\[
    \boxed{T^{\prime} (t) = - \lambda T(t), -\Delta G = \lambda G}
\]

Boundary condition is given by \(G = 0\) on \(\partial \Omega \).

For each positive integer \(k\), we get a solution to the heat equation and the boundary condition in the form:

\(\boxed{u_k(x,t)=e^{-\lambda _k t}u_k(x)}\) where \(u_k\) is the \(k\)-th eigenfunction.

Note that since this is a linear homogeneous differential equation, we can use the superposition principle to get:

\[
    \boxed{u(x,t) = \sum_{k=1}^{\infty} c_k e^{-\lambda _k t}u_k (x)}
\]

Are also solutions to the heat equation, if we suppose this converges and we can take derivative.

For the boundary condition, we choose \(c_k\) so that they converge to \(f\) in \(L^2\).

\hrulefill

Class 19: 02/21

Recall: we have,

\(-\Delta u_j = \lambda _j u_j\) in \(\Omega \) 

\(u_j = 0\) on \(\partial \Omega \) 

\(0 < \lambda_1 < \lambda_2 < \cdots \to \infty\) 

We are skipping a lot about heat equation, namely interesting stuff about mean value theorem, uniqueness, regularity etc.

\section*{Approach to Equilibrium}

If we start with the heat equation: [homogeneous]

\(u_t = \Delta u\) in \(x\in\Omega,t>0\) 

\(u = g(x)\) for \(x\in \partial \Omega, t>0\) 

\(u(x,0)=f(x)\) for \(x\in \Omega \) 

So that \(\Omega \subset \mathbb{R}^n\), open, bounded.

One approach to the question of existence is by the eigenfunction expansion that we get from seperation of variabls. This only applies when the boundary condition is \(0\) though. Something similar can be done though.

\[
    u(x,t) = \sum_j c_j e^{-\lambda_j t}u_j(x)
\]

We can use the maximum principle to show that there is a unique solution. There are other way to do this.

This belongs to a class of differential equations that we call parabolic. They often approach an equilibrium.

Consider \(\displaystyle \lim_{t \to \infty} u(x,t)\). We consider the limit in a \(L^2\) sense.

\underline{Claim}: As \(t \to \infty\), \(u\) approaches an equilibrium solution. The equilibrium \(u_e(x)\) is a function of \(x\), it doesn't depend on time.

Note that, if the claim is true, we should get a \underline{harmonic} function.

\(u_e\) solves \(\Delta U_e = 0\) in \(\Omega \) and \(u_e = g(x)\) on \(\partial \Omega\).

The equilbrium doesn't care about the \(t=0\) condition.

We can always solve this using \underline{Perron's Method}.

\begin{proposition}
    \[
        \lim_{t\to\infty} \int_{\Omega }^{} \vert u(x,t)-u_e(x) \vert ^2 \,\mathrm{d}x = 0
    \]
\end{proposition}

\begin{proof}
    Define \(v(x,t)\coloneqq u(x,t)-u_e(x)\).

    Then \(v_t - \Delta v = u_t - \Delta u - \Delta u_e = 0\)
    
    Boundary Condition:

    \(v(x,t)=g(x)-g(x)=0\) for \(x\in \Omega, t>0\).

    Initial Condition:

    \(v(x,0)=u(x,0)-u_e(x)=f(x)-u_e(x)\) 

    We use energy method.

    \[
        e(t)\coloneqq \int_{\Omega }^{} v(x,t)^2 \,\mathrm{d}x 
    \]

    Then \(e^{\prime} (t) = 2 \int_{\Omega}^{} v v_t \,\mathrm{d}x = 2 \int_{\Omega }^{} v \Delta v \,\mathrm{d}x \) 

    Use IBP: \(= -2 \int_{\Omega }^{} \vert \Delta v \vert ^2 \,\mathrm{d}x + 2\int_{\partial \Omega}^{} v \Delta v\cdot\nu \,\mathrm{d}S \) 

    The second inequality goes to \(0\) as \(t \to \infty\) 

    Recall the Poincar\'e Inequality: If \(v\) is any function vanishing on the boundary of a domain, then \(\int_{\Omega }^{} v^2 \,\mathrm{d}x \leq \frac{1}{\lambda_1} \int_{\Omega}^{} \vert \Delta v \vert ^2 \,\mathrm{d}x \) 

    \[
        e^{\prime} (t) \leq -2 \lambda_1 e(t)
    \]

    \(e^{\prime} + 2\lambda_1 e \leq 0 \implies (e(t) e^{2\lambda _1 t})^{\prime} \leq 0 \implies e(t)e^{2\lambda _1 t}\) is decreasing. So,
    
    \[
        e(t)e^{2 \lambda_1 t} \leq e(0) = \left(\int_{\Omega}^{} \vert f(x) - u_e(x) \vert ^2 \,\mathrm{d}x\right) e^{-2 \lambda_1 t}
    \]

    \[
        \implies \int_{\Omega}^{} \vert u(x,t) - u_e(x) \vert ^2 \,\mathrm{d}x \to 0
    \]

    And it converges exponentially with rate \(2\lambda_1\) 

\end{proof}

\section*{Wave Equation}

We derive the 1d wave equation.

Physically: consider a \underline{vibrating string}. We don't care about the endpoints. Assume the length is infinite.

Let \(u(x,t)\) be the \underline{vertical displacement} of the string at co-ordinate \(x\) and time \(t\). We ignore horizontal displacement.

We use Newton's second law: (Mass)(Acceleration)=Force, or (rate of change of momentum) = Force.

Let \(\rho=\rho(x)\) be the density of the string at coordinate \(x\).

Notation: use \(T\) for \(\vert \vec{T} \vert \) 

If we multiply density with velocity, and integrate it: we get momentum. The rate of change of momentum is the LHS of Newton's law.

\[
    \frac{\mathrm{d}}{\mathrm{d}t} \int_{x_1}^{x_2} \rho(x)u_t(x,t) \,\mathrm{d}x 
\]

RHS is the force. We have the plucking, but assume it already happened. Then the only force remanining is the \underline{Tension}. [There is also gravity, say it's negligible].

We take the vertical components of the tension force. Suppose angle is \(\alpha_1,\alpha_2\)  So, the RHS is:

\[
    - T(x_1,t)\sin \alpha_1 + T(x_2,t)\sin \alpha_2
\]

This is an alphabet soup. We have the assumptions:

\begin{enumerate}
    \item We only consider vertical displacement
    \item We only consider `small' displacement, so \(\alpha_1,\alpha_2\) are small angles. So \(\vert u_x \vert \) isn't too big. So \(T\sin \alpha_1 = T\frac{u_x(x_1,t)}{\sqrt{1 + u_x(x,t)^2} }\approx T u_x(x_1,t)\)
    \item Also assume tension \(T\) in the string is constant.
    \item Assume \(c \coloneqq \sqrt{\frac{T}{\rho}}\) is a constant.
\end{enumerate}

Thus, our equation becomes:

\[
    \int_{x_1}^{x_2} \rho(x)u_{tt}(x,t) \,\mathrm{d}x = - T u_x(x_1,t) + Tu_x(x_2,t) = T \int_{x_1}^{x_2} u_{xx}(x,t) \,\mathrm{d}x 
\]

Therefore, we have:

\[
    \int_{x_1}^{x_2} (\rho u_{t t} - T u_{x x}) \,\mathrm{d}x 
\]

Since the integral is arbitrary, we have:

\[
    u_{tt} - \frac{T}{\rho}u_{x x} = 0
\]

Setting \(c = \sqrt{\frac{T}{\rho}}\), we get the \underline{One Dimensional Wave Equation}:

\[
    \boxed{ u_{t t} - c^2 u_{x x} }
\]

We rewrite the 1d wave equation in the following way using \underline{linear operators}:

\[
    u_{t t} - c^2 u_{x x} = (\partial_t + c \partial_x)(\partial_t - c \partial _{x}) u = 0
\]

Thus, if \(u_t - cu_x = 0\) then \(u\) solves the 1d wave equation. Also, if \(u_t + cu_x = 0\) then \(u\) also solves the 1d wave equation.

Note that \(u_t - cu_x\) is siply the transport equation: if \(F\) is \(u\) at \(t=0\) then \(u = F(x-ct)\). Similarly, the solution to \(u_t + cu_x = 0\) is \(G(x+ct)\). We can take \(F,G:\mathbb{R} \to \mathbb{R}\) arbitrary. One is wave moving to the right with speed \(c\), the other is the wave moving to the left with speed \(c\).

Are we missing any solution? The answer is no!

\hrulefill

Class 20: 02/23

Today we answer whether \(u(x,t)=F(x-ct)+G(x+ct)\) gives us all the solutions.

Let \(\xi = x- ct, \eta = x + ct\) 

Change variables:

\(u(x,t) = U(\xi(x,t),\eta(x,t))\) 

\(u_t = U_\xi \xi_t + U_\eta \eta_t = -cU_\xi + cU_\eta\)

\(u_{tt} = - c U_{\xi\xi}\xi_t - cU_{\xi\eta}\eta_t + c U_{\eta\xi}\xi_t + c U_{\eta\eta}\eta_t\)

\(u_{tt} = c^2 U_{\xi\xi}-2c^2 U_{\xi\eta}+c^2 U_{\eta\eta}\) 

\(u_x = U_\xi\xi_x + U_\eta\eta_x = U_\xi+U_\eta\)

\(u_{x x} = U_{\xi\xi}\xi_x + U_{\xi\eta}\eta_x + U_{\eta\xi}\xi_x + U_{\eta\eta}\eta_x\)

\(u_{x x} = U_{\xi\xi}+2U_{\xi\eta}+U_{\eta\eta}\)

Thus, applying the operator \(\partial_t^2 - c^2 \partial _x^2,\) we get,

\(c^2 U_{\xi\xi} - 2c^2 U_{\xi\eta}+c^2 U_{\eta\eta} -c^2 U_{\xi\xi} - 2c^2 U_{\xi\eta}-c^2 U_{\eta\eta} = 0 \) 

\(\implies U_{\xi\eta} = 0\) 

\(\implies U_\xi = \tilde F(\xi) \implies U(\xi,\eta) = \int \tilde F(\xi) \,\mathrm{d} \xi + G(\eta)\) which was what we wanted.

Now we solve the \underline{cauchy problem} for the 1d wave equation.

\(u_{t t} - c^2 u_{x x} = 0\) for \(-\infty < x < \infty , t > 0\) and \(u(x,0) = f(x)\) 

\underline{This is not enough}. We need some other condition. Physically, we can start with any position, but we can start also with any `velocity'. Meaning we need to specify \(u_t (x,0)\) 

Another idea: consider Taylor's theorem.

\(u(x,t) = u(0,0) + u_x (0,0)x + u_t(0,0)t + \frac{1}{2}u_{x x}(0,0)x^2 + \frac{1}{2}u_{t t}(0,0)t^2 + u_{x t}(0,0)xt\) 

We know \(u(0,)\).

We know \(u_x(0,0)\) by \(f^{\prime} (0)\) 

\underline{We do not know} \(u_t(0,0)\).

That is why we want \(u_t\). Add initial condition \(\boxed{u_t(x,0)=g(x)}\) 

Note that we can get all the Taylor coefficients from derivatives of \(f\) and \(g\).

Recall the \underline{d'Alembert's Formula}:

\(u(x,t) = F(x - ct) + G(x + ct)\)

We want to find \(F,G\) in terms of \(f,g\).

\(u(x,0) = F(x) + G(x) = f(x)\) 

\(u_t(x,0) = -c F^{\prime} (x) + c G^{\prime} (x)=g(x)\) 

Two equations, two unkowns. Lets integrate the second equation:

\(F(x)+G(x) = f(x)\) 

\(-c F(x) + cG(x) = \int_0^x g(s)\,\mathrm{d} s + k \) 

Thus, \(2c F(x) = cf(x) - \int_0^x g(s)\,\mathrm{d} s - k\)

\(F(x) = \frac{1}{2}f(x)-\frac{1}{2c}\int_0^x g(s)\,\mathrm{d} s - \frac{k}{2c}\) 

\(2cG(x) = cf(x) + \int_0^x g(s)\,\mathrm{d} s + k\) 

\(G(x) = \frac{1}{2}f(x) + \frac{1}{2c}\int_0^x g(s)\,\mathrm{d} s + \frac{k}{2c}\) 

Adding, we get:

\(u(x,t)=F(x-ct)+G(x+ct)\)

\[
    =\frac{1}{2}\left[ f(x-ct) + f(x+ct) \right] - \int_{0}^{x-ct} g(s) \,\mathrm{d}s + \int_{0}^{x+ct} g(s) \,\mathrm{d}s
\]

Thus,

\[
    u(x,t) = \boxed{ \frac{1}{2}\left[ f(x-ct) + f(x+ct) \right] + \frac{1}{2c} \int_{x - ct}^{x + ct} g(s) \,\mathrm{d}s  }
\]

Also note the special case: \(g \equiv 0\).

Then,

\[
    u(x,t) = \frac{1}{2}\left[ f(x - ct) + f(x + ct) \right] 
\]

Think of it graphically. Suppose we have a wave. Then if we turn on the time, it is the `average' of the wave \(ct\) units left and \(ct\) units right.

If the wave tapers off at both sides, then we can think of it as \(f\) moving away from the origin at half the amplitude on both sides. Both are moving away at `speed' \(c\).

\(c\) is often called the \underline{wave speed}.

Now consider the the case where \(f,g\) are compactly supported.

Then, suppose \(f,g \equiv 0\) for \(\vert x \vert \geq r\).

That is to say, the string is infinite but we `mess' with it in some finite interval.

If \(x + ct \leq - r\) then \(x - ct \leq -r\).

So, plugging in, everything is outside support, so \(u(x,t) = 0\).

If \(x - ct \geq r\) then \(x + ct \geq r\) 

So, plugging in, everything is outside support, so \(u(x,t) = 0\) 

If we draw a picture in the \((x,t)\) plane with the inequalities, we see that the support grows as time passes, but at any given time the support is contained in a compact region. That is, for all \(t > 0\), the function \(x \mapsto u(x,t)\) has compact support. It is \(0\) outside \([- r - ct, r + ct]\) 

Recall, in heat equation, we had \underline{infinite propagation speed}. But in wave equation, we have \underline{finite propagation speed}.

Wave equation is an example of \underline{hyperbolic} PDEs.

\hrulefill

Class 21: 02/26

Recap:

\underline{1d Wave Equation}:

\(u_{t t} - c^2 u_{x x} = 0\), \(-\infty <x < \infty, t>0, u(x,0)=f(x),u_t(x,0)=g(x)\) for \(-\infty <x < \infty\) gives us \underline{d'Alembert's solution:}

\[
    u(x,t) = \frac{1}{2}\left[ f(x+ct) + f(x-ct) \right] + \frac{1}{2c} \int_{x - ct}^{x + ct} g(s) \,\mathrm{d}s 
\]

Also, in heat equation we had infinite propagation speed. But in wave equation, the \underline{information} is travelling (on both sides) at `speed' \(c\).

Recall that the Cauchy problem for the heat equation \(u_t = \kappa u_{x x}, u(x,0)=f(x)\) had solution:

\[
    u(x,t) = \int_{-\infty}^{\infty} \Phi(x-y,t)f(y) \,\mathrm{d}y 
\]

Since \(\Phi\) is a gaussian, \(u\in C^{\infty}\) for \(t > 0\).

By contrast, for the wave equation, we have \underline{no smoothing}. We need \(f\in C^2,g\in C^1\), otherwise we can't calculate \(u_{t t}\) and \(u_{x x}\). So we need these to obtain a \(C^2\) solution.

\section*{Domain of dependence / Domain of influence}

Pick \(x_0,t_0\). Then \(u(x_0,t_0)\) is defined. The \underline{domain of dependence} is the set of \(x\)-values on which \(u(x_0,t_0)\) depends in term of \(f\) and \(g\). This is a portion of the \(x\)-axis.

We have an explicit formula:

\[
    u(x,t) = \frac{1}{2}\left[ f(x+ct) + f(x-ct) \right] + \frac{1}{2c} \int_{x-ct}^{x+ct} g(s) \,\mathrm{d}s 
\]

Thus, it depends on \(x_0 - ct_0\) and \(x_0 + ct_0\) in terms of \(f\).

It depends on \([x_0 - ct_0, x_0 + ct_0]\) in terms of \(g\).

Thus, our answer is the interval \([x_0 - ct_0, x_0 + ct_0]\).

[draw a picture]

We can draw lines \(x-ct = x_0 - ct_0\) and \(x+ct = x_0 + ct_0\), then our domain of dependence is the interval where these lines intersect the \(x\)-axis.

Consider the reverse prolem: given some subset of the \(x\)-axis, for which points \((x,t)\) is the solution \(u(x,t)\) influenced by \(f\) and \(g\) on this subset?

Take \(S = [a,b]\) the interval. Take a point \(p\in [a,b]\).

Draw the lines \(x-ct = p, x+ct = p\). \(f(p)\) affects \(f\) when \(x-ct = p\) or \(x + ct = p\). So, it influences the points on those lines. It influences the region inside these two lines in terms of \(g\).

As a result, the region bounded by \(x-ct = b, x+ct = a\) gives us the region of influence of \([a,b]\). [insert picture].

Special case: suppose \(g \equiv 0\). Then we have \(V\)'s going up from every \(c\in [a,b]\). As a result, there opens up a \underline{gap} from the rightmost \(V\) and the leftmost \(V\).

\section*{Boundary Value Problem with \(\frac{1}{2}\)-infinite string}

Suppose \(u_{tt} - c^2 u_{xx} = 0\) for \(0 < x < \infty, t>0\) such that \(u(x,0)=f(x),u_t(x,0)=g(x)\). This is incomplete, we need some boundary condition for \(u(0,t)\) 

We can have Dirichlet boundary conditon, \(u(0,t)=0\) for \(0 < t < \infty\).

Think of it as holding a string, we're not letting it vibrate at all.

There are two approaches.

\underline{First approach}:

Any solution of the wave equation must be given by \(u(x,t)=F(x-ct)+G(x-ct)\) for some \(F,G:\mathbb{R} \to \mathbb{R}\).

Note that \underline{d'Alembert's Solution} still works as long as we are plugging in positive numbers. So, if \(x+ct,x-ct>0\) the solution is still valid. Thus, if \(x > ct\), d'Alembert's solution still works.

[insert picture of x=ct]

On the right of the \(x=ct\) line, we have \(x > ct\). In this region, d'Alember'ts solution works.

Also, the formula for \(G\) is still valid outside the region. That is,

\[
    G(x)=\frac{1}{2}f(x) + \frac{1}{2c}\int_{0}^{x} g(s) \,\mathrm{d}s + k
\]

We need a new \(F\).

\(u(0,t)=0\) implies \(F(0-ct)+G(0+ct)=0 \implies F(-ct)=-G(ct)\).

Thus, for any \(y < 0, F(y)=-G(-y)\).

Thus, when \(0 < x < ct\),

\[
    u(x,t) = F(x-ct) + G(x + ct)
\]

\[
    = -G(-x + ct) + G(x + ct)
\]

\[
    = -\frac{1}{2}f(-x + ct) - \frac{1}{2c}\int_{0}^{-x + ct} g(s) \,\mathrm{d}s - k + \frac{1}{2}f(x + ct) + \frac{1}{2c}\int_{0}^{x + ct} g(s) \,\mathrm{d}s + k
\]

\[
    = -\frac{1}{2} f(-x+ct)+\frac{1}{2}f(x+ct) +\frac{1}{2c} \int_{-x + ct}^{x + ct} g(s) \,\mathrm{d}s 
\]

Thus, the solution is given by:

\[
    u(x,t) = \begin{dcases}
        \frac{1}{2}\left[ f(x - ct) + f(x + ct) \right] + \frac{1}{2c} \int_{x - ct}^{x + ct} g(s) \,\mathrm{d}s , &\text{ if } x \geq ct ;\\
        \frac{1}{2}\left[ f(x + ct) - f(-x + ct) \right] + \frac{1}{2c} \int_{-x + ct}^{x + ct} g(s) \,\mathrm{d}s , &\text{ if } x < ct ;
    \end{dcases}
\]

If \(x = 0\) then \(x < ct\), and if we plug it in the formula we get \(0\).

When \(x = ct\), the formula gives us, in both cases,

\[
    \frac{1}{2}[f(0)+f(x+ct)] + \frac{1}{2c} \int_{0}^{x + ct} g(s) \,\mathrm{d}s 
\]

\[
    \frac{1}{2}[f(x)-f(0)] + \frac{1}{2c}\int_{0}^{x + ct} g(s) \,\mathrm{d}s 
\]

Thus, if we want \(u\) to be continuous, we need \(f(0)=-f(0)\) and thus we need \(f(0) = 0\).

The condition \(f(0)=0\) is called a \underline{compatibility condition}. The boundary condition and the initial condition shares a point: \(u(0,0)=0=f(0)\). This is a compatibility condition between the initial and the boundary condition.

We also need to ask the question: does the first derivative have a compatibility condition? By differentiating \(u(x,t)\), one can check that by the continuity of first derivative we need \(g(0)=0\).

\underline{Claim}: One can also check that for continuity of second derivative, one needs \(f^{\prime\prime} (0) = 0\). This is called a \underline{higher order compatibility condition}.

The second approach uses the \underline{odd reflection} trick from the homework.

\hrulefill

Class 22: 02/28

Today we wrap up the discussion on the 1d wave equation.

Second approach to 1d wave equation on the 1/2-infinite string:

Suppose \(u_{t t} - c^2 u_{x x} = 0\) for \(0 < x < \infty, t > 0\) such that \(u(x,0)=f(x), u_t(x,0)-g(x)\) and \(u(0,t)=0\) for \(0 < t < \infty\) 

Trick: extension. If we have a dirichlet boundary condition, we want odd extension since odd functions go thorugh 0 at 0. If we have neumann boundary condition, we want even extension since even functions have derivatives that go through 0 at 0.

We use odd extension here.

Define: \(f_0(x) = \begin{dcases}
    f(x), &\text{ if } 0 \leq x < \infty ;\\
    -f(-x), &\text{ if } - \infty < x < 0 ;
\end{dcases}\) 

\(g_0(0) = \begin{dcases}
    g(x), &\text{ if } 0 \leq x < \infty ;\\
    -g(-x), &\text{ if } - \infty < x < 0 ;
\end{dcases}\) 

Lets assume \(f(0)=0\) and \(g(0)=0\) because otherwise \(f_0\) and \(g_0\) are not continuous.

Strategy: Solve the wave equation.

Let \(\square\) be the differential operator.

\(\square u_0 = 0\) for \(-\infty < x < \infty , t > 0\) via d'Alembert with the initial conditions given by \(f_0\) and \(g_0\).

Claim: \(u_0(x,t)\) is odd. We can prove this via brute force with d'Alembert. More elegant way: define \(v(x,t)=u_0(x,t)+u_0(-x,t)\), then if we prove that \(v\equiv 0\) we're done.

\(u_0(x,t)\) satisfies the wave equation, \(u_0(-x,t)\) also satisfies the wave equation [two \(x\) derivatives gives you two minuses which beomce a plus].

Thus, \(\square v = 0\).

Also, \(v(x,0)=0\) and \(v_t(x,0)\) because the initial conditions are odd and hence become \(0\) for \(v\).

Therefore, \(v\equiv 0\) 

Thus \(u(0,t)=0\) for all \(t\).

We have the solution:

\[
    u(x,t) = \frac{1}{2}\left[ f_0(x-ct) + f_0(x+ct) \right] + \frac{1}{2c} \int_{x-ct}^{x+ct} g_0(s) \,\mathrm{d}s 
\]

Example:

Consider the half infinite case.

Suppose we have no initial velocity: \(g(x)\equiv 0\).

\(f\) is zero except for in the interval \([1,2]\) where it becomes a `bump'.

We can think of it like this: for a while, \(u\) does not know that it is in a half infinite string!

Take the equation:

\(u_{t t} - 4 u_{x x} = 0\) 

\(0 < x < \infty , t > 0\) 

\(u(0,t) = 0\) 

\(u(x,0) = f(x)\) 

\(u_t(x,0) = 0\) 

Now, \(c^2 = 4 \implies c = 2\).

Thus, \(u(x,t) = \frac{1}{2} \left[ f_0(x+2t) + f_0(x-2t) \right]  \) 

Now, think of \(f_0\). It has a `negative bump' on \([-2,-1]\).

Turn on the time. The positive bump splits into half, and one goes to the right with speed \(2\) and one goes to left with speed 2.

Same happens to the `ghost string' on the left.

At \(t = \frac{1}{2}\) we will have:

| negative half bump | nothing | negative half bump | half bump | nothing |half bump

At \(t = 1\) we will have:

| negative half bump | nothing | \underline{positive half bump} | \underline{negative half bump} | nothing | positive half bump

The negative ghost bump becomes real!  The wave gets inverted as it reflects off the boundary.

\section*{n-dim wave equation}

The equation is:

\(u_{t t} - c^2 \Delta u = 0\) 

\(x\in\mathbb{R}^n, t > 0\) 

\(u(x,0) = f(x)\) 

\(u_t(x,0) = g(x)\) 

If \(n = 2\), this is a model of surface waves, eg on a pond.

If \(n = 3\), this is a model of acoustic waves, eg pressure waves like sound. In certain settings, maxwell's equations lead to the wave equation as well.

We use the \underline{Method of Spherical Means}.

Idea: average the function over a sphere to see what happens. It is based on the invariance of the Laplacian under rotation.

\begin{definition}
    For any smooth function \(h:\mathbb{R}^n \to \mathbb{R}\), define the spherical means of \(h\) for a given radius \(r > 0\) :

    \[
        M_h(x,r) = \frac{1}{\omega(n)r^{n - 1}} \int_{\{ y : \vert y - x \vert = r \} }^{} h(y) \,\mathrm{d}S 
    \]

    Recall that \(\omega(n) = \) surface measure of \(S^{n-1}\) 
\end{definition}

We are going to derive a PDE satisfying \(M_h\). We are going to show a relationship between \(h\) derivatives and \(r\) derivatives.

We change variables to take derivatives with respect to \(x\). Define \(y = x + r \xi\) where \(\xi\) lies on the unit sphere, that is \(\vert \xi \vert = 1\)

Then \(\mathrm{d} S_y = r^{n-1}\mathrm{d} S_\xi\) 

Thus, we have:

\[
    M_h(x,r) = \frac{1}{\omega(n)} \int_{\{ \xi : \vert \xi \vert = 1 \}}^{} h(x+r\xi) \,\mathrm{d}S_\xi 
\]

We also extend \(M_h(x,r)\) to be defined for all \(r\) as an even function. At \(r = 0\) we are averaging over a point, so \(M_h(x,0)=h(x)\). If \(r < 0\) then \(M_h(x,r) = M_h(x,-r)\) since we are essentially integrating over the same sphere.

Let's start with \(r\)-derivatives. Let \(\Omega\) be the unit sphere

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{1}{\omega (n)}\int_{\xi\in S^{n-1}}^{} \sum_{i=1}^{n} h_{x_i} (x+r\xi)\cdot\xi_i \,\mathrm{d}S_\xi 
\]

Going back to \(y\) variables,

\[
    r \frac{\partial}{\partial x_i} = \frac{\partial}{\partial \xi_i} 
\]

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{1}{\omega(n)r} \int_{\xi\in S^{n-1}}^{} \sum_{i=1}^{n} h_{\xi_i} (x+r\xi)\cdot\xi_i\,\mathrm{d}S_\xi 
\]

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{1}{\omega(n)r} \int_{\xi\in S^{n-1}}^{} \nabla_\xi h\cdot\nu \,\mathrm{d}S_\xi 
\]

We can apply divergence theorem:

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{1}{\omega(n)r} \int_{\{ \xi: \vert \xi \vert < 1 \} }^{} \Delta_\xi h(x+r\xi) \,\mathrm{d}\xi 
\]

We get two powers of \(r\) from all the double derivatives. Moving to \(x_i\) derivatives:

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{r}{\omega(n)} \int_{\{ \xi : \vert \xi \vert < 1 \} }^{} \Delta_x h(x+r\xi) \,\mathrm{d}\xi 
\]

Taking out the laplacian operator,

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{r}{\omega(n)} \Delta _x \int_{\{ \xi : \vert \xi \vert < 1 \} }^{} h(x+r\xi) \,\mathrm{d}\xi 
\]

Back to \(y\) , we have \(y = x + r\xi\) and thus \(\mathrm{d} y = r^n \mathrm{d\xi} \). Therefore,

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{1}{\omega(n) r^{n-1}} \Delta_x \int_{\{ y : \vert y - x \vert < r \} }^{} h(y) \,\mathrm{d}y 
\]

\hrulefill

Class 23: 03/01

Recall \(n\)-dim wave equation:

\[
    u_{t t} - c^2 \Delta u = 0
\]

For \(x\in\mathbb{R}^n\) and \(t>0\) 

\(u(x,0)=f(x)\) 

\(u_t(x,0)=g(x)\) 

We also defined \underline{spherical means}:

For a function \(h\), we have:

\[
    M_h(x,r) = \frac{1}{\omega_n r^{n-1}} \int_{\{ \vert y - x \vert = r \} }^{} h(y) \,\mathrm{d}S_y 
\]

\[
    M_h(x,r) = \frac{1}{\omega_n} \int_{\vert \xi \vert = 1}^{} h(x+r\xi) \,\mathrm{d}S_\xi 
\]

The latter allows us to take derivative through \(x\) easier and also define this properly for \(r < 0\) 

Last time we found: for any smooth enough \(h:\mathbb{R}^n\to\mathbb{R}\):

\[
    \frac{\partial}{\partial r} M_h(x,r) = \frac{1}{\omega_n r^{n-1}} \Delta_x \int_{\{ y : \vert y - x \vert < r \} }^{} h(y) \,\mathrm{d}y 
\]

RHS can be modified as follows:

\[
    = \frac{1}{\omega _{n}r^{n-1} }\Delta_x \int_{0}^{r} \int_{\vert x - y \vert = \rho }^{} h(y) \,\mathrm{d}S_{y}   \,\mathrm{d}\rho  
\]

\[
    = \frac{1}{\omega_{n-1}} \Delta _x \int_{0}^{r} \frac{\rho^{n-1}}{\omega_n \rho^{n-1}} \int_{\vert y - x \vert = \rho }^{} h(y) \,\mathrm{d}S_y \,\mathrm{d}\rho 
\]

\[
    \frac{1}{\omega_{n-1}}\Delta _x \int_{0}^{r} \rho ^{n-1} M_h(x,\rho) \,\mathrm{d}\rho  
\]

Thus, we have:

\[
    r^{n-1} \frac{\partial}{\partial r} M_h(x,r) = \Delta_x \int_{0}^{r} \rho ^{n-1} M_h(x,\rho ) \,\mathrm{d}\rho  
\]

Therefore, by FTC,

\[
    \frac{\partial}{\partial r} \left[ r^{n-1} \frac{\partial}{\partial r} M_h(x,r) \right] = \Delta _x \left[ r^{n-1} M_h(x,r) \right] 
\]

Thus, by product rule,

\[
    r^{n-1} \frac{\partial^2}{\partial r^2} M_h + (n-1) r^{n-2} \frac{\partial}{\partial r} M_h = r^{n-1} \Delta _x M_h
\]

\[
    \implies \frac{\partial^2}{\partial r^2} M_h + \frac{n-1}{r} \frac{\partial}{\partial r} M_h = \Delta_x M_h
\]

This is called Darboux's equation. Note that RHS is the radial laplacian!

Now, suppose \(h\) is \(u\), the solution of the \(n\)-dimensional wave equation. Then,

\[
    \frac{\partial^2}{\partial r^2} M_u + \frac{n-1}{r} \frac{\partial}{\partial r} M_u = \frac{1}{\omega_n r^{n-1} } \Delta_x \int_{\vert y - x \vert = r}^{} u(y,t) \,\mathrm{d}S_y 
\]

\[
    = \frac{1}{\omega_n} \Delta_x \int_{\vert \xi  \vert =}^{} u(x+r\xi,t) \,\mathrm{d}S_\xi 
\]

\[
    = \frac{1}{\omega_n} \int_{\vert \xi  \vert =}^{} \Delta_x u(x+r\xi,t) \,\mathrm{d}S_\xi 
\]

\[
    = \frac{1}{\omega_n} \int_{\vert \xi  \vert = 1}^{} \frac{1}{c^2} u_{t t} (x+r\xi, t) \,\mathrm{d}S_\xi
\]

\[
    =\frac{1}{\omega_n r^{n-1}} \int_{\vert y - x \vert = r}^{} \frac{1}{c^2} u_{t t}(y,t) \,\mathrm{d}S_y
\]

\[
    =\frac{1}{c^2} \frac{\partial^2}{\partial t^2} M_u(x,r,t)
\]

Thus, we have:

\[
    \frac{\partial^2}{\partial t^2} M_u = c^2 \left[ \frac{\partial^2}{\partial r^2} M_u + \frac{n-1}{r} \frac{\partial}{\partial r} M_u \right] 
\]

This is caled the Euler-Poisson-Darboux equation.

This allows us to solve te equation directly for odd dimension. Take \(n = 3\) and multiply by \(r\) :

\[
    \frac{\partial^2}{\partial t^2} (r M_u) = c^2 \frac{\partial^2}{\partial r^2} (rM_u)
\]

Thus \(r M_u\) satisfies the 1d wave equation, and we can solve this using d'Alembert!

Initial conditions will be \(rM_u(x,r,0)=rM_f\) 

\(\frac{\partial}{\partial t} (rM_u (x,r,0)) = rM_g\) 

Therefore, \(rM_u(x,c,t)=\) 

\[
    \frac{1}{2} [(r+ct) M_f(x,r+ct)+(r-ct)M_f(x,r-ct)] + \frac{1}{2c} \int_{r-ct}^{r+ct} s M_g(x,s) \,\mathrm{d}s
\]

Recover \(u(x,t)\) by taking \(\lim_{r \to 0} M_u(x,r,t)\).

Thus, \(u(x,t)=\) 

\[
    \lim_{r \to 0} \frac{1}{r}\left(\frac{1}{2} \left[ (r+ct) M_f(x,r+ct) + (r-ct) M_f(x,r-ct) \right] + \frac{1}{2c} \int_{r-ct}^{r+ct} s M_g(x,s) \,\mathrm{d}s\right) 
\]

We are going to use l'Hopital. When \(r\to 0\) the first part of the numerator cancels out and the second part is integral of odd function over symmetric interval. So it is indeed \(\frac{0}{0}\). Differentiating the numerator, we have:

\[
    \lim_{r \to 0} \frac{1}{2} \left[ M_f(x,ct) + ct \frac{\partial}{\partial r} \bigg|_{r=0} M_f(x,r+ct) + M_f(x,-ct) - ct \frac{\partial}{\partial r}\bigg|_{r=0} M_f(x,r-ct) \right]
\]

\[
    + \frac{1}{2c} ct M_g(x,ct) - \frac{1}{2c}(-ct) M_g(x,-ct)
\]

Note that,

\[
    \frac{\partial}{\partial r} M_f(x,r+ct) = \frac{1}{c} \frac{\partial}{\partial t} M_f(x,ct), \frac{\partial}{\partial r} M_f(x,r-ct) = -\frac{1}{c} \frac{\partial}{\partial t} M_f(x,-ct)
\]

Therefore,

\[
    u(x,t) = M_f(x,ct) + t \frac{\partial}{\partial t} M_f(x,ct) + t M_g(x,ct)
\]

Note that:

\[
    \frac{\partial}{\partial t} M_f(x,ct) = \frac{\partial}{\partial t} \frac{1}{\omega_n} \int_{\vert \xi \vert = 1}^{} f(x+ct\xi) \,\mathrm{d}S_\xi 
\]

\[
    = \frac{1}{\omega_n} \int_{\vert \xi  \vert = 1}^{} \nabla f(x+ct\xi)\cdot c\xi \,\mathrm{d}S_\xi 
\]

\[
    \frac{1}{\omega_n c^2 t^2} \int_{\vert y - x \vert = ct}^{} \nabla_y f(y) \cdot \frac{y-x}{t} \,\mathrm{d}S_y 
\]

Therefore, \(u(x,t) \) is:

\[
    \frac{1}{4 \pi c^2 t^2} \int_{\{y:\vert y - x \vert = ct\}}^{} \left[ f(y) + t g(y)+\Delta_y f(y)\cdot (y-x) \right]  \,\mathrm{d}S_y 
\]

\pagebreak

Midterm involves:

\begin{enumerate}
    \item Simple Transport: \(u_t +\vec{v_0} \cdot \nabla u = 0\) 
    \item \(\Delta u = 0\) 
    \item \(\Delta u = f\) 
    \item Fourier Transform
    \item \(u_t = \Delta u\) 
    \item \(u_t = \Delta u + F\) 
\end{enumerate}

\hrulefill

Class 24: 03/04

\underline{Midterm Review}

Wednesday: 6:30 PM - 8:00 PM

Rawles Hall 368

Exam will not include the wave equation.

\begin{enumerate}

\item Classification:

    linear vs nonlinear PDE. There are three subcategories of nonlinear - that is not important for midterm.

    But the distinction of linear vs non-linear is important.
\item Simple Transport:

    \(u_t + v_0\cdot \Delta u = 0, v_0\in\mathbb{R} ^n, u(x,0)=f(x)\)

    In homework we had \(u_t + v_0\Delta u+c=0\). Stuff from homework is fair game.

\item Laplace/Poisson:

    There is radial laplacian. Memorize that. Be comfortable with the chain rule: there may be in exam: come up with the radial laplacian ODE.

    \[
        u_{r r} + \frac{n-1}{r}u_r
    \]

    Fundamental Solutions are related to this. If we solve the above eqn = 0 then we get the fundamental solutions. These will be given if needed.

    Newtonian potential: If \(-\Delta u = f\) on \(\mathbb{R}^n\) the solution is \(\Phi \ast f\) 

    \underline{Properties of Harmonic Functions}

    \begin{itemize}
        \item Mean Value Principle. Might be asked to prove this.
        \item Strong Maximum/Minimum Principle for Harmonic Function via the mean value property.
        \item Derivative Estimates: we have estimates of any derivative of harmonic function. Remember that it is in terms of powers of \(r\). How can we bound the first derivative of harmonic functions?
        \item Liouville's Theorem. How to use the derivative estimate to deduce that bounded solutions of PDEs are constant?
        \item Harnack's Inequality: sup is bounded by the inf. No need to remember the proof, but remember the statement and review the related homework problems.
        \item Green's Functions: If we have a fundamental solutions, we can construct green's function \(G\) to solve stuff with boundary conditions.
        
        We take our fundamental solution \(\Phi\). We want to make replace \(\Phi(\vert x - y \vert)\) with \(G(x,y)=\Phi(\vert x - y \vert ) + v(x,y)\), adding a corrector that satisfies some boundary condition but doesn't violate Green's Identity. We need \(v\) harmonic such that \(G\) satisfies some homogeneous [=0] boundary condition. We are primarily focussed on Dirichlet Boundary Conditions.

        Example: Balls/Half Spaces. If needed, Poisson Kernels and Poisson Integal Formula for ball/half space will be given.

        May be asked: given this solution, show that it is the solution. Show that the limit approaches the boundary, the kernel has integral 1 etc.

        \item Fundamental solutions have a singularity in the origin. This is just the right kind of singularity. Review the three limits involving \(\Phi\) integrated against any continuous function over a ball/sphere.
        \item The shell method for integrating over a ball
        \item Divergence theorem: \(\int_{\Omega}^{} \operatorname{div} \vec{F} \,\mathrm{d}x = \int_{\partial \Omega }^{} \vec{F} \cdot \vec{n} \,\mathrm{d}S \)
        
        If \(\vec{F} = u \nabla v\) then we get:

        \[
            \int_{\Omega }^{} u \nabla v \,\mathrm{d}x = - \int_{\Omega}^{} \nabla u \cdot \nabla v \,\mathrm{d}x + \int_{\partial \Omega}^{} u \nabla v \cdot \vec{n} \,\mathrm{d}S 
        \]

        Swap \(u,v\) and subtracting gives us green's equation

        \[
            \int_{\Omega}^{} (u \nabla v - v\nabla u) \,\mathrm{d}x = \int_{\partial \Omega }^{} (u \nabla v \cdot \vec{n} - v \nabla u \cdot \vec{n}) \,\mathrm{d}S 
        \]

        \item We won't be asked to derive the corrector. But we might be asked what boundary condition \(v\) has to satisfy to get an answer. That comes from the Green's equation: If we choose \(u\) to be our solution and \(v\) to be the green's function in the green's equation then we get the constraint.

    \end{itemize}

    \item Heat Equation and Fourier Transforms.
    
    \begin{itemize}

    \item Know the definition of fourier transform: \(\hat{f},\check{f}\) 

    \item Fourier inversion formula: \(\check{\hat{f}}=f\) 
    \item Properties of Fourier trnasform: in multi index notation,

        \[
            \widehat{D^\alpha f} = (iy)^\alpha \hat{f}
        \]
    Proof: Integrate by parts. Know the derivation

    \item Remember the formula: \(\widehat{f * g} = (2\pi)^\frac{n}{2} \hat{f} \hat{g}\). Given if needed.
    
    \item Plancherel identity: \(\int_{}^{} \vert f \vert ^2 \,\mathrm{d}x = \int_{}^{} \vert \hat{f} \vert ^2 \,\mathrm{d}y \) 
    
    \item \(\widehat{e^{- a \vert x \vert ^2}} - \frac{1}{(2a)^{\frac{n}{2}}}e^{-\frac{\vert y \vert ^2}{4a}}\). Given if needed.
    
    \item Solution to \(u_t = \kappa \Delta u, x\in\mathbb{R} ^n, t>0, u(x,0)=f(x)\). If needed, heat kernel will be given: \(\Phi(x,t) = \frac{1}{(4\pi t)^{n / 2}}e^{- \vert x \vert ^2 / 4 \kappa t}\). Question: what do we do with this for the cauchy problem? Convolution. etc.
    \item Properties of the solution. eg Infinite smoothing, explain and proof. Infinite propagation speed. Decay of heat equation [remember homework]. Duhenel's formula will be given if needed for solution to the cauchy problem for inhomogeneous, \(u_t = \Delta u + F(x,t)\). Remember the homework: the trick of subtracting boundary condition and odd/even reflections to obtain \(u(0,t)=0\) or \(u_x(0,t)=0\).

    \end{itemize}

    \item Uniqueness for laplace, poisson, heat equations via: energy method, weak maximum principle.

\end{enumerate}

\pagebreak

Class 25: 03/06

\underline{3D Wave Equation (Cauchy Problem)}

\[
    u_{t t} - c^2 \Delta u = 0
\]

\(x\in\mathbb{R} ^n, t > 0\) 

\(u(x,0) = f(x), u_t(x,0)=g(x)\) 

Solution via \underline{spherical means}

\[
    u(x,t) = \frac{1}{4\pi c^2 t^2} \int_{\{ y: \vert y -x \vert = ct \} }^{} f(y) \,\mathrm{d}S_y + \frac{1}{4 \pi c^2 t} \int_{\{ y: \vert y - x \vert ct \} }^{} g(y) \,\mathrm{d}S_y
\]

\[
    + t \frac{\partial}{\partial t} \left( \frac{1}{4\pi c^2 t^2} \int_{\{ y : \vert y - x \vert = ct \} }^{} f(y) \,\mathrm{d}S_y \right) 
\]

Equivalently,

\[
    u(x,t) = \frac{1}{4\pi  c^2 t^2} \int_{\{ y : \vert y - x \vert = ct \} }^{} \left[ f(y) + t g (y) + \nabla f(y)\cdot (y-x) \right]  \,\mathrm{d}S_y 
\]

Note: ``Loss of Regularity''

Need \(f\) to be \(C^3\) and \(g\) to be \(C^2\) to gurantee that \(u\in C^2\). This is in contrast with previous solution, where even `bad' functions could become infinitely differentiable.

Lets talk about the \underline{cauchy problem in 2d}. We have a `miracle' in odd dimension that allows us to get d'Alembert, but in 2D that doesn't work. But something else works.

2d Wave equation cauchy problem:

\[
    u_{t t} - c^2 \Delta u = 0, x\in \mathbb{R} ^2, t > 0
\]

\[
    u(x,0) = f(x)
\]

\[
    u_t(x,0) = g(x)
\]

We use \underline{Hadamard's Method of Descent}. This is used to take a solution in \(n\) dimension and try to make it to dimension \(n-1\).

So, we use the 3d solution but where \(f\) is a function of \(x_1\) and \(x_2\) [no \(x_3\)] and \(g\) a function of \(x_1\) and \(x_2\) [no \(x_3\)] and note that the solution will be independent of \(x_3\).

Why is this true?

Argue that \(\frac{\partial u}{\partial x_3} \equiv 0 \)

Take \(x_3\) derivative of \(u_{t t} - c^2 \Delta u\) 

we get:

\[
    (u_{x_3})_{t t} - \Delta u_{x_3} = 0
\]

When \(t\) is \(0\),

\[
    u_{x_3}(x_1,x_2,x_3,0) = f_{x_3} \equiv 0
\]

\[
    \frac{\partial }{\partial t} (u_{x_3}(x_1,x_2,x_3,0)) \equiv g_{x_3} \equiv 0
\]

So, with enough regularity \(u_{x_3}\) is a solution to a wave equation with initial and boundary condition \(0\).

So, \(u_{x_3} \equiv 0\).

Since the solution is independent of \(x_3\), make the convenient choice of making \(x_3\) to be \(0\).

Domain of integreation is \(\{ y = (y_1,y_2,y_3): (x_1 - y_1)^2 + (x_2 - y_2)^3 + y_3^2 = c^2 t^2 \} \) 

Since \(f,g\) only depend on \(y_1\) and \(y_2\), the three integrands are symmetric if we replace \(y_3\) with \(-y_3\).

So, we can just integrate over the upper hemisphere and double the answer.

Define \(r = d((x_1,x_2),(y_1,y_2))\) 

\[
    u(x_1,x_2,0,t) = \frac{1}{2\pi c^2 t^2} \int_{y : y_3 = \sqrt{c^2 t^2 - r^2} }^{} \dots \,\mathrm{d}S_y 
\]

We parametrize the upper hemisphere: \(T(y_1,y_2) = (y_1, y_2, \sqrt{c^2 t^2 - r^2} )\) 

Recall:

\[
    \mathrm{d} S = \left\vert \frac{\partial T}{\partial y_1} \times \frac{\partial T}{\partial y_2} \right\vert \mathrm{d} y_1 \mathrm{d} y_2
\]

\[
    = \left\vert \det \begin{pmatrix}
        i & j &  k \\
        1 & 0 &  \frac{x_1 - y_1}{\sqrt{c^2 t^2 - r^2} } \\
        0 & 1 &  \frac{x_2 - y_2}{\sqrt{c^2 t^2 - r^2} } \\
    \end{pmatrix} \right\vert \mathrm{d} y_1 \mathrm{d} y_2
\]

\[
    = \left\vert \left( \frac{y_1 - x_1}{\sqrt{c^2 t^2 - r^2} }, \frac{y_2 - x_2}{\sqrt{c^2 t^2 - r^2} }, 1 \right)  \right\vert 
\]

\[
    = \sqrt{\frac{(y_1 - x_1)^2 + (y_2 - x_2)^2}{c^2 t^2 - r^2} + \frac{c^2 t^2 - r^2}{c^2 t^2 - r^2} } = \frac{ct}{\sqrt{c^2 t^2 - r^2} }
\]

We drop \(x_3\) part since it doesn't matter anymore.

\[
    u(x_1,x_2,t) = \frac{1}{2\pi c^2 t^2} \int_{\{ y: \vert y - x \vert < ct \} }^{} f(y_1, y_2) \,\mathrm{d}S_y + \frac{1}{2\pi c^2 t} \int_{\{ y:\vert y - x \vert < ct \} }^{} g(y_1, y_2) \,\mathrm{d}S_y 
\]

\[
    + t \frac{\partial}{\partial t} \left( \frac{1}{2\pi c^2 t^2} \int_{\{ y : \vert y - x \vert < ct \} }^{} f(y_1,y_2) \,\mathrm{d}S_y \right) 
\]

Consider \(3d\) solution, and take support of \(f,g\) to be the ball \(\{ x\in\mathbb{R}^3 : \vert x \vert < \rho \} \) 

Then, after time \(t\), support \(\{ x: ct - \rho < \vert x \vert < \rho + ct \} \) 

This is ``Sharp Hoygen's Principle'. Support consists of an envelop of spheres.

In 2d we have weak hoygen's principle.

2d is modelling `surface waves', in a pond for example.

Again, choose support to be \(\{x : \vert x \vert < \rho + ct\}\) 

Then support becomes \(\{x:\vert x\vert < \rho+ct\}\) 

Recall in one dimension, we have:

\[
    u(x,t) = \frac{1}{2} \left[ f(x+ct) + f(x-ct)\right] + \frac{1}{2c}\int_{x-ct}^{x+ct} g(s)\,\mathrm{d}s
\]

So it is weak unless \(g\equiv 0\) 

\hrulefill

Class 26: 03/18

Review:

1: 1d wave equation have solution:

\[
    u = F(x+ct)+G(x-ct)
\]

\(\Rightarrow\) d'Alembert's solution to 1d Cauchy Problem

2: 3d wave equation: we solved via \underline{spherical means}.

We have \underline{strong Huygen's principle}.

3: 2d wave equation: we solved via \underline{descent}, look at a 3d solution that doesn't depend on the third variable. We have \underline{weak Huygen's principle}.

Today: Inhomogeneous wave equation (Duhanel's Principle).

Also: Uniqueness / Domain of Influence where we don't explicitly use the formula. This is more general. For example, we can have `variable wave speed'.

Inhomogeneous wave equation:

\[
    u_{t t} - c^2 \Delta u = F(x,t)
\]

For \(x\in\mathbb{R}^n, t>0\) 

\[
    u(x,0) = 0
\]

\[
    u_t(x,0) = 0
\]

for \(x\in\mathbb{R}^n\) 

\underline{Strategy}: let \(U = U(x,t,s)\)

Solve \(U_{t t} - c^2 \Delta U = 0\) for \(x\in\mathbb{R}^n, t>s\)

\(U(x,s,s) = 0\) for \(x\in\mathbb{R}^n\) 

\(U_t(x,s,s)=F(x,s)\) for \(x\in\mathbb{R}^n\) 

To find \(U\) set \(V(x,t,s)\) as solution to \(V_{t t} - c^2 \Delta V = 0\) for \(x\in\mathbb{R}^n, t=0\) with \(V(x,0,s)=0\) and \(V_t(x,0,s)=F(x,s)\) and have \(U(x,t,s)=V(x,t-s,s)\). We can find \(V\) via solution of homogeneous wave equation.

Assume \(F\) is \(C^2\) on \(\mathbb{R}^n \times [0,\infty)\) [Recall that \(F\) is \(C^1\) for \(n = 1\)]

Now, let \(u(x,t) = \int_0^t U(x,t,s)\,\mathrm{d}s\).

Claim: It solves the original equation.

\begin{proof}
    \(u(x,0) = 0\) 
\end{proof}

\(u_t = U(x,t,t) + \int_0^t U_t(x,t,s)\,\mathrm{d}s\) 

Note, first term is \(0\) 

\(u_{t t} = U_t (x,t,t) + \int_0^t U_{t t}(x,t,s)\,\mathrm{d}s\) 

Note, first term is \(F(x,t)\) 

\(\Delta u = \int_0^t \Delta U(x,t)\,\mathrm{d}s\) 

Thus, \(u_{t t} - c^2 \Delta u = F(x,t) + \int_0^t (U_{t t}-c^2\Delta U)\,\mathrm{d}s = F(x,t)\) 

Example: \(n = 1\) 

We want to solve \(u_{t t} - c^2 u_{x x} = F(x,t)\) with initial data \(u(x,0)=0, u_t(x,0)=0\) 

Using d'Alembert, \(U(x,t,s)=V(x,t-s,s)\) so:

\[
    u(x,t) = \int_0^t \frac{1}{2c}\int_{x - c(t-s)}^{x+c(t-s)} F(y,s)\,\mathrm{d}y\,\mathrm{d}s
\]

Domain of Dependence/Influence: 

The \(y\) values that matters are from \(x_0 + c(t_0-s)\) and \(x - c(t_0-s)\) where \(s\) varies from \(0\) to \(t_0\).

One extreme: \(x_0 - ct_0\) to \(x_0 + ct_0\) 

Other extreme: \(x_0\) to \(x_0\)

We keep integrating between, so we get the whole `triangle'.

This is consistent with the fact that we have propagation speed \(c\).

\hrulefill

Class 27: 03/20

Today we will talk about:

Energy associated with the wave equation.

Lets go back and talk about \underline{domain of influence}. In \(\mathbb{R}^n\times (0,\infty)\)  with the latter being time,

\[
    u_{t t} - c^2 \Delta u = 0
\]

\(u(x,0)=f(x),u_t(x,0)=g(x)\) 

At any time [we take a slice of time], which \(x\)'s are influenced by the value of \(f\) and \(g\) at \(x_0\)?

It's the `light cone', think about a cone centered at \(x_0\) and spreading out with \(ct\). Domain of influence is the set of points sitting inside the cone.

So, domain of influence of \(x_0\) is:

\[
    \{ (x,t):\vert x - x_0 \vert = ct \} \text{ [in 3d, so we don't have interior]} 
\]

In 2d, we need the interior

\[
    \{ (x,t): \vert x - x_0 \vert \leq ct \} 
\]

[draw pictures of cones]

Now lets talk about \underline{energy}.

There's a `physical' energy associated to it. We can think about it as energy = kinetic + potential energy. It is given by:

\[
    E(u,\Omega)\coloneqq \int_{\Omega}^{} \left[ \frac{1}{2} u_t^2 + \frac{c^2}{2} \vert \nabla_x u \vert ^2 \right] \,\mathrm{d}x 
\]

Asssume \(f\) and \(g\) are compactly supported.

For \(t > 0\), our explicit formulas for \(u(x,t)\) imply that \(u(-,t)\) is also compactly supported for any fixed \(t\).

Let's compute \(\frac{\mathrm{d}E(u,\mathbb{R}^n)}{\mathrm{d}t} \)

\[
    \frac{\mathrm{d}}{\mathrm{d}t} \int_{\mathbb{R}^n}^{} \frac{1}{2} u_t^2 + \frac{c^2}{2} \vert \nabla u \vert ^2 \,\mathrm{d}x 
\]

Note, \(E(u,\mathbb{R}^n) < \infty\) for each \(t > 0\) because compact support

\[
    = \frac{\mathrm{d}}{\mathrm{d}t} \int_{B_R}^{} \frac{1}{2} u_t^2 + \frac{c^2}{2} \vert \nabla u \vert ^2 \,\mathrm{d}x 
\]

where \(R\) is big enough to contain the support of \(U(-,t)\) 

Taking the derivative inside the integral sign,

\[
    = \int_{B_R}^{} u_t u_{t t} + \frac{c^2}{2} 2 \nabla u\cdot\nabla(u_t) \,\mathrm{d}x 
\]

\[
    = \int_{B_R}^{} u_t u_{t t} \,\mathrm{d}x - c^2 \Delta u u_t + \int_{\partial B_R}^{} c^2 \nabla u\cdot\nu u_t \,\mathrm{d}S 
\]

We can drop the boundary term because compact support.

From the integral we can factor \(u_t\) to get wave equation

\(= 0\) 

Thus, energy is conserved.

Heat equation is a `dissipative' PDE since energy dissipates. Wave equation is a `conservative' PDE.

\begin{proposition}
    Assume \(u\) solves \(u_{t t} - c^2 \Delta u = 0, x\in\mathbb{R}^n, t>0\) and suppose the initial displacement and initial velocity \(u(x,0)=0\) and \(u_t(x,0)=0\) in a ball \(B(x_0,R)\subset \mathbb{R}^n\).  

    Then \(u(x,t) = 0\) inside the cone \(C = \left\{ (x,t):\vert x - x_0 \vert < c \left( \frac{R}{c} - t \right)  \right\} \) 
\end{proposition}

\begin{proof}
    Consider:

    \[
        E(u,B(x_0, c(R / c - t))) = E(t) = \int_{B(x_0,c(R / c - t))}^{} \frac{1}{2} u_t^2 + \frac{c^2}{2} \vert \nabla u \vert ^2 \,\mathrm{d}x 
    \]

    Note that \(E(0)=0\) 

    To compute it, we just use the shell method

    \[
        \frac{\mathrm{d}E}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t} \int_{0}^{c(R / c - t)} \int_{\partial B(x_0,r)}^{} \frac{1}{2} u_t^2 + \frac{c^2}{2} \vert \nabla u \vert ^2 \,\mathrm{d}S  \,\mathrm{d}r 
    \]

    \[
        = -\frac{c}{2} \int_{\partial B(x_0, c(R / c - t))}^{} u_t^2 + c^2\vert \nabla u \vert ^2 \,\mathrm{d}S + \int_{B(x_0, c(R / c - t))}^{} u_t u_{t t} + c^2 \nabla u\cdot\nabla(u_t) \,\mathrm{d}x 
    \]

    \[
        =-\frac{c}{2} \int_{\partial B(x_0, c(R / c - t))}^{} u_t^2 + c^2\vert \nabla u \vert ^2 \,\mathrm{d}S + \int_{B(x_0,c(R / c - t))}^{} u_t(u_{t t} - c^2 \Delta u) \,\mathrm{d}x 
    \]

    \[
        + \int_{\partial B(x_0, c(R / c - t))}^{} c^2 \nabla u\cdot\nu u_t \,\mathrm{d}S 
    \]

    the second integral is just \(0\) 
    
    Also, note that \(c\vert (c\nabla u\cdot\nu)u_t \vert \leq c(\frac{c^2}{2}(\nabla u\cdot\nu)^2 + \frac{1}{2} u_t^2) \leq \frac{c}{2}(c^2\vert \nabla u \vert ^2 +\frac{1}{2} u_t^2)\) 

    since \(ab \leq \frac{1}{2}a^2 + \frac{1}{2}b^2\) 

    Thus \(\frac{\mathrm{d}E}{\mathrm{d}t} \leq 0\) 

    Thus \(E(t) \equiv 0\) which means \(u\equiv 0\) in \(C\).  

\end{proof}

To finish, we discuss Elliptic and Hyperbolic operators.

Elliptic: Laplace operator

Hyperbolic: wave operator

Recall: laplace operator is:

\(u_{x_1 x_1} + \cdots + u_{x_n x_n}\)

Replace \(\frac{\partial}{\partial x_i}\) with \(\lambda_i\)

So, the principal symbol is:

\(\lambda_1^2 + \cdots + \lambda_n^2\) 

For wave operator, the principal symbol is:

\(u_{x_{n+1} x_{n+1}} - c^2 (u_{x_1 x_1} + \cdots + u_{x_n x_n})\) to

\(\lambda_{n+1}^2 - (\lambda_1^2 + \cdots + \lambda_n^2)\) 

If principle symbol (polynomial) only vanishes for \(\vec{\lambda} = 0\) then it is elliptic

If polynomial has non-zero solutios then it is hyperbolic.

We also have parabolic: if \(u_t =\) elliptic operator then it is called parabolic (heat equation).

\hrulefill

Class 28: 03/22

\section*{First Order PDEs}

\underline{1st Order Quasilinear PDE} 

Let \(X = (X_1,\cdots,X_n)\)

Then general quasilinear PDE looks like

\[
    a_1(x,u)u_{x_1}+a_2(x,u)u_{x_2}+\cdots+a_n(x,u)u_{x_n} = c(x,u)
\]

Specify `initial condition' \(u=f(x)\) for \(x\in \Gamma\) [a hypersurface, so \(n-1\) dimension].

Here \(f:\Gamma \to \mathbb{R}\) 

General Method:

\underline{Method of Characteristics};

[We are somewhat deviating from Evans here, in notation for example]

\underline{Key idea}: View \(\sum a_j (x,u) u_{x_j} = \nabla u \cdot \vec{a}\) as a directional derivative along curves.

\underline{Examples}:

1: Suppose \(n = 2\). Instead of writing \((x_1,x_2)\) write \((x,y)\). We solve:

\[
    a_1 u_x + a_2 u_y = 0
\]

where \(a_1,a_2\) are constant.

Let \(\Gamma\) be the \(x\)-axis.

Then \(u(x,0)=f(x)\)

PDE is equivalent to:

\[
    \nabla u\cdot (a_1,a_2) = 0
\]

So the directional derivative at \((a_1, a_2)\) is \(0\). Informally, it doesn't change in that direction.

Let's parametrize a curve along which PDE becomes an ODE by \(\tau\).

We also parametrize initial curve \(\Gamma\) by \(s\)

Let \(\frac{\partial x}{\partial \tau} = a_1, \frac{\partial y}{\partial \tau} = a_2 \)

So the slope is given by vector \(\vec{a}\).

Now parametrize \(\Gamma\): 

\(x(s,0)=s,y(s,0)=0\) 

Then,

\[
    \frac{\partial u}{\partial \tau} = \left( \frac{\partial u}{\partial x} \frac{\partial x}{\partial \tau} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial \tau} \right) = \nabla u \cdot \vec{a} = 0 
\]

And \(u(s,0)=f(s)\) 

This is called the \underline{characteristic system}.

It is a system of ODEs.

Solve to find \(x(s,\tau),y(s,\tau),u(s,\tau)\).

This is not the final solution, we want it in terms of \(x,y\).

\(\frac{\partial x}{\partial \tau} = a_1, x(s,0)=s\) so for solving we can just integrate.

\(x(s,\tau) = a_1\tau + c_1(s)\) 

\(x(s,0)=s=c_1(s)\) 

So \(x(s,\tau)=a_1\tau + s\) 

Similarly calculating, \(y(s,\tau)=a_2\tau\) 

Thus, \(u(s,\tau)=f(s)\)

Now we want to revert to the original variables.

\(x = a_1\tau + s, y=a_2\tau\).

Solving, see that: \(s = s(x,y) = x - \frac{a_1}{a_2}y\) and \(\tau = \tau(x,y) = \frac{y}{a_2}\)

So, \(u(x,y)=f(s(x,y))=f(x-\frac{a_1}{a_2})y\) 

2: \(x u_x + 2 u_y = 1\) 

\(u(x,0)=f(x)\) 

(\(n = 2\), \(\Gamma = x\)-axis)

\(a_1 - x, a_2 = 2\)

\(\nabla u \cdot (x,2) = 1\) 

So, system:

\[
    \frac{\partial x}{\partial \tau} = x
\]

\[
    \frac{\partial y}{\partial \tau} = 2
\]

\[
    \frac{\partial u}{\partial \tau} = 1
\]

\(x(s,0)=s,y(s,0)=0,u(s,0)=f(s)\) 

Now, solving the ODEs,

\(x_\tau = x \implies x(s,\tau)=c(s)e^\tau\), setting \(\tau=0, x(s,0)=s=c(s) \implies x(s,\tau) = s e^\tau\)

\(y_\tau=2\implies y(s,\tau)=2\tau+c(s)\) so \(y(s,\tau)=2\tau\)  

So, \(u(s,\tau)=\tau + f(s)\) 

Now invert:

\(\tau = \frac{y}{2}\) and \(s = x e^{-\tau} = x e^{-y / 2}\)

So, \(u(x,y) = \tau + f(s) = \frac{y}{2} + f \left( x e ^{- \frac{y}{2}} \right) \) 

Sketch the characteristic projections:

\(\tau = (x(s,\tau),y(s,\tau))\) 

\(s = x e^{-\frac{y}{2}}\)

\(x =s e^{\frac{y}{2}}\)

So we can draw the graph \(x = s e^{\frac{y}{2}}\) for each \(s\).

[insert picture]

3: \(x u_x + 2 u_y = u^2\) 

\(u(x,0)=f(x)\) 

We jump right intocharacteristic system

\(\frac{\partial x}{\partial \tau} = x, x(s,0)=s\)

\(\frac{\partial y}{\partial \tau} = 2, y(s,0)=0\) 

\(\frac{\partial u}{\partial \tau} = u^2, u(s,0)=f(s)\) 

As before,

\(x(s,\tau)= s e^\tau\) 

\(y(s,\tau) = 2\tau\) 

\(\frac{\partial u}{\partial \tau} = u^2\) is seperable ODE so

\(\int \frac{1}{u^2}\,\mathrm{d} u = \int \mathrm{d} \tau\)

\(- \frac{1}{u} = \tau + c(s)\) 

So \(-\frac{1}{f(s)} = c(s)\)

\(- \frac{1}{u} = \tau - \frac{1}{f(s)}\)

\(\frac{1}{u} = \frac{1-\tau f(s)}{f(s)}\)

So, \(u(s,\tau)=\frac{f(s)}{1-\tau f(s)}\) 

So, \(u(x,y) = \frac{f(x e^{- \frac{y}{2}})}{1 - \frac{y}{2} f(x e^{-\frac{y}{2}})}\) 

Note, solution might blow up as \((x,y)\) approaches the set of points where \(\{ (x,y): \frac{y}{2}f(x e^{-\frac{y}{2}}) = 1 \} \) 

\hrulefill

Class 29: 03/25

\underline{Example}:

\[
    x u_x + (x^2 + y)u_y + (\frac{y}{x} - x)u = 1
\]

\(u = 1\) on the line \(x = 1\)

Again, we want to treat it as an ODE

\(\frac{\partial x}{\partial \tau} = x\)

\(\frac{\partial y}{\partial \tau} = x^2 + y\)

\(\frac{\partial u}{\partial \tau} + \left( \frac{y}{x} - x \right) u = 1 \) 

Initial condition: \(x(s,0) = 1\) and \(y(s,0) = s\) and \(u(s,0) = 0\) 

Characteristic System

Equation for \(x\) decouples

\(\implies \)

\(x(s,\tau) = c_{1} (s) e^\tau \) 

\(x(s,0)=1 = c_1(s)\)

\(x(s,\tau) = e^\tau\) 

Substitute into ODE for \(y\) 

\(\frac{\partial y}{\partial \tau} = e^{2\tau} + y\)

\(\frac{\partial y}{\partial \tau} - y = e^{2\tau}\)

\((y e^{\tau})^{\prime} = e^\tau\)

\(y e^{-\tau} = e^\tau + c_2(s)\) 

\(y(s,t) = e^{2\tau} + c_2(s)e^\tau\)

\(y(s,0)=s \implies s = 1+c_2(s)\)

\(y(s,\tau) = (s-1)e^{\tau} + e^{2\tau}\) 

\(\frac{\partial y}{\partial \tau} = (s-1)e^\tau + 2 e^{2\tau}\) 

Characteristic projection:

\(y = (s-1)x + x^2\)  

Parabola with vertex: \(\left( \frac{1-s}{2}, - \frac{(1-s)^2}{4} \right) \) 

Note that the parabola always passes through \((0,0)\).

For different values of \(s\) we get a foliation of the plane.

Now solve ODE for \(u\) 

\[
    \frac{y}{x} - x = \frac{(s-1)e^\tau + e^{2\tau}}{e^\tau} - e^\tau = (s-1)
\]

\[
    \frac{\partial u}{\partial \tau} + (s-1) u = 1
\]

\[
    (u e^{(s-1)\tau})_\tau = e^{(s-1)\tau}
\]

Integrate

\[
    u e^{(s-1)\tau} = \frac{e^{(s-1)\tau}}{s-1} + c_3(s)
\]

\[
    u(s,0) = 0 \implies 0 = \frac{1}{s-1} + c_3(s)
\]

\[
    u(s,\tau) = \boxed{ \frac{1}{s-1} - \frac{1}{s-1} e^{(1-s)\tau} }
\]

for \(s \neq 1\)

When \(s=1, \frac{\partial u}{\partial \tau} = 1\)

So, \(u(s,\tau) = \tau\) 

Revert to \(x\) and \(y\) 

\[
    \boxed{u(x,y) = \frac{x}{y - x^2} - \frac{x}{y-x^2} e^{\left( x - y / x \right) \ln x}}
\]

When \(y \neq x^2\). When \(y=x^2\),

\[
    \boxed{u(x,y) = \ln x}
\]

Only defined for \(x > 0\) 

\section*{General Problem}

First Order quasi-linear

\[
    a_1(x,u) u_{x_1} + \cdots + a_n(x,u) u_{x_n} = c(x,u)
\]

\(u(x) = f(x)\) when \(x\in \Gamma \)

\(\Gamma =\) hypersurface in \(\mathbb{R}^n\)  

\(x = (x_1,\cdots, x_n)\) 

\underline{Characteristic System}:

\[
    \frac{\partial x_1}{\partial \tau} = a_1(x,u)
\]

\[
    \vdots
\]

\[
    \frac{\partial x_n}{\partial \tau} = a_n(x,u)
\]

\[
    \frac{\partial u}{\partial \tau} = c(x,u)
\]

plus initial conditions depending on \(s\) 

Where \(s = (s_1,\cdots, s_{n-1})\) is used to parametrize \(\Gamma\)

We need theory for first order (nonlinear) systems of ODE's depending on parameters

\begin{theorem}
    Given a system

    \[
        \frac{\partial y}{\partial \tau} = g(\tau, y)
    \]

    \[
        y(s,0) = h(s)
    \]

    where \(g: \mathbb{R} \times \mathbb{R}^k \to \mathbb{R}^k\)
    
    \(y = y(s,\tau)\) maps \(\mathbb{R} ^l \times \mathbb{R} \to \mathbb{R} ^k\)
    
    \(h: \mathbb{R}^l \to \mathbb{R}^k\)
    
    assume \(g\) is \(C^1\) in \(\tau\) and \(y\) 

    and \(h\) is \(C^1\) 

    Then \(\forall s_0\in\mathbb{R}^l, \exists \tau_0 > 0\) and \(\exists \overline{s} > 0\) such that a unique \(C^1\) solution \(y = y(s,\tau)\) exists to this problem provided \(\tau < \tau_0\) and \(\vert s - s_0 \vert < \overline{s} \) 

\end{theorem}

That is: \(g,h\) \(C^1\) implies \(\exists\) unique local solution.

We can change \(C^1\) to \(C^{\infty}\) and we still have only a local solution. The solution we cook up can blow up after a little while.

local is the best we can hope for (as a general theorem). Classic example: \(y^{\prime} = y^2, y(0)=1\)

So, \(- \frac{1}{y} = \tau + c, -1 = c\) so \(y = \frac{1}{1-\tau}\) so the solution blows up at \(\tau = 1\) 

\underline{Idea of existence} 

Suppose \(\frac{\partial y}{\partial \tau} = g(y)\)

Rewrite: \(y(\tau) = \int_{0}^{\tau} g(y(t)) \,\mathrm{d}t + y_0 = M(y)\)

Then this becomes a problem of finding a fixed point of \(M\).

We want a function \(y\) such that \(M(y) = y\)

Show this with \underline{contraction mapping}

Let \(\mathcal{S}=\left\{ y \in C([0,\tau_0];\mathbb{R}^k): \sup_{0 \leq \tau \leq \tau_0} \vert y(\tau) - y_0 \vert_{\mathbb{R}^k} \leq M \right\} \) 

To use contraction mapping:

show: \(M:S\to S\)

show \(M\) is a contraction: \(|M(y_2)-M_(y_1)| \leq \lambda \vert y_2 - y_1 \vert \) 

when \(\lambda < 1\)

provided \(\tau\) sufficiently small.

\hrulefill

Class 30: 03/27

1st order quasi-linear PDE

\(n\) variables [\(x = (x_1,\cdots,x_n)\) ]

\[
    a_1(x,u)u_{x_1} + a_2(x,u)u_{x_2} + \cdots + a_n(x,u)u_{x_n} = c(x,u)
\]

\[
    u(x) = f(x) \text{ for } x\in \Gamma
\]

\underline{Assumptions}:

\(\forall j\in \{ 1,\cdots,n \}, a_j(x,z)\) is \(C^1\) in a neighborhood of \((x,z):x\in \Gamma, z=f(x)\)

\(c(x,z)\) is \(C^1\) in this neighborhood as well.

\(f: \Gamma \to \mathbb{R}\) is \(C^1\) 

\(\Gamma\) is a \((n-1)\)-dimensional hypersurface admitting a \underline{regular parametrization}; that is: \(\forall x_0\in \Gamma, \exists R > 0\) such that \(\Gamma \cap B(x_0,R)\) can be described as

\[
    \Gamma \cap B(x_0,R) = \{ h(s):s\in D \} 
\]

where \(D \subset \mathbb{R}^{n-1}\), open

\(s = (s_1,\cdots,s_{n-1}), h:D\to\mathbb{R}^n\)

given by \(h(s)=(h_1(s),\cdots,h_n(s))\in C^1\) 

with \(\frac{\partial h}{\partial s_1}, \cdots, \frac{\partial h}{\partial s_{n-1}}\) being linearly independent.

Note: our solution is a graph in \((n+1)\)-dimensions.

\(\frac{\partial h}{\partial s_1},\cdots,\frac{\partial h}{\partial s_{n-1}}\) form a basis for the tangent plane.

[We are going to need one more assumption for later].

\underline{Method of Characteristics}

1. Solve the characteristic system of ODEs in new variables \(s\in\mathbb{R}^{n-1},\tau\in\mathbb{R}\)

2. Revert back to \(x_1,\cdots,x_n\)

For each \((x,f(x))\) for \(x\in \Gamma\) we need to find a `curve', weaving them together gives us the solution in whole plane. 

The whole idea is based on considering LHS as a directional derivative.

When we are considering \(x\) as a function, we will use \(X\).

Then, we have the \underline{Characteristic System} 

\[
    \frac{\partial X_1}{\partial \tau} = a_1(X,U) \quad X_1(s,0)=h_1(s)
\]

\[
    \vdots
\]

\[
    \frac{\partial X_2}{\partial \tau} = a_n (X,U) \quad X_n(s,0)=h_n(s)
\]

\[
    \frac{\partial U}{\partial \tau} = c(X,U)\quad U(x,0)=f(h(s))
\]

Step 1: By existence and uniqueness for \(C^1\) ODE system, \(\forall s_0\in D, \exists R > 0, \exists \delta >0\) such that there exists a unique solution \(X(s,\tau),U(s,\tau)\) [where \(X = (X_1,\cdots, X_n)\)] for \(\vert s - s_0 \vert < R, \vert \tau \vert < \delta\) 

Step 2: We want to invert the change of variables.

\((\underbrace{s}_{\in\mathbb{R}^{n-1}},\underbrace{\tau}_{\in\mathbb{R}}) \to X(s,\tau)\) is a \(C^1\) map from \(\mathbb{R}^n \to \mathbb{R}^n\)

This inversion is possible (locally) provided the Jacobian at \(s=s_0,\tau=0\) is non-zero.

Need:

\[
    \det \begin{bmatrix}
        \mid & \mid &  & \mid &  \mid \\
        \frac{\partial X}{\partial s_1} (s_0,0) & \frac{\partial X}{\partial s_2} (s_0,0) & \cdots & \frac{\partial X}{\partial s_{n-1}} (s_0,0) & \frac{\partial X}{\partial \tau} (s_0,0)  \\
        \mid & \mid &  & \mid & \mid  \\
    \end{bmatrix} \neq 0
\]

Equivalently,

\[
    \det \begin{bmatrix}
        \mid & \mid &  & \mid &  \mid \\
        \frac{\partial h}{\partial s_1} (s_0) & \frac{\partial h}{\partial s_2} (s_0) & \cdots & \frac{\partial h}{\partial s_{n-1}} (s_0)  &  a_j(h(s),f(h(s))) \\
        \mid & \mid &  & \mid & \mid  \\
    \end{bmatrix} \neq 0
\]

So, we need the additional assumption:

\(\begin{bmatrix}
     a_1(h(s),f(h(s))) \\
     \vdots \\
     a_n(h(s),f(h(s))) \\
\end{bmatrix}\) does not lie on the tangent plane.

Note that, this is an \underline{overdetermined} problem. We have initial data, and then the PDE tells us how stuff moves from that initial data. But the two things need not agree! Having the determinant non-zero, that is, not having this at the tangent space lets us avoid the disagreement.

At \(s=s_0\), then we say \(\Gamma\) and \(f\) are \underline{non}-\underline{characteristic} at \(x_0 = h(s_0)\)

Note: if the PDE is linear or semi-linear, then the condition of being non-characteristic depends only on \(\Gamma\).

\begin{theorem}
    Under all these assumptions, there exists a unique solution to the first order quasi-linear PDE in a neighborhood of every point in \(\Gamma\). 
\end{theorem}

\hrulefill

Class 31: 03/29

Today we talk about the proof of the theorem.

\begin{theorem}
    \(\forall x_0\in \Gamma , \exists r_0 > 0\) such that the formula obtained via the method of characteristics yields a \(C^1\) solution. 
\end{theorem}

\begin{proof}
    Recall: \(X_{1_\tau}=a_1(X,V),\cdots,X_{n_\tau}=a_n(X,V)\)

    \(U_\tau = c(X,u),U(s,0)=f(h(s))\) 
    
    \(X_1(s,0)=h_1(s),\cdots,X_n(s,0)=h_n(s)\) 

    \(\exists X(s,\tau)\) solving this system \(\forall \vert s - s_0\vert < \overline{s} , \vert \tau \vert < \tau_0 \) 
    
    Non-charactteristic condition:

    \(\exists C^1\) (local) inverse \(S(x),T(x)\)
    
    Claim: \(u(x)\coloneqq U(X ^{-1} (x))=U(S(x),T(x))\)
    
    solves the PDE and the initial condition.

    2 approaches:

    1. Brute force

    Ex: \(n=2\) 

    \(u(x_1,x_2)=U(s(x_1,x_2),\tau(x_1,x_2))\) 

    \(u_{x_j}=U_s s_{x_j}+U_\tau\tau_{x_j}\) 

    \(j=1,2\) 

    \(x_i = X_i(s(x_1,x_2)\tau(x_1,x_2))\) 

    taking \(\frac{\partial}{\partial x_j}\)
    
    \(\delta_{ij}=X_{i_s}\boxed{s_{x_j}}+X_{i_\tau}\boxed{\tau_{x_j}}\) 

    taking the boxed as unknown we have 4 equations 4 unknowns, solving and plugging it into the PDE gives us the answer.

    \underline{Good Geometric Approach}:

    Rewrite PDE:

    \[
        (a_1(x,u),\cdots,a_n(x,u),c(x,u))\cdot (\nabla u, -1)
    \]

    \((\nabla u, -1)\) is a normal to the graph of \(u\) [\((x,z):u(x)-z=0\)]

    Geometrically, the PDE is solvable if the graph has this property.

    Propososed solutionis is given in two ways.

    non-parametrically via \(u(x) = U(X ^{-1} (x))\) 

    parametrically via \((s,\tau) \mapsto (X(s,\tau),U(s,\tau))\)
    
    Fixing any \(s\) in our parameter domain.

    \(\tau \mapsto (X(s,\tau),U(s,\tau))\) is a curve on the surface. 

    So, \(X_\tau, U_\tau\) is a tangent vector in a tangent plane.

    Thus, \((a_1(x,u),\cdots, a_n(x,u),c(x,u))\) is a tangent vector.

    So it is orthogonal to the normal.

    So it solves the PDE.
    
\end{proof}

\begin{theorem}
    The solution is unique.
\end{theorem}

\begin{proof}
    Let \(v\) be any solution. Fix any \(\tilde{s}\) near \(s_0\).

    Let \(w(\tau)\coloneqq U(\tilde{s},\tau)-v(X(\tilde{s},\tau))\)
    
    \(w(0)=U(\tilde{s},0)-V(X(\tilde{s},0))\) 

    \(= f(h(\tilde{s}))-v(h(\tilde{s}))\) 

    If \(v\) is a solution it satisfies initial condition \(f\) 

    \(=f(h(\tilde{s}))-f(h(\tilde{s}))=0\)
    
    So \(w(0)=0\) 

    Now we show that \(w_\tau=0\) 

    \(w_\tau = U_\tau(\tilde{s},\tau)-\nabla v(X(\tilde{s},\tau))\cdot X_\tau\) 

    \(= c(X(\tilde{s},\tau),U(\tilde{s},\tau))-\sum_{j=1}^n a_j(X(\tilde{s},\tau),U(\tilde{s},\tau))\cdot v_{x_j}(X)\) 

    Since \(v\) is the solution to the PDE this has to be \(0\) 

    So \(w_\tau = 0\)
    
    So \(w\) is \(0\) 

    So the solution is unique.
    
\end{proof}

Example: \(u_x - 2u_y = 0\)

\(u=0\) on \(\Gamma = (x,y):y=1-x^2\)

Char System:

\(x_\tau = 1\) and \(x(s,0)=s\) 

\(y_\tau = -2\) and \(y(s,0)=1-s^2\) 

\(u_\tau = 0\) and \(u(s,0)=0\) 

Characteristic (projections) are lines with slope \(-2\) since \(\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{y_\tau}{x_\tau}=-2\) 

Note: at \((1,0)\) there's trouble: tangent:

So \(\det \begin{pmatrix}
    1 &  1 \\
    -2s &  -2 \\
\end{pmatrix}=-2+2s\) so we fail the inverse function theorem when \(s=1\) aka at \((1,0)\) 

So, we are not guranteed an inverse.

We may luck out, we may have trouble.

Claim: no solution valid in the neighborhood of \((1,0)\) 

Initial condition: \(u(x,1-x^2)=0\) 

take derivative w.r.t.\ \(x\) 

\(u_x(x,1-x^2)+u_y(x,1-x^2)(-2x)=0\) 

when \(x=1\),

\(u_x(1,0)-2u_y(1,0)=0\) 

Actually this is a situation where we are safe.

We were supposed to do \(u_x - 2 u_y = x\) 

When we do luck out, we can lose uniqueness.

Now we continue with \(u_x - 2u_y = x\) 

We have \(u_{\tau} = x \)

At \(x=1\) we actually don't have a solution since we have a contradiction.

Solving the char system,

\(x = \tau + s\)

\(y = -2\tau + 1 - s^2\)

\(u_\tau = \tau + s\) 

\(u = \frac{1}{2} \tau ^2 + s\tau\)

So, \(2x+y = 1-s^2 + 2s\) 

So, \(s^2 - 2s + 1 = 2 - 2x - y \implies s = 1 \pm \sqrt{2 - 2x - y} \) so we have a problem.

\hrulefill

Class 32: 04/01

\section*{Conservation Laws}

\underline{Review ofDerivation} 

\(u = u(x,t)\) where \(x\in\mathbb{R} ^3\) density

Rate of change in \(\Omega \in\mathbb{R}^3\)

\[
    \frac{\mathrm{d}}{\mathrm{d}t} \int_{\Omega}^{} u(x,t) \,\mathrm{d}x = - \int_{\partial \Omega}^{} F \cdot \vec{n} \,\mathrm{d}S = - \int_{\Omega}^{} \nabla\cdot F \,\mathrm{d}x 
\]

For arbitrary \(\Omega\) we have:

\[
    \int_{\Omega}^{} \left( u_t + \nabla\cdot F \right)  \,\mathrm{d}x = 0
\]

\(F\) is flux.

Thus, we have

\[
    \boxed{u_t + \nabla\cdot F = 0}
\]

If \(F = - \kappa \nabla u\) we have Fourier's law:

\[
    \implies u_t = k \Delta u
\]

What if \(F = F(u(x,t))\)?

Example: traffic flows

Road can be empty or in a jam, flux is non-linear.

\(F = F(u)\). Substitute.

\[
    u_t + \nabla\cdot F(u) = 0
\]

Specialize to 1 space dimension.

\(u = u(x,t)\) where \(x\in\mathbb{R} ,t>0\) 

So,

\[
    u_t + \frac{\partial}{\partial x} (F(u)) = 0
\]

Equivalently,

\[
    u_t + F^{\prime} (u)u_x = 0
\]

Initial condition: \(u(x,0)=u_0(x)\) where \(u_0:\mathbb{R} \to \mathbb{R}\) is given

This is called scalar conservation law in 1 space dimension.

Going back to method of characteristics

For \(a_1 u_{x_1} + \cdots + a_n x_n = c(x,u)\) 

We have \(\frac{\partial x_1}{\partial \tau}=a_1,\cdots,\frac{\partial x_n}{\partial \tau} = a_n\) and \(\frac{\partial u}{\partial \tau} = c(x,v)\)

Thinking of \(t\) as \(x_n\) we have \(\frac{\partial x_n}{\partial \tau} = 1,x_n(s,0)=0\)

So, \(x_n = \tau\)

So, \(t = \tau\).

So we don't need to introduce \(\tau\) since it is just \(\tau = t\) 

\[
    u_t + F^{\prime} (u) u_x = 0
\]

\[
    u(x,0) = u_0(x)
\]

When \(-\infty < x < \infty\)

Note: If \(F(u)=cu\) aka \(F\) is linear and thus \(F^{\prime} = c\) then \(u_t + cu_x = 0\) is just simple transport, solution is \(u(x,t)=u_0(x-ct)\) 

Continuing with method of characteristics,

\(\frac{\partial X}{\partial t} = F^{\prime} (U)\) 

\(\frac{\partial U}{\partial t} = 0\) 

\(X(s,0)=s\)

\(U(s,0)=u_0(s)\) 

Since \(\frac{\partial U}{\partial t} = 0\) it doesn't depend on \(t\) so \(U(s,t)=u_0(s)\) 

Therefore, \(\frac{\partial X}{\partial t} = F^{\prime} (U)=F^{\prime} (u_0(s))\) and \(X(s,0)=s\)

\(X(s,t)= F^{\prime} (u_0(s))t + s\) 

Now invert

We want \(s = S(x,t)\) 

\(x = F^{\prime} (u_0(s))+s\)

Using this we have some non-linear equation for \(s\).

Check inverse function theorem:

\[
    \det \begin{pmatrix}
        \frac{\partial X_1}{\partial s} & \frac{\partial X_2}{\partial s} \\
        \frac{\partial X_1}{\partial \tau} & \frac{\partial X_2}{\partial \tau}  \\
    \end{pmatrix} = \det \begin{pmatrix}
        1 & F^{\prime} (u_0(s)) \\
        0 &  1 \\
    \end{pmatrix} = 1 \neq 0
\]

So, there exists solution for \(\vert t \vert\) small and \(u_0\) smooth.

So, \(u(x,t)=u_0(S(x,t))\) 

Sketch charateristics in \(x-t\) plane

\(X = F^{\prime}(u_0(s))t + s\)

So characteristics are lines for fixed \(s\) in terms of \(t\).

\(\frac{\mathrm{d}t}{\mathrm{d}x} = \frac{1}{F^{\prime} (u_0(s))}\)

It hits \(x\) axis when \(s\) is \(x\)

2 cases:

i: \(s \mapsto F^{\prime} (u_0(s))\) is non-decreasing

ii: \(s\mapsto F^{\prime} (u_0(s))\) decreases on some interval 

Case 1:

Lines of increasing intervals foliate the upper half plane!

Lines \underline{fan out} to cover the upper half plane.

Case 2:

Then lines can intersect, but the solutions are constants along each line and they can be different constants. So this is disastrous!

\underline{Collision of Characteristics} 

So solution has singularities (maybe lots of them)

\underline{Example}:

Take simplest non-linear: \(F(u) = \frac{1}{2}u^2 \implies F^{\prime}(u)=0\)

So, PDE is \(u_t + u u_x = 0\) 

This is called \underline{Burger's Equation}

take initial condition \(u(x,0)=e^{-x^2}=u_0(x)\) 

Slopes of characteristics:

\[
    \frac{\mathrm{d}t}{\mathrm{d}x} = \frac{1}{F^{\prime}(u)} = \frac{1}{u} = \frac{1}{e^{-s^2}} = e^{s^2}
\]

At \(s=0\) the slope is \(1\) and after \(s=0\) we have increasing slopes so we get collisions.

What happens before collision? \(u_0(0)=1\) so \(u=1\) along the line.

\(u_0(\frac{1}{2})=e^{-\frac{1}{4}}\) so \(u=e^{-\frac{1}{4}}\) along that line. So that's a collision.

Before collision: they are practically vertical, so we have very little change. The peak has been carried along the line of slope \(1\) 

But it is dangerously close to collision, so there is a big derivative drop.

This is called `wave-breaking'

This is a non-linear wave effect.

\hrulefill

Class 33: 04/03

Scalar Conservation Law in 1 Space Dimension

\[
    u_t + F(u)_x = 0
\]

\[
    u(x,0)=u_0(x)
\]

Reall: \(u_t + F^{\prime} (u) u_x = 0\)

Method of Characteristics:

\(\frac{\partial X}{\partial t} = F^{\prime} (U)\), \(X(s,0)=s\)

\(\frac{\partial U}{\partial t} = 0\), \(U(s,0)=u_0(s)\) 

Then, \(X(s,t) = F^{\prime} (u_0(s))+s\)

And \(U(s,t)=u_0(s)\) 

Characteristics are lines with slope

\[
    \frac{\mathrm{d}t}{\mathrm{d}x} = \frac{1}{F^{\prime} (u_0(s))}
\]

Note that \(t > 0\) 

We have two cases:

Nice case: If \(s \mapsto F^{\prime} (u_0(s))\) is non-decreasing, then the lines are not intersecting so we get nice solutions

Bad case: If \(s \mapsto F^{\prime} (u_0(s))\) is decreasing in some interval, then the lines intersect so we get singularities. Singularities develop in finite time.

\underline{Estimating time of existence of a classical solution}:

Inverse Function Theorem gurantees (at least for a little while) that there exists \(S(x,t)\) which is the inverse of the map: \(x = F^{\prime} (u_0(S(x,t)))t + S(x,t)\)

The solution is given by:

\[
    u(x,t) = u_0(S(x,t))
\]

Use this to write an implicit formula for \(u\) 

\[
    u(x,t) = u_0(x - F^{\prime}(u(x,t))t)
\]

Compute \(u_x\). Assume \(F\in C^2\) 

\[
    u_x = u_0^{\prime} (x - F^{\prime}(u(x,t))t)(1-F^{\prime\prime} (u(x,t))u_x\cdot t)
\]

Solve for \(u_x\) 

\[
    u_x = \frac{u_0^{\prime}(x - F^{\prime} (u(x,t))t)}{1 + F^{\prime\prime} (u)u_0^{\prime} (x-F^{\prime} (u(x,t))t)t}
\]

Now, \(F^{\prime\prime} (u(x,t))u_0^{\prime} (x-F^{\prime} (u(x,t))t) = \frac{\mathrm{d}}{\mathrm{d}s} F^{\prime} (u_0(s))\) 

There's not going to be trouble if we never have to divideby \(0\) 

If \(F^{\prime} (u_0(s))\) has a positive derivative, then at any time, the denominator is \(>0\) so we don't have any problem.

If \(s \mapsto F^{\prime}(u_0(s))\) decreases somewhere then \(\exists t > 0\) when denominator \(=0\)

First time this happens is at \(t^{\ast}=-\frac{1}{F^{\prime} (u_0(s^{\ast}))}\)

Where \(F^{\prime\prime} (u_0(s^{\ast}))u_0^{\prime} (s^{\ast})\) minimizes \(F^{\prime\prime}(u_0(s))u_0^{\prime} (s)\)

ex: Burger's Equation

\(F(u)=\frac{1}{2}u^2\)

\(u_t + u u_x = 0\) 

Take \(u_0(x)=e^{-x^2}\) 

Then \(F^{\prime} (u_0(s))=u_0(s)=e^{-s^2}\)

Minimize \((e^{-s^2})^{\prime}\) equals minimize \(-2s e^{-s^2}\)  

Can easily check \(s^{\ast} = \sqrt{\frac{2}{e}}\)

So \(t^{\ast} = \sqrt{\frac{e}{2}} \) 

\section*{Weak Solutions of Conservation Laws}

\underline{Idea} Let \(\phi\) be a \underline{test function} \(\phi = \phi (x,t)\) where \(\phi\in C^{\infty}\) for \(x\in\mathbb{R} , t \geq 0\). Support \(\phi\) is compact in \(x\) and also \(\phi\) vanishes for \(t\) large (or \(t\) near \(0\) )      

\[
    0 = \int_{0}^{\infty} \int_{-\infty}^{\infty} \left( \phi u_t + \phi F(u)_x \right)  \,\mathrm{d}x  \,\mathrm{d}t 
\]

\[
    = - \int_{0}^{\infty} \int_{-\infty}^{\infty} \left( \phi_t u + \phi_x F(u) \right)  \,\mathrm{d}x  \,\mathrm{d}t 
\]

Note that equality to \(0\) requires it to be a classical solution. But the equality of integrals doesn't require any differentiability of \(u\).

\begin{definition}
    If \(u\) is locally integrable (\(L^1_{loc}(\mathbb{R} \times (0,\infty))\)) and if:

    \[
        \int_{0}^{\infty} \int_{-\infty}^{\infty} \phi_t u + \phi _x F(u) \,\mathrm{d}x  \,\mathrm{d}t = 0
    \]

    For every \underline{test function} then we say \(u\) is a \underline{weak solution} of the PDE \(u_t + F(u)_x = 0\) 
\end{definition}

A key example of a weak solution is a shock solution.

We seek a weak solution of the following form:

\[
    u(x,t) = \begin{dcases}
        u_L(x,t), &\text{ if } \Omega_L ;\\
        u_R(x,t), &\text{ if } \Omega_R ;\\
    \end{dcases}
\]

where \(\mathbb{R} \times (0, \infty) = \Omega_L \cup \Omega_R \cup \Gamma\) where \(\Gamma\) is a curve and \(u_L\) and \(u_R\) are classical solutions in \(\Omega_L\) and \(\Omega_R\) respectively.

\(u_L \neq u_R\) for \((x,t)\in \Gamma\)  

\underline{Question}: Is there a condition on \(\Gamma\) such that this formula yields a weak solution?

\hrulefill

Class 24: 04/05

Skipped

\hrulefill

Class 25: 04/10

Reivew: weak solutions of conservatin laws

\(u_t + F(u)_x = 0\) 

Characteristics: Straight Lines with slope \(\frac{\mathrm{d}t}{\mathrm{d}x} = \frac{1}{F(u_0)}\) 

\(u\) constant along characteristics.

Weak solution:

\[
    \int_{}^{} \int_{}^{} \phi_t u + \phi _x F(u) \,\mathrm{d}x  \,\mathrm{d}t = 0 
\]

for all test function \(\phi\) 

Given curve \(x=s(t)\) on the left solution \(u = u_L(x,t)\) on the right solution \(u=u_R(x,t)\) is a weak solution iff

\[
    s^{\prime}(t) = \frac{F(u_L)-F(u_R)}{u_L - u_R}(R-H)
\]

2 examples of `Riemann problems'

ex 1: Burger's eqn \(F(u)=\frac{1}{2}u^2\)

\(u_0(x)=1\) for \(x<0, u_0(x)=0\) for \(x>0\) 

ex 2: \(F(u)=\frac{1}{2}u^2\) 

\(u(x,t)=0\) for \(x<\frac{1}{2}t, u(x,t)=1\) for \(x>\frac{1}{2}t\) 

Another kind of wave solution is `Rarefaction waves'. A type of solution that is constant along rays.

Back to example 2.

Look for a solution that continuously interpolates between \(0\) and \(1\)

To try this.

Seek a solution in the region \(0 < x < t\) of the form \(u(x,t)=f(\frac{x}{t})\)

Let \(\eta = \frac{x}{t}\)

\(u_x = f^{\prime} (\frac{x}{t})\cdot \frac{1}{t}\)

\(u_t = f^{\prime} (\frac{x}{t})\cdot(-\frac{x}{t^2})\) 

Substitute into Burger's equation.

\(u_t + u u_x = f^{\prime} (\frac{x}{t})(-\frac{x}{t^2})+f(\frac{x}{t})\frac{1}{t}\) 

Multiply by \(t\) 

\(f^{\prime} (\eta)(-\eta)+f(\eta)f^{\prime} (\eta)=0\)

\(\implies f^{\prime} (\eta)(f(\eta)-)\equiv 0\) 

\(f(\eta)=\eta \implies ;\) either \(f^{\prime}(\eta)\equiv 0, f(\eta)=\eta\) 

\(f(\eta)=\eta \implies u(x,t)=\frac{x}{t}\) 

We get a caondidate for a weak solution:

\(u(x,t)\begin{dcases}
    0, &\text{ if }  z <[;.];\\
    \frac{x}{t}, &\text{ if }  ;\\
    1 &\text{ otherwise} .
\end{dcases}\) 

This is not a classical solution:

\[
    \lim_{x \to 0^-} \frac{u(x,t)-u(0,t)}{x}=0
\]

\[
    \lim_{x \to 0^+} \frac{u(x,t)-u(x,0)}{x} = \frac{1}{t} 
\]

Need a more refined notion of a weak solution.

A good shock (``physical'' shock is one)

It is one such that characteristics run \underline{into} the shock as \(t\) increases

If two characteristics interact at a point \(P\) in the \(x-t\) space then each one doesn't but any other characersistic no one traces back time back down to \(t=0\) 

Characteristics run into shock:

\[
    \frac{1}{F^{\prime}(u_L)} < \frac{1}{S^{\prime}(t)} < \frac{1}{F^{\prime}(u_R)}
\]

Or:

\(F^{\prime} (u_R) < S^{\prime} (t) < d^{\prime} < F^{\prime} (u_L)\) 

This is alled the entropy condition.

A `good shock' must satisfy (RH) and the entropy condition.

If we consider uniformly convex fluxes \(F\)

ie \(F^{\prime\prime}(u) \geq \theta >0\) for all \(u\) 

Then \(F^{\prime}\) is inceasing.

Then the entropy condition becomes much simpler. It is equivalent to \(u_R < s^{\prime}(t)<F^{\prime}(u_L)\) along \(x=s(t)\) 

Note: for the shock in example 2, \(u_L=0, u_R=1\) and \(s^{\prime} = \frac{1}{2}\) and it fails. 

\hrulefill

Class 26: 04/12

We discuss HW5 P1.

Back to class.

Example 3: Last example of a weak solution to Burger's Equation.

\(u_t + u u _x = 0\) 

On \(-\infty < x < \infty , t > 0\).

\(u(x,0)=u_0(x)\) 

\(u_0(x) = \begin{dcases}
    0, &\text{ if } x < 0 ;\\
    1, &\text{ if } 0 < x < 1 ;\\
    0, &\text{ if } x > 1 .
\end{dcases}\) 

Some have two piecewise discontinuities.

Recall that for \(u_0(x)=\begin{dcases}
    1, &\text{ if } x < 0 ;\\
    0, &\text{ if } x > 0 .
\end{dcases}\)  we have a shock solution.

Also, for \(u_0(x)=\begin{dcases}
    0, &\text{ if } x < 0 ;\\
    1, &\text{ if } x > 0 ;
\end{dcases}\) we have 2 solution. ``bad'' shock solution (failed entropy condition, \(F^{\prime} (u_R) < s^{\prime} (t) < F^{\prime} (u_L)\) ) and ``good'' \underline{rarefaction wave}.

Recall slopes of characteristics: \(\frac{\mathrm{d}t}{\mathrm{d}x} = \frac{1}{F^{\prime} (u_0)} = \frac{1}{u_0}\)

So, we have vertical lines for \(x<0\), slope \(1\) lines from \(0<x<1\) and again vertical lines for \(x>1\) 

For \(0 < x < t\) let \(u(x,t)\) be the rarefaction wave: \(u(x,t)=\frac{x}{t}\)

From \(x=1\) we have collision of characteristic so we seperate them with a shock.

\(RH\): \(u_L = 1, u_R = 0\) 

So \(s^{\prime} (t) = \frac{F(u_L)-F(u_R)}{u_L - u_R }= \frac{\frac{1}{2}1^2 - \frac{1}{2}0^2}{1-0} = \frac{1}{2}\) 

Also \(s(0)=1\) 

So, \(s(t)=\frac{1}{2}t + 1\)

So, shock line is \(x = \frac{1}{2}t + 1\) 

Therefore, we have:

\[
    u(x,t)=\begin{dcases}
        0, &\text{ if } x < 0 ;\\
        \frac{x}{t}, &\text{ if } 0 < x < t ;\\
        1, &\text{ if } t < x < \frac{1}{2}t + 1;\\
        0, &\text{ if } x > \frac{1}{2}t + 1.
    \end{dcases}
\]

This is only good for a little while though, since the rarefaction wave \((u=\frac{x}{t})\)  hits the shock when \(x=t\) hit \(x=\frac{1}{2}t+1\) aka when \(t=2\) 

So, solution is only true for \(0 < t < 2\).

We look for a 2nd shock \(x = \tilde{s}(t)\)  emanating from \((2,2)\) 

We use RH.

\(\tilde{s}^{\prime} (t)=\frac{F(u_L)-F(u_R)}{u_L - u_R}\)

Note that \(u_L = \frac{x}{t}\)

So, \(\tilde{s}^{\prime} (t) = \frac{\frac{1}{2}\left( \frac{\tilde{s}(t)}{t} \right)^2 - \frac{1}{2}0^2}{\frac{\tilde{s}(t)}{t}- 0} \) 

Thus, \(\tilde{s}^{\prime} (t)=\frac{1}{2}\frac{\tilde{s}(t)}{t}\)

Also \(\tilde{s}(2)=2\) 

So, \(\frac{\mathrm{d} \tilde{s}}{\tilde{s}}=\frac{1}{2}\frac{1}{t}\mathrm{d} t\)

Integrating, \(\ln \tilde{s} = \frac{1}{2} \ln t + C\)

Thus, \(\tilde{s}(t)=\tilde{C}e^{\frac{1}{2}\ln t}=\tilde{C}\sqrt{t}\) 

So, \(x=\tilde{s}(t)=\sqrt{2t} \) 

So, \(t=\frac{1}{2}x^2\) 

We therefore have a piecewise shock.

Note: \(x = \tilde{s}(t)\) satisfies the entropy condition since \(F^{\prime} (u_R)=0 < \tilde{s}^{\prime}(t) =\frac{1}{\sqrt{2t}} < F^{\prime} (u_L)=u_L = \frac{x}{t} = \frac{\sqrt{2t}}{t} = \frac{\sqrt{2}}{\sqrt{t}} \) 

\hrulefill

Class 27: 04/15

\(u_t + F(u)_x = 0, -\infty < x < \infty , t > 0\) 

\(u(x,0)=u_0(x)\)

- Classical Solution via Method of Characteristics

- Weak Solution (In the sense of Distribution) (Rieman Problems)

-- Shock solution (Rankine-Hugoniot) (Rarefaction Waves)

-- Possible non-uniqueness alleviated by an entropy condition

What about existence/uniquenes for general initial data \(u_0\)?

Today: Discuss Lax-Oleinik Formula

Assume \(F:\mathbb{R} \to \mathbb{R}\) is uniformly convex. \(F^{\prime\prime} (u) \geq \Theta > 0 \forall u\in\mathbb{R}\).

Then tangent line always lies below the graph.

Graph of tangent line at \(v\): \(F(v)+F^{\prime} (v)(u-v)\).

So, \(F(u) > F(v) + F^{\prime} (v)(u-v)\) 

We'll need: \underline{Convex Dual of \(F\)}.

\[F^{\ast} (z) \coloneqq [\max_v zv - F(v)]\]

We are maximizing \(v \mapsto zv - F(v)\).

Differentiating, \(z = F^{\prime} (v)\)

Second derivative: \(-F^{\prime\prime}(v) < 0\)

So, at \(z = F^{\prime} (v)\) this becomes maximum.

Thus, \(F^{\ast} (z)= z (F^{\prime})^{-1} (z)-F((F^{\prime} )^{-1} (z))\) 

Since \(F^{\prime}\) is increasing its inverse is well defined.

Let \(b(z)\coloneqq (F^{\prime})^{-1} (z)\) 

Then \(F^{\ast} (z)=z b(z)-F(b(z))\) 

example: burger's eqn.

\(F(u)=\frac{1}{2}u^2 \implies F^{\prime} (u)=u \implies b(z)=z \implies F^{\ast} (z) = z^2 - F(z)= z^2 - \frac{1}{2} z^2 = \frac{1}{2} z^2\) 

So, it is self-dual.

In general, \(F^{\ast} (z)=F^{\prime} (v)v - F(v)\)

where \(z = F^{\prime} (v)\)

Assume \(u_0 \in L^1(\mathbb{R})\)

Meaning \(\int_{-\infty}^{\infty} \vert u_0(x) \vert  \,\mathrm{d}x < \infty\) 

Also assume \(F\) strictly convex. Aka graph always lies above tangent line.

\(F(v)+F^{\prime} (v)(u-v) \leq F(u)\) with strict inequality unless \(u=v\). 

Suppose \(u\) classically solves our conservation law and the initial condition.

Consider antiderivative: \(\mathcal{U}(x,t) = \int_{-\infty}^{x} u(x^{\prime} , t) \,\mathrm{d}x^{\prime}  \) 

Integrating \(u_t + F(u)_x = 0\) over the real line,

\[
    \mathcal{U}_t + F(\mathcal{U}_x) = 0
\]

\[
    \mathcal{U}(x,0) = \int_{-\infty}^{x} u_0(x^{\prime}) \,\mathrm{d}x^{\prime}  
\]

This is a Fully nonlinear PDE called Hamilton-Jacobian equation.

For any \(v\in\mathbb{R}\) we have the following:

\[
    \mathcal{U}_t + F^{\prime} (v) \mathcal{U}_x = F^{\prime} (v) \mathcal{U}_x - F(\mathcal{U}_x) \leq F^{\prime}(v) v - F(v)
\]

Inequality is strict unless \(v = U_x = u\) 

Now fix any \((x,t)\) where \(t > 0\).

For any \(v\) let \(y = y(x,t ; v)\) be the point on \(x\)-axis by tracing back by a line with slope \(\frac{\mathrm{d}x}{\mathrm{d}t} = F^{\prime}(v)\).

So, \(F^{\prime} (v) = \frac{x-y}{t}\) 

Parametrize this line: \(l_v(\tau) = (x,t)+\tau(F^{\prime} (v),1)\)

Note: \(l_v(0)=(x,t), l_v(-t)=(x-t F^{\prime} (v), 0)=(y,0)\)

\[
    \frac{\mathrm{d}}{\mathrm{d}\tau} \mathcal{U}(l_v(\tau)) = \mathcal{U}_x \cdot F^{\prime} (v) + \mathcal{U}_t
\]

Integrate both sides of \((\ast), \int_{-t}^{0} \cdot \,\mathrm{d}\tau \) 

\[
    \mathcal{U}(x,t) - \mathcal{U}(y,0) \leq \int_{-t}^{0} (F^{\prime}(v)v - F(v)) \,\mathrm{d}\tau 
\]

\[
    \mathcal{U}(x,t) \leq \mathcal{U}_0(y) + t (F^{\prime} (v) v - F(v))
\]

with equality only when \(v = \mathcal{U}_x = u(x,t)\) 

Equivalently, \(\mathcal{U}(x,t) \leq \mathcal{U}_0(y) + t F^{\ast} (z)\) 

where \(z = F^{\prime} (v) = \frac{x-y}{t}\) 

Instead view \(v = v(x,t;y)\) 

Define \(Y(x,t)\) as a minimizer of \(y \mapsto \mathcal{U}_0(y)+t F^{\prime} (v(x,t;y))v(x,t;y)-F(v(x,t;y))\)

Equivalently, \(Y\) minimizes:

\[
    y \mapsto \mathcal{U}_0(y) + F^{\ast} (z) = \mathcal{U}_0(y) + F^{\ast}\left(\frac{x-y}{t}\right)
\]

Since \(Y\) minimizes, using that \(Y\),

\(v(x,t;Y)=u(x,t)\) should solve the original conservation law.

The Lax-Oleinik formula is:

\[
    u(x,t)=b \left( \frac{x-Y(x,t)}{t} \right) 
\]

where \(Y\) minimizes \(\mathcal{U}_0(y)+F^{\ast} \left( \frac{x-y}{t} \right) \) 

and \(b = (F^{\ast})^{-1} \) 

Check:

-- This is consistent with solution obtained via method characteristics.

-- Lax-Oleinik proves a formula for a weak solution valid \(\forall t > 0\)

\hrulefill

Class 04/17

Recap

\(u_t + F(u)_x = 0\) 

\(u(x,0)=u_0(x)\) 

\(F\) uniformly convex

Then \(F^{\ast} (z)\coloneqq \max_u zu - F(u)\)

\(\mathcal{U}(x,t)= \int_{-\infty}^{x} u(x^{\prime} , t) \,\mathrm{d}x^{\prime}  \) 

\(\mathcal{U}_t + F(\mathcal{U}_x) = 0\) 

We have \underline{Lax-Oleinik Formula} 

Let \(Y(x,t)\) denote \underline{a minimum} [not necessarily unique] of:

\[
    y \mapsto \mathcal{U}_0 (y) + t F^{\ast} \left( \frac{x-y}{t} \right) 
\]

Where \(\mathcal{U}_0(y) \coloneqq \int_{-\infty}^{y} u_0(x^{\prime} ) \,\mathrm{d}x^{\prime}  \)

Now, \(u(x,t) \coloneqq b \left( \frac{x-Y(x,t)}{t} \right) \) 

solves \(u_t + F(u)_x = 0\)

where \(b = (F^{\prime})^{-1} \) 

\underline{Relation to the method of characteristics} 

\(u_t + F^{\prime} (u) u_x = 0\)

\(\frac{\partial X}{\partial t} = F^{\prime}(U) \quad\quad X(s,0) = s\)

\(\frac{\partial U}{\partial t} = 0\quad\quad U(s,0)=u_0(s)\)

Now, \(U(s,t)=u_0(s)\) 

\(\frac{\partial X}{\partial t} = F^{\prime} (u_0(s))\)

\(X(s,t)=F^{\prime} (u_0(s))t + s\)

We want to solve in terms of \(X\) and \(t\)

So, we want to find the value of \(s\) from \(x = F^{\prime} (u_0(s))t + s\) 

\(x = F^{\prime} (u)t + s\) for as long as solution exists classically, we can find \(s = S(x,t)\) exists smoothly.

We have:

\(x = F^{\prime} (u) t + S(x,t)\)

\(\implies \frac{x-S(x,t)}{t} = F^{\prime} (u)\)

Thus, \(u(x,t) = b \left( \frac{x-S(x,t)}{t} \right) \) 

So, the mysterious minimizer in the Lax-Oleinik formula in the classical case is just \(S(x,t)\) !!!

Non-uniqueness of \(Y(x,t) \iff\) collision of characteristics

Real importance of Lax-Oleinik formula:

\underline{Claim}: Lax-Oleinik formula yields an \underline{entropy-satisfying} weak solution for all \(t > 0\) 

\underline{One key aspect of the proof}:

One shows that if \(Y(x_1,t)\) is any minimum and \(Y(x_2,t)\) is also any minimum then \(x_1 < x_2\) implies:

\(Y(x_1,t) < Y(x_2,t)\) 

Again,

\(u_t + F(u)_x = 0\) 

\(u(x,0)=u_0(x)\) 

Another approach assumping only that \(F\) is Lipschitz continuous

That is: \(\vert F(u_2) - F(u_1) \leq L(u_2 - u_1) \vert \) for all \(u_1, u_2\in\mathbb{R}\) 

Further assume \(\int \vert u_0 \vert < \infty\) 

Also that \(u_0\) is of bounded variation

Steps for solution:

1. Fix any \(N\in\mathbb{Z} ^+\) [eventually \(\to \infty\) ] 

2. Approximate \(u_0\) by a function \(u_0^N\) such that:

2.1: \(u_0^N\) piecewise constant

2.2: \(u_0^N\) only takes value in the set \(\{ 2^{-N} j ; j\in\mathbb{Z} \} \) 

3. Approximate flux \(F\) by \(F_n\) where \(F_N\) is piecewise linear on each interval

\([2^{-N}j, 2^{-N}(j+1)]\) for all \(j\in\mathbb{Z}\)  

4. Attempt to solve \(u_t + F_N(u)_x = 0\) 

where \(u(x,0)=u_0^N (x)\) 

using only \underline{shocks}

[Note: \(F_N\) is not differentiable at all points, but it is piecewise continuous.]

Stop after first collision.

They carry the values from dyadic sets.

Restart solving new Riemann problems 

From this, we again solve the riemann equations

Proof hinges on showing that the number of restarts and the number of shocks are bounded independent of \(N\) 

\hrulefill

Class 29: 04/19

\(u_t + F(u)_x = 0\)  (1)

Recall the derivation

\(u\) = density

\(Q\) = flux

Let \(u_t + Q_x = 0\) 

conservation law

Modeling diffusion:

\(Q = - \kappa u_x\)

\(\implies u_t = \kappa u_{x x}\) 

if \(Q = F(u)\) (traffic flow)

Another approach to obtaining `good' weak solutions valid for all \(t > 0\) 

Replace some missing effect that incorporates independence on \(u_x\) 

Use \(Q = F(u) - \epsilon u_x\)

\(0 < \epsilon \ll 1 \implies \)

\(u_t + F(u)_x = \epsilon u_{x x}\)  (2) [this comes from the \(u_t + Q_x = 0\) equation]

``viscous conservation law''

Hybrid between (hyperbolic) conservation law and the heat equation

Maybe ``good'' weak solutions of (1) are those that arise as limit of solutions of (2) as \(\epsilon \to 0\)

Given \(u(x,0)=u_0(x)\) it turns out solutions to (2) exist for all \(t > 0\) and are \underline{smooth} 

Adding in \(\epsilon u_{x x}\) to (1) to get (2) leads to a \underline{singular perturbation} problem in PDE

(Meaning: The perturbation involves the highest order of the PDE)

Lets pursue a connection between (1) and (2) for Riemann-type (piecewise constant) initial condition

ex: Assume \(F\) is strictly convex.

Take: \(u_0(x) = \begin{dcases}
    u_L, &\text{ if } x < 0 ;\\
    u_R, &\text{ if } x > 0 .
\end{dcases}\) 

Assume \(u_L > u_R\) 

Again, draw \(x-t\) graph.

\(\frac{\mathrm{d}x}{\mathrm{d}t} = F^{\prime} (u_0)\) 

Slope is \(F^{\prime} (u_L)\) vs \(F^{\prime} (u_R)\)

not necessarily the same curve as burgers equation, but we have \(F^{\prime} (u_L) > F^{\prime} (u_R)\) so there is collision

So \(s^{\prime} (t) = \frac{F(u_L)-F(u_R)}{u_L - u_R}\) 

\(s^{\prime} (t) = \frac{F(u_L)-F(u_R)}{u_L - u_R}\)

So \(s(t) = \underbrace{(\frac{F(u_L)-F(u_R)}{u_L - u_R})}_{v_0}t\) 

Since \(F^{\prime} (u_L) > F^{\prime} (u_R)\) the characteristics run into the shock so this is an entropy satisfying solution

So, weak solution:

\[
    u(x,t) = \begin{dcases}
        u_L, &\text{ if } x < v_0 t ;\\
        u_R, &\text{ if } x > v_0 t ;
    \end{dcases}
\]

In fact, another way to write it is: \(u(x,t) = u_0(x - v_0t)\) 

This is a \underline{travelling wave} solution to PDE

Now turn to (2)

Seek a travelling wave solution to (2). That is: find \(U:\mathbb{R} \to \mathbb{R}\) such that:

\(u(x,t)=U(x - vt)\) 

We have equation:

\(u_t + F(u)_x = \epsilon u_{x x}\)

\(\implies u_t + F^{\prime} (u) u_x = \epsilon u_{x x}\) 

And we seek \(u(x,t)=U(x - vt)\) where \(v\in\mathbb{R}\) 

Write \(y\coloneqq x - vt\)

\(u(x,t)=U(y)\). We have \(y_x = 1, y_t = -v\) 

\(u_t = -v U_y\)

\(u_x = U_y\) 

\(u_{x x} = U_{y y}\) 

So, our equation (2) becomes:

\[
    - v U_y + F^{\prime} (U) U_y = \epsilon U_{y y}
\]

We also need to specify: \(\lim_{y \to \infty} U(y) = U_R, \lim_{y \to -\infty} U(y) = u_L\) 

Now it's an ODE problem.

This is a second order ODE.

Integrate \(\int_{-\infty}^y\) to get a first order ODE. Assume \(U_y (\pm \infty)=0\) 

\[
    \int_{-\infty}^{y} - v U_y\,\mathrm{d}y + \int_{-\infty}^{y} F(u)_y \,\mathrm{d}y = \epsilon \int_{-\infty}^{y} U_{y y} \,\mathrm{d}y  
\]

\[
    - v (U(y) - u_L) + F(U) - F(U_L) = \epsilon U_y
\]

This is a first order ODE

Still want \(u_R, u_L\) at \(\pm \infty\) 

Note that \(U\) really depends on \(\epsilon\) 

Rewrite:

\[
    - v (U^{\epsilon}(y) - u_L) + F(U^{\epsilon}) - F(u_L) = \epsilon U^{\epsilon}_y 
\]

eliminate \(\epsilon\) by introducing \(z = \frac{y}{\epsilon}\)  

Then \(\frac{\partial}{\partial y} U = \frac{\partial U}{\partial z} \cdot \frac{1}{\epsilon}\) 

Substituting,

\(-v (U(z)-U_L) + F(U(z)) - F(u_L)=U_z (z)\) (3)

Still want: \(U(\infty)=u_R, U(-\infty)=u_L\) 

Again assume \(U_z(\pm \infty)=0\) 

Set \(z\to \infty\) at (3):

\(-v (u_R - u_L) + F(u_R) - F(u_L) = 0\) 

So, \(v = \frac{F(u_L)-F(u_R)}{u_L - u_R}\) 

Note: upto here, our analysis of (2) did \underline{NOT} assume \(u_L > u_R\) 

\hrulefill

Class 30: 04/22

Viscous Conservation Laws

\(u_t + F(u)_x = 0\) 

\(u(x,0)=u_0(x)=\begin{dcases}
    u_L, &\text{ if } x<0 ;\\
    u_R, &\text{ if } x>0 ;
\end{dcases}\) 

\(F\) strictly convex.

We instead solve:

\(u^{\epsilon} _t + F(u^{\epsilon} )_x + \epsilon u^{\epsilon} _{x x}\)

\(u (x,0)=u_0(x)\) 

Can we obtain a limit of \(u^{\epsilon}\)?

If so, does that limit solve the conservation law?

Look for travelling wave solution:

\(u^{\epsilon} = U^{\epsilon}(\underbrace{x-vt}_y)\)

\(\implies - v U_y^{\epsilon} + F^{\prime} (U^{\epsilon})U_y^{\epsilon} = \epsilon U_{y y}^{\epsilon}   \) 

Let \(z = \frac{x-vt}{\epsilon}\) to get a PDE with no \(\epsilon\)  

\(- v U_z + F^{\prime} (U) U_z = U_{z z}\) 

We also have: \(U(\infty)=u_R, u(-\infty)=u_R\) from \(u_0\) 

Assuming \(U_z(\pm \infty)=0\)

Integrate the ODE \(\int_{-\infty}^{z} \cdot \,\mathrm{d}z^{\prime} \) 

\[
    - v [U(z)-u_L] + F(U(z)) - F(u_L) = U_z
\]

Note: \(U(\infty)=u_R, U_z(\infty)=0\) 

\(v = \frac{F(u_L)-F(u_R)}{u_L - u_R} \eqqcolon v_0\) [RH]

So, if we have a `travelling wave' solution, it must have the correct `speed'

We consider shock solutions of it. Recall that for \(F\) strictly convex, \(u_L > u_R\) gives us `good' [entropy satisfying] shocks, and \(u_L < u_R\) gives us `bad shocks' [characteristics don't go into it].

First: assume \(u_L > u_R\) 

Rewrite: \(U_z = g(U)\) where \(g(U)= - v_0 (U-u_L) + F(U) - F(u_L)\) 

Goal: \(\exists\) solution such that \(U(\infty)=u_R, u(-\infty)=u_L\)  

Note: \(g(u_L)=g(u_R)=0\) 

Also, by convexity of \(F\) we see that:

\(g(U) < 0\) for \(u_R < U < u_L\)

[draw a graph of \(g\). It should look like a `parabola']

Consider any initial data.

Consider \(U(0)=a, u_R < a < u_L\) 

\(U_z = g(U), U(0)=a\) 

So, \(\exists !\) local solution.

Can be extended for all \(z>0\) and \(U(\infty)=u_R, U(-\infty)=u_L\)

We've found a solution to the viscous conservation law \(U^{\epsilon}(x,t)=U\left( \frac{x-v_0t}{\epsilon}\right)  \) 

Let \(\epsilon \to 0\)

If \(x > v_0 t\) then \(U^{\epsilon} \to U(\infty)=u_R\) 

If \(x < v_0 t\) then \(U^{\epsilon} \to U(-\infty) = u_L\) 

If instead \(u_R > u_L\)

recall that we found 2 weak solutions to \(u_t + F(u)_x\) 

So, if we plot the characteristics, there is a gap.

1: shock solution satisfing RH but not the entropy condition

2: the rarefaction wave, that `interpolates' between the gaps

For \(u_L < u_R\) one finds \(g(u) < 0\) for \(u_L < u < u_R\) 

Again, if we try to extend, we get \(U^{\epsilon} \to u_L\) and not \(u_R\) since \(g\) is negative

Cole Hopf transformation

Take Burger's equation:

\(F(u)=\frac{1}{2}u^2\)

\(u_t + (\frac{1}{2}u^2)_x = \epsilon u_{x x}\) 

Define \(w(x,t)\coloneqq \int_{-\infty}^{x} u(x^{\prime} , t) \,\mathrm{d}x^{\prime}  \) 

Then, we get:

\(w_t + \frac{1}{2} w_x ^2 = \epsilon w_{x x}\) 

\(w(x,0)=w_0(x) \coloneqq \int_{-\infty}^{x} u_0(x^{\prime}) \,\mathrm{d}x^{\prime}  \) 

[Assuming \(u_0 \to 0\) as \(\vert x \vert \to \infty\)]

Let \(v \coloneqq \phi(w)\) where \(\phi:\mathbb{R} \to \mathbb{R}\) to be determined.

Is there a \(\Phi\) that would give us a `nicer' solution?

\(v_t = \phi^{\prime} (w) w_t\)

\(v_x = \phi^{\prime} (w) w_x\)

\(v_{x x} = \phi^{\prime\prime} (w) w_x^2 + \phi^{\prime} (w) w_{x x}\) 

Multiply the PDE by \(\phi^{\prime} (w)\)

\(\phi ^{\prime} (w) w_t + \frac{1}{2} \phi^{\prime} (w) w_x^2 = \epsilon \phi^{\prime} (w) w_{x x}\) 

\(\implies v_t + \frac{1}{2} \phi^{\prime}(w)w_x^2 = \epsilon \phi ^{\prime} (w) w_{x x}\) 

\(\implies v_t = - \frac{1}{2}\phi^{\prime}(w) w_x^2 + \epsilon \phi ^{\prime} (w) w_{x x}\) 

\(\implies v_t = - \frac{1}{2} \phi^{\prime} (w) w_x ^2 + \epsilon v_{x x} - \epsilon \phi^{\prime\prime} (w) w_x^2\) 

\(\implies v_t = \epsilon v_{x x} - \left( \frac{1}{2} \phi^{\prime}(w) + \epsilon \phi^{\prime\prime} (w) \right) w_x^2 \) 

Choose \(\phi\) such that: \(\epsilon\phi^{\prime\prime} (w) + \frac{1}{2} \phi ^{\prime} (w) = 0\)  

Let \(\psi \coloneqq \phi ^{\prime} \).

We have the ODE \(\psi^{\prime} = - \frac{1}{2\epsilon} \psi\)

So, \(\psi = e^{-\frac{1}{2 \epsilon}w}\) 

So, \(\phi = \int \psi = e^{-\frac{1}{2 \epsilon}w}\) 

[we want any solution so constant doesn't matter]

For this choice of \(v\):

\(v\) solves \(v_t = \epsilon v_{x x}\) 

This is a heat equation!

\(v(x,0)= e^{-\frac{1}{2 \epsilon}w_0(x)}=e^{-\frac{1}{2\epsilon}\int_{-\infty}^{x} u_0(x^{\prime}) \,\mathrm{d}x^{\prime}  }\) 

Solution explicit using the heat kernel!

\hrulefill

Class 31: 04/24

Recap:

\underline{Cole-Hopf Transformation} 

\(u_t + u u_x = \epsilon u_{x x}\)

\(u(x,0) = u_0(x)\) 

\((F(u) = \frac{1}{2} u^2)\), Burger's Flux

\[
    w(x,t) \coloneqq \int_{-\infty}^{x} u(x^{\prime} , t) \,\mathrm{d}x^{\prime}  
\]

Make a good choice of function \(\phi\) so that:

\(v(x,t) \coloneqq \phi(w(x,t))\) 

solves a simpler equation

\[
    v_t = \epsilon v_{x x} - \left(\frac{1}{2}\phi^{\prime}(w) + \epsilon \phi^{\prime\prime} (w)\right)w_x^2
\]

Choose \(\phi\) such that \(\epsilon \phi^{\prime\prime} + \frac{1}{2} \phi^{\prime} (w) = 0\)

\(\psi \coloneqq \phi ^{\prime} \)

\(\psi ^{\prime}  = -\frac{1}{2\epsilon} \psi\)

\(\psi = e^{- \frac{1}{2\epsilon}w}\)

\(\phi = c_1 e^{-\frac{1}{2\epsilon}w}+c_2\)

So \(\phi = e^{-\frac{1}{2\epsilon}w}\) works

So, our final equation is:

\[
    v_t = \epsilon v_{x x}
\]

With initial condition:

\[
    v(x,0) = \phi(w(x,0)) = \phi\left( \int_{-\infty}^{x} u(x^{\prime},t) \,\mathrm{d}x^{\prime}   \right) 
\]

\[
    v(x,0) = \exp\left( -\frac{1}{2\epsilon} \int_{-\infty}^{x} u_0(x^{\prime}) \,\mathrm{d}x^{\prime}   \right) 
\]

Solving [using the heat kernel]

\[
    v(x,t) = \int_{-\infty}^{\infty} \Phi(x-y,t) v(y,0) \,\mathrm{d}y 
\]

\[
    v(x,t) = \frac{1}{\sqrt{4 \pi \epsilon t}} \int_{-\infty}^{\infty} \exp\left( \frac{-(x-y)^2}{4 \epsilon t}\right) \exp \left( -\frac{1}{2\epsilon} \int_{-\infty}^{y} u_0(x^{\prime}) \,\mathrm{d}x^{\prime}  \right)   \,\mathrm{d}y 
\]

\[
    v(x,t) = \frac{1}{\sqrt{4 \pi \epsilon t}} \int_{-\infty}^{\infty} e^{ - \left[ \frac{(x-y)^2}{4t} + \frac{1}{2} \int_{-\infty}^{x} u_0(x^{\prime}) \,\mathrm{d}x^{\prime}   \right] \big/ \epsilon } \,\mathrm{d}y 
\]

Now, \(v = e^{-\frac{1}{2 \epsilon}w} \iff w = - 2 \epsilon \ln v\)

\(u = w_x = - 2\epsilon \frac{v_x}{v}\) 

Thus,

\[
    u(x,t) = \frac{ - 2 \epsilon \int_{-\infty}^{\infty} -\frac{1}{2} \frac{x-y}{\epsilon t} e^{-\left[ \frac{(x-y)^2}{4t} + \frac{1}{2} \int_{-\infty}^{y} u_0(x^{\prime}) \,\mathrm{d}x^{\prime} \right]  \big/ \epsilon} \,\mathrm{d}y}{ \int_{-\infty}^{\infty} e^{-\left[ \frac{(x-y)^2}{4t} + \frac{1}{2} \int_{-\infty}^{y} u_0(x^{\prime}) \,\mathrm{d}x^{\prime} \right]  \big/ \epsilon} \,\mathrm{d}y }
\]

We want to take \(\epsilon \to 0\) in this equation.

Laplace's Method for taking the limit

\[
    \lim_{\epsilon \to 0} \left[\int_{-\infty}^{\infty} f(y) e^{- \frac{1}{\epsilon } \phi(y)} \,\mathrm{d}y \right] \bigg / \int_{-\infty}^{\infty} e^{- \frac{1}{\epsilon} \phi(y)} \,\mathrm{d}y 
\]

Assume \(\phi\) has unique minimum at \(y = y_0\) 

Then only the point around the minimum matters because of the \(-\frac{1}{\epsilon}\) term.

Idea: Expand \(\phi(y) \approx \phi(y_0) + \frac{1}{2} \phi^{\prime\prime} (y_0) (y - y_0)^2\) [no linear term since \(y_0\) is minimum]

\begin{theorem}
    Assume \(f, \phi\) are continuous and \(\exists ! y_0\) such that \(\phi(y) \geq \phi(y_0) + c_0 \vert y - y_0 \vert ^2 \forall y\in\mathbb{R}\)
    
    Assume \(\vert f(y) \vert \leq c_2 + c_3 \vert y \vert\) . Then,

    \[
        \lim_{\epsilon \to 0} \frac{ \int_{-\infty}^{\infty} f(y) e^{ - \frac{1}{\epsilon}\phi(y)} \,\mathrm{d}y }{\int_{-\infty}^{\infty} e^{-\frac{1}{\epsilon}\phi(y)} \,\mathrm{d}y } = f(y_0)
    \]
\end{theorem}

\begin{proof}
    Define \(\mu_{\epsilon}(y) \coloneqq \frac{e^{(\phi(y_0)-\phi(y)) / \epsilon}}{\int_{-\infty}^{\infty} e^{(\phi(y_0) - \phi(y)) / \epsilon} \,\mathrm{d}y } \) 

    Observations:

    \(u_{\epsilon} > 0 \)
    
    \(\int_{-\infty}^{\infty} u_{\epsilon}(y) \,\mathrm{d}y = 1\)
    
    Since \(e^{(\phi(y_0)-\phi(y)) / \epsilon} \leq e^{c_0(y - y_0^2) / \epsilon}\) 

    So, \(\nu_{\epsilon} \to 0\) exponentially first away from \(y_0\)
    
    Note:

    \[
        \int_{-\infty}^{\infty} e^{- c_0(y - y_0)^2 / \epsilon} \,\mathrm{d}y \simeq \sqrt{\epsilon}  
    \]

    by letting \(s = \frac{y - y_0}{\sqrt{\epsilon}}, \mathrm{d} s = \frac{1}{\sqrt{\epsilon}}\mathrm{d} y\)
    
    We've seen for such a sequence:

    \[
        \left\vert\int_{-\infty}^{\infty} f(y) \mu_{\epsilon}(y)\,\mathrm{d}y - f(y_0)\right\vert 
    \]

    \[
        \left\vert\int_{-\infty}^{\infty} [f(y)-f(y_0)] \mu_{\epsilon}(y)\,\mathrm{d}y\right\vert 
    \]

    \[
        \leq \int_{\{ y : \vert y - y_0 \vert < \delta \} } \cdot + \int_{\{ y : \vert y - y_0 \vert \geq \delta \} } \cdot \to 0
    \]

\end{proof}

Now, recall:

\[
    u(x,t) = \frac{ - 2 \epsilon \int_{-\infty}^{\infty} -\frac{1}{2} \frac{x-y}{\epsilon t} e^{-\left[ \frac{(x-y)^2}{4t} + \frac{1}{2} \int_{-\infty}^{y} u_0(x^{\prime}) \,\mathrm{d}x^{\prime} \right]  \big/ \epsilon} \,\mathrm{d}y}{ \int_{-\infty}^{\infty} e^{-\left[ \frac{(x-y)^2}{4t} + \frac{1}{2} \int_{-\infty}^{y} u_0(x^{\prime}) \,\mathrm{d}x^{\prime} \right]  \big/ \epsilon} \,\mathrm{d}y }
\]

Consider \(u_0(x) = \begin{dcases}
    0, &\text{ if } x < 0 ;\\
    1, &\text{ if } x > 0 ;
\end{dcases}\) 

Non intersecting characteristics.

We had two solutions:

shock by: \(u = 0, u=1\) broken by \(t=2x\), not entropy satisfying

rarefaction wave: interpolating by \(u = x / t\) 

Now, fix \(x\in\mathbb{R}, t > 0\)

\(\phi(y) = \frac{(x-y)^2}{4t} + \frac{1}{2} \int_{-\infty}^{y} u_0(x^{\prime}) \,\mathrm{d}x^{\prime} \) 

\(\phi^{\prime}(y) = -\frac{1}{2t}(x-y) + \frac{1}{2}u_0(y)\) 

For \(y < 0\), \(\phi(y) = \frac{(x-y)^2}{4t}\) 

For \(y > 0\), \(\phi(y) = \frac{(x-y)^2}{4t} + y\) 

Case 1: \(x\) is negative.

\(\phi^{\prime} (y) = 0 \implies (x-y) = t u_0(y) \implies y = x - t u_0(y)\)

Suppose \(x < 0\)

Minimum occurs at \(y = x\) [minimum of \(\phi = 0\)]


By laplace's method:

\[
    \lim_{\epsilon \to 0} u^{\epsilon}(x,t) = `f(x)` = 0  
\]

Since \(f = \frac{x-y}{t}\) 

Case 2: suppose \(x > t\) 

\(\phi^{\prime} (y) = 0\)

So \(-\frac{1}{2t}(x-y) + \frac{1}{2} = 0\) if \(y > 0\) 

So, \(y = x - t\) if \(y > 0\) 

If \(y < 0\),

\(-\frac{1}{2t}(x-y) = 0 \implies x = y\) which is not possible since \(x > t > 0\) 


Then, \(\lim_{\epsilon \to 0} u^{\epsilon} (x,t) = \frac{x-(x-t)}{t} = 1  \) 

Case 3: \(0 < x < t\) 

Note \(\phi^{\prime} (y) < 0\) if \(y < 0\) 

\(\phi^{\prime} (y) > 0\) if \(y > 0\) 

Minimum at \(y = 0\) 

Laplace implies \(\lim_{\epsilon \to 0} u^{\epsilon}(x,t) = f\) evaluated at \(y=0\) which equals \(\frac{x-0}{t}=\frac{x}{t}\) which is the rarefaction wave.   

\hrulefill

Class 32: 04/26

Final Exam: Friday May 3, 10:20 - 12:20, RH104.

5 questions

Some will be exactly the homework

Some will be a minor change. Memorizing would not help.

Formulas: If there are any question about 2/3 dim wave equation, the formula will be given.

If needed formula of heat kernel will be given.

Should remember the formula of fourier transform.

Fourier transform of gaussian will be given if needed.

Remember d'Alembert's formula if needed.

Remember solving first order ODEs. Can come up in the method of characteristics.

Remember RH

\underline{Topics}

- Laplace/Poisson

- Heat Equation / Fourier Transformation

- Wave equation, 1,2,3 dimensions

- Method of Characteristics

- Conservation Laws

\underline{Not on Exam}:

- Duhenel's Principle

- Lax-Oleinik

- Cole-Hopf Transformation

- Viscous Conservation Laws (travelling wave approximation of shocks)

\section*{Review}

\subsection*{Method of Characteristics}

Consider:

\[
    a(x,y,u) u_x + b(x,y,u) u_y = c(x,y,u)
\]

\[
    u = f(x,y) \text{ on } \Gamma \subset \mathbb{R}^2
\]

Let \((x_0(s),y_0(s))\) be a parametrization of the curve \(\Gamma\) 

Then:

\(\frac{\partial X}{\partial \tau} a(X,Y,U)\) and \(X(s,0)=x_0(s)\) 

\(\frac{\partial Y}{\partial \tau } = b(X,Y,U)\) ad \(Y(s,0)=y_0(s)\) 

\(\frac{\partial U}{\partial \tau} = c(X,Y,U)\) and \(U(s,0)=f(x_0(s),y_0(s))\) 

We solve this system. We literally solve or show a solution exists. Then we get:

\(X(s,\tau),Y(s,\tau),U(s,\tau)\)

Now (try to) invert. Look at the inverse function theorem.

Consider the Jacobian of the map. We try to see if there is inverse near \(\Gamma\).  So we look at:

\[
    \det \begin{pmatrix}
        \frac{\partial X}{\partial s} (s,0) & \frac{\partial X}{\partial \tau}(s,0)   \\
        \frac{\partial Y}{\partial s} (s,0) & \frac{\partial Y}{\partial \tau}(s,0) \\
    \end{pmatrix}
\]

\[
    = \det \begin{pmatrix}
        x_0^{\prime}(s) &  a(x_0(s),y_0(s), f(x_0(s),y_0(s))) \\
        y_0^{\prime}(s) & b(x_0(s),y_0(s),f(x_0(s),y_0(s)))  \\
    \end{pmatrix}
\]

If \(\det \neq 0\) there exists an inverse by inverse function theorem.

\underline{Note}: if \(\det = 0\) [\(\Gamma\) is ``characteristic''] that doesn't necessarily mean no inverse exists!

In this case, one idea is: use initial data \(u=f(x,y)\) on \(\Gamma\), differentiate this to get a differential equation that disagrees with the original differential equation. That is a way to show that no solution exists.

\[\frac{\partial}{\partial s}|_\Gamma u = \frac{\partial}{\partial s} f(x_0(s),y_0(s))\] 

If there is a characteristic question, it will be `sketch the characteristics'. Once we get \(X(s,\tau), Y(s,\tau)\) we can eliminate \(\tau\) to get equations for characteristics.

\subsection*{Conservation Laws}

\[
    u_t + F(u)_x = 0
\]

\[
    u_t + F^{\prime}(u)u_x = 0
\]

Initial condition is usually \(u(x,0)=u_0(x)\) 

We don't need \(\tau\) since \(t\) plays that role. Then,

\(\frac{\partial X}{\partial t} = F^{\prime}(U)\) with \(X(s,0)=s\) 

\(\frac{\partial U}{\partial t} = 0\) with \(U(s,0)=u_0(s)\) 

Since \(\frac{\partial U}{\partial t} = 0\) \(U(s,t)=U(s,0)\), aka along characteristics \(U\) is constant.

Then, \(\frac{\partial X}{\partial t} = F^{\prime}(u_0(s)) \implies X(s,t)=F^{\prime} (u_0(s))t + s\) [lines]

Characteristics are lines with slope \(F^{\prime} (u_0(s))\) 

For a solution to be classical, we don't want lines to intersect.

So, we want them to fan out.

So, we want \(\frac{\mathrm{d}t}{\mathrm{d}x}\) dcreasing.

Note that \(\frac{\mathrm{d}x}{\mathrm{d}t} = F^{\prime}(u_0(s))\)

Thus, \(\frac{\mathrm{d}t}{\mathrm{d}x}\) decreasing \(\iff s \mapsto F^{\prime} (u_0(s))\) increasing.

If there exists interval where the function decreases then the function must intersect, so there can't be a classical solution.

When does the solution stop existing? We can just draw lines:

Let it be decreasing on \(\overline{s}, \overline{\overline{s}} \) 

\(x = F^{\prime}(u_0(\overline{s}))t + \overline{s} = F^{\prime} (u_0(\overline{\overline{s}}))t + \overline{\overline{s}}  \) 

\(t = \frac{\overline{\overline{s}} - \overline{s}}{F^{\prime} (u_0(\overline{s}))-F^{\prime}(u_0(\overline{\overline{s}}))}\) 

\(t = \frac{-1}{\frac{F^{\prime}(u_0(\overline{s}))-F^{\prime}(u_0(\overline{\overline{s}}))}{\overline{s} - \overline{\overline{s}}  }}\) 

The denominator is a derivative by mean value theorem. So, first collision:

\[
    t^{\ast} = \frac{-1}{\min \frac{\partial}{\partial s} (F^{\prime}(u_0(s)))}
\]

\section*{Weak Solution}

\(u\) is a weak solution if for all test functions \(\phi\in C^1_0\) [compactly supported, first derivative exists] we have:

\[
    \int_{}^{} \int_{}^{} \phi_t u + \phi_x F(u) \,\mathrm{d}x  \,\mathrm{d}t = 0
\]

Examples:

1: Shocks with \(u_L, u_R\) seperated by \(x=s(t)\) provided \(u_L, u_R\) are classical and \(s^{\prime} (t) = \frac{F(u_L)-F(u_R)}{u_L - u_R}\) evaluated on the shock. 

\underline{Entropy-Satisfying Shock}

We want characteristics to run into shock.

So, we want: \(F^{\prime} (u_R) < s^{\prime} (t) < F^{\prime} (u_L)\) 

This is called the weak entropy condition

If entropy is not satisfied:

\underline{Rarefaction wave}: solutions that are constant along rays. So, \(u(x,t) = v(\frac{x - a}{t})\) 

Burger's: \(v(\eta)=\eta\) 



\end{document}

