\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{tikz-cd}
\usepackage{geometry}
    \geometry{
        a4paper,
        left = 40mm,
        top = 20mm,
        right = 40mm,
        bottom = 30mm
    }
\setlength{\parindent}{0pt}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newcommand{\ad}{\operatorname{ad}}
\newcommand{\Ad}{\operatorname{Ad}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\Lie}{\operatorname{Lie}}

\title{Lie Groups and Lie Algebras Math 508}
\author{Taught by: Dr. Matvei Libine \\ Written by: Thanic Nur Samin}
\date{\vspace{-5ex}}

\begin{document}

\maketitle

Prof: Matvei Libine

Class 1: 01/09

\section*{Semisimple and Reductive Lie Algebras}

\begin{definition}[textbook, non-standard]
    A complex lie algebra \(\mathfrak{g} \) is called reductive if there exists a compact (matrix) lie group \(K\) with Lie algebra \(\mathfrak{k} \) so that \(\mathfrak{g} =\mathfrak{k} _\mathbb{C} =\mathfrak{k} +i\mathfrak{k} \)

    A complex lie algebra \(\mathfrak{g} \) is \underbar{semisimple} if it is reductive and its center is trivial.
\end{definition}

\begin{definition}[standard, equivalent]
    A lie algebra (real or complex) \(\mathfrak{g} \) is:
    \begin{itemize}
        \item \underbar{simple} if the only ideals of \(\mathfrak{g} \) are \(0,\mathfrak{g} \) and \(\mathfrak{g} \) is not commutative (or \(\dim\mathfrak{g} \geq 2\) or \(\mathfrak{g} \neq \mathbb{R} ,\mathbb{C} \) )
        \item \underbar{semisimple} if \(\mathfrak{g} =\bigoplus \text{(simple ideals)} \) 
        \item \underbar{reductive} if \(\mathfrak{g}=\text{center} \oplus \text{(semisimple ideal)} =\text{(commutative ideal)} \oplus \bigoplus \text{(simple ideals)} \) 
    \end{itemize}
\end{definition}

\begin{definition}[Compact Real Forms]
    Let \(\mathfrak{g}\) be a complex semisimple lie algebra. A real subalgebra \(\mathfrak{g} \) is a \underbar{compact real form} of \(\mathfrak{g} \) if \(\mathfrak{k} \) can be realized as the lie algebra of some compact (matrix) lie group and \(\mathfrak{g} =\mathfrak{k} \otimes \mathbb{C} \) : every element \(Z\in \mathfrak{g} \) can be written uniquely as  \(Z=X+iY, X,Y\in \mathfrak{k} \) 
\end{definition}

Fact: Compact real forms exist and are essentially unique.

By essentially unique we mean that if \(\mathfrak{k} ,\mathfrak{k}^{\prime} \subset \mathfrak{g} \) are two compact real forms, then there exists a lie algebra automorphism \(\mathfrak{g} \to \mathfrak{g} \) so that \(\mathfrak{k}\to \mathfrak{k}^{\prime}  \) is an isomorphism. 

Example:

\(\mathfrak{sl}(n,\mathbb{C} ),n\geq 2,\mathfrak{so}(n,\mathbb{C} ),n\geq 3 \) are semisimple.

Their compact real forms are \(\mathfrak{su}(n) \) and \(\mathfrak{so}(n) \) respectively.

\(\mathfrak{gl} (n,\mathbb{C} ),n\geq 1,\mathfrak{so}(2,\mathbb{C} )\simeq\mathbb{C}  \) are reductive lie algebras. Their compact real forms are \(\mathfrak{u}(n) \) and \(\mathfrak{so}(2,\mathbb{R}) \) respectively.

Explanation: \(SU(n),SO(n),U(n)\) are compact matrix lie groups with lie algebras \(\mathfrak{su}(n),\mathfrak{so} (n),\mathfrak{u} (n) \).

\(\mathfrak{su} (n)\otimes \mathbb{C} =\mathfrak{sl} (n,\mathbb{C} ),\mathfrak{so}(n)\otimes \mathbb{C} =\mathfrak{so}(n,\mathbb{C} ),\mathfrak{u}(n)\otimes \mathbb{C} =\mathfrak{gl} (n,\mathbb{C} )   \) 

\begin{proposition}
    Let \(\mathfrak{g} =\mathfrak{k} _\mathbb{C} \) be a reductive lie algebra. Then there exists an inner product on \(\mathfrak{g}\) that is real valued on \(\mathfrak{k} \) such that the adjoint action of \(\mathfrak{k} \) on \(\mathfrak{g} \) is unitary. By unitary we mean:

    \begin{equation}\tag{\(*\) }
        \langle \ad_x y,z  \rangle +\langle y,ad_x z \rangle = 0
    \end{equation}

    This means \(\ad_x\) is skew-adjoint.

    Define an operation \(z\mapsto z^{\ast}\) on \(\mathfrak{g} \) by \((x+iy)^{\ast} =-x+iy,x,y\in \mathfrak{k} \)  

    Then any inner product satisfying \((*)\) also satisfies

    \[
        \langle ad_x y,z \rangle = \langle y,ad_{x^{\ast}} z \rangle 
    \]

\end{proposition}

Motivation: If \(\mathfrak{g} =\mathfrak{gl} (n,\mathbb{C} )\) and \(\mathfrak{k} =\mathfrak{u} (n)\) , then the definition of \(*\) is the matrix adjoint (conjugate+transpose) \(A\mapsto \overline{A^{\top} } \) 

\begin{proof}
    Let \(K\) be a compact (matrix) lie group with lie algebra \(\mathfrak{k} \). \(K\) acts on \(\mathfrak{k},\mathfrak{g} \) via the \(\Ad\) action.
    
    Find a \(K\) - invariant inner product on \(\mathfrak{k} \) and extend it to \(\mathfrak{g} =\mathfrak{k} _\mathbb{C} \), conjugate-linear w.r.t. first variable. Since Ad action of K on \(\mathfrak{g} \) is unitary, for \(\mathfrak{k} \) it implies the relation \((*)\)  
    
\end{proof}

\begin{proposition}
    Suppose \(\mathfrak{g} =\mathfrak{k} _\mathbb{C} \) is a reductive lie algebra. Choose an inner product on \(\mathfrak{g} \) as above. Then if \(\mathfrak{h} \) is an ideal in \(\mathfrak{g} \) , so is \(\mathfrak{h} ^{\perp} \) and we also have \(\mathfrak{g} =\mathfrak{h} \oplus \mathfrak{h} ^{\perp} \) as lie algebra.
\end{proposition}

\begin{proof}
    \(\mathfrak{h} ^{\perp} \) is a vector subspace of \(\mathfrak{g} \) from basic linear algebra. We want to prove that it is an ideal.

    Let \(x\in \mathfrak{g} ,y\in \mathfrak{h} ^{\perp} \)

    We want to show that \([x,y]=\ad_x y\in \mathfrak{h} ^{\perp} \) 

    Pick any \(z\in \mathfrak{h}\). Then we have,
    
    \(\langle \ad_x y,z \rangle = \langle y,ad_{x^{\ast} }z \rangle \) 

    Since \(y\in \mathfrak{h} ^{\perp} \) and \([x^{\ast} ,z]\in \mathfrak{h}, \langle y,\ad_{x^{\ast} }z \rangle =0\). So, \(\langle \ad_x y,z \rangle =0\) which means \([x,y]\in \mathfrak{h}^{\perp} \) which implies that \(\mathfrak{h} ^{\perp} \) is an ideal.
    
    Now, note that \(\mathfrak{g} =\mathfrak{h} \oplus \mathfrak{h} ^{\perp} \) as vector space. For \(x\in \mathfrak{h} ,y\in \mathfrak{h} ^{\perp} ,[x,y]\in \mathfrak{h} \cap \mathfrak{h} ^{\perp} =\{0\}\implies [x,y]=0\) so this is true as lie algebras.
    
\end{proof}

\begin{proposition}
    Every reductive (complex) lie algebra \(\mathfrak{g} \) decomposes as \(\mathfrak{g} =\mathfrak{z} \oplus \mathfrak{g}_{s} \) (direct sum of ideals) where \(\mathfrak{z} =\text{ center of } \mathfrak{g} \) and \(\mathfrak{g} _{s} \) is semisimple. Note that \(\mathfrak{g} _{s} \) can be recovered from \(\mathfrak{g} \) as \(\mathfrak{g} _{s} =[\mathfrak{g} ,\mathfrak{g} ]\) .
\end{proposition}

\begin{proof}
    The center \(\mathfrak{z} \) is an ideal of \(\mathfrak{g}\). We define \(\mathfrak{g} _{s} =\mathfrak{z} ^{\perp} \) . From the previous proposition, we only need to prove that \(\mathfrak{g} _{s} \) is semisimple.

    Suppose \(z\) is in the center of \(\mathfrak{g} _{s} \) . Then it must also be in the center of \(\mathfrak{g} \) . Then we have \(z\in \mathfrak{g} _{s} \cap \mathfrak{z} =\{0\}\) . So, \(\text{center}(\mathfrak{g}_{s})=\{0\} \)
    
    We need to show that there exists a compact group \(K^{\prime} \) with lie algebra \(\mathfrak{k}^{\prime} \) so that \(\mathfrak{g} _{s} =\mathfrak{k}^{\prime} \otimes \mathbb{C} \).

    Let \(\mathfrak{k} \) be the compact real form of \(\mathfrak{g} \) . If \(z=x+iy\in \mathfrak{z} ,x,y\in \mathfrak{k} \) , then \(z\) commutes with \(\mathfrak{k} \) and \(x,y,x-iy\in \mathfrak{z} \implies \mathfrak{z} =(\mathfrak{z} \cap \mathfrak{k} )_\mathbb{C}\) and \(\mathfrak{g} _{s} =\mathfrak{z} ^{\perp} =(\mathfrak{g} _{s} \cap \mathfrak{k} )_\mathbb{C} \) 

    Also, note that \(\Ad:K \to GL(\mathfrak{k} )\) 

    Define \(K^{\prime} \coloneqq\) image of this map.
    
    \(K^{\prime} =\Ad(K)\subset GL(\mathfrak{k})\) 

    Since \(K\) is compact and \(\Ad\) is continuous, \(K^{\prime} \) is also compact.

    Lie algebra tells us, since \(\Ad:K \to GL(\mathfrak{k} )\) we have \(\ad:\mathfrak{k} \to \mathfrak{gl} (\mathfrak{k} )\).

    kernel of \(\ad\) is \(\mathfrak{z} \cap \mathfrak{k} \) and so \(\mathfrak{k}^{\prime} \simeq \mathfrak{k}/(\mathfrak{z}\cap \mathfrak{k} )=\mathfrak{g} _{s} \cap \mathfrak{k} \). So, finally, \(\mathfrak{g} _{s} =(\mathfrak{g}_s\cap \mathfrak{k} )_\mathbb{C} =\mathfrak{k}^{\prime} _\mathbb{C} \)  

\end{proof}

\begin{proposition}
    If \(K\) is simply connected compact matrix lie group with lie algebra \(\mathfrak{k} \) then \(\mathfrak{g} =\mathfrak{k} _\mathbb{C} \) is semisimple.
\end{proposition}

\begin{proof}
    We need to prove that \(K\) being simply connected fores the center of \(\mathfrak{k} \) to be \(0\).

    \underbar{Caution:} \(K\) may have non-trivial (discrete) center. If \(K=SU(2)\) which is \(\sim S^3\) simply connected, we have \(\text{center}(SU(2))=\{\pm I\}\) 
\end{proof}

\hrule
\hfil

Class 2: 01/11

Recall the textbook and non-standard definitions of semisimple and reductive lie algebras.

Most of the time problem with textbook definition is finding the \(K\)-compact lie group, otherwise it is simpler. This is the tradeoff.

\begin{proposition}
    If \(K\) is simply connected (matrix) lie group with lie algebra \(\mathfrak{k} \) then \(\mathfrak{g}  = \mathfrak{k} _\mathbb{C} \) is semisimple.
\end{proposition}

\begin{proof}
    We need to show that: \(\text{center}(\mathfrak{g})=\{ 0 \} \)
    
    \(\mathfrak{k} \) decomposes as lie algebra \(\mathfrak{k} =\mathfrak{z}^{\prime} \oplus \mathfrak{k} _{s} \). We have, \(\mathfrak{z}^{\prime} =\text{center}(\mathfrak{k} )  \)
    
    Earlier Result: (Theorem 5.11, where we need \(\mathfrak{k} \) to be simply connected)

    \(K=Z^{\prime} \times K_{s} \) where \(Z^{\prime} \) and \(K_{s} \) are closed simply connected subgroup of \(K\) with lie algebra \(\mathfrak{z}^{\prime}  \) and \(\mathfrak{k} _{s} \) respectively.

    \(K\) is simply connected.

    \(\mathfrak{k} =\mathfrak{k}_1 \oplus \mathfrak{k}_2\) 

    \(\mathfrak{k} \to \mathfrak{k} _1, K_1 =\)

    \(\mathfrak{k} \to \mathfrak{k} _2, K_2=\) 
    
    If \(Z^{\prime} \) is simply connected, it cannot be compact.

    So, \((\mathbb{R}^{\dim \mathfrak{z}^{\prime} },+)\) is a simply connected lie group with lie algebra \(\mathfrak{z}^\prime \simeq Z^{\prime} \)  

    So it is simply connected and compact. This forces \(\mathfrak{z}^{\prime} =\{ 0 \} \) .

\end{proof}

\begin{theorem}
    Suppose \(\mathfrak{g} \) is semisimple. Then, \(\mathfrak{g} =\bigoplus_{j=1}^{m} \mathfrak{g} _{j} \) [direct sum of ideals with each of them simple.] 

\end{theorem}

This bridges the gap between the definition. Also we study the finite sum, because in the infinite case none of the theory applies.

\begin{proof}
    Recall that simple means no proper ideals and dimension at least \(2\).
    
    If \(\mathfrak{h} \subset \mathfrak{g} \) is an ideal, then \(\mathfrak{g} =\mathfrak{h} \oplus \mathfrak{h} ^{\perp} \)

    [If we keep decomposing like that, since the dimension of each are decreasing, at some point we must stop]

    If \(\mathfrak{h}^{\prime} \subset \mathfrak{h} \) is an ideal, we need it to be an ideal of \(\mathfrak{g} \) as well. This is true since it is perpendicular to \(\mathfrak{h} ^{\perp} \).

    Since \(\mathfrak{h}^\prime\) is an ideal of \(\mathfrak{g} \) we see that \((\mathfrak{h}^{\prime}) ^{\perp}\cap \mathfrak{h} =\mathfrak{h}^{\prime\prime}   \) 

    So we have \(\mathfrak{g} =\mathfrak{h}^{\prime} \oplus \mathfrak{h}^{\prime\prime} \oplus \mathfrak{h}^{\perp}  \) 

    We can keep doing this.

\end{proof}

\begin{proposition}
    If \(\mathfrak{g} \) is (complex) semisimple then the ideals \(\mathfrak{g}_{j} \) in the decomposition \(\mathfrak{g}=\bigoplus_{i=1}^{m} \mathfrak{g}_{j}\) are unique.  
\end{proposition}

Clarification: if \(\mathfrak{g}=\bigoplus_{j=1}^{m} \mathfrak{g}_{j} =\bigoplus_{k=1}^{n} \mathfrak{g}^{\prime} _{k}   \) are two possible decompositions then we have \(m=n\) and we also have some permutation \(\sigma \) so that \(\mathfrak{g} _{j} =\mathfrak{g}^{\prime\prime}_{\sigma (j)}\) and these are the same vector subspaces of \(\mathfrak{g} \).

The similar statement fails for reductive lie algebras. (WHY???)

The statement still applies to the semisimple part though.

\begin{proof}
    We use representation theory. Treat \(\mathfrak{g} \) as a representation over itself via the adjoint action \(\ad\).

    Then ideals of \(\mathfrak{g} \) are subrepresentations of \((\ad,\mathfrak{g} )\).

    An ideal of \(\mathfrak{g} \) not containing any proper ideals are irreducible subrepresentations of \((\ad,\mathfrak{g} )\).

    Therefore, we have the decomposition \(\mathfrak{g} =\bigoplus_{j=1}^{m} \mathfrak{g} _{j} \) each of them an irrdeucible subrepresentation and are pairwise non-isomorphic, then \(\mathfrak{g} \) acts on \(\mathfrak{g} _{j}  \) in such a way that only \(\mathfrak{g}_{j} \) acts non-trivially on \(\mathfrak{g}_{j} \), any \(\mathfrak{g}_{k} \) where \(k\neq j\) annihilates \(\mathfrak{g} _{j} \).  

    Now suppose \(\mathfrak{h}\subset \mathfrak{g} \) is a simple ideal. For each \(j,1\leq j\leq m\), we have a projection \(\pi _j:\mathfrak{g}\to \mathfrak{g} _{j} \) which is identity on \(\mathfrak{g}_{j} \) and \(0\) everywhere else.
        
    These \(\pi_{j} \) are intertwining maps of representation of \(\mathfrak{g} \) [They commute with the action of the lie algebra].

    Thus, by Schur's lemma, each \(\pi_j|_{\mathfrak{h} }\) is either \(0\) or an isomorphism. 

    Then exactly one \(\pi_k|_\mathfrak{h} \) is an identity all others are \(0\).

    So each component of the decomposition comes from a subrepresentation and those are unique, so we're done.

\end{proof}

\section*{Invariant Bilinear Forms and Semisimplicity, Killing Form}

Doing this is very difficult from the textbook definitions.

Let \((\rho ,V)\) be a representation of \(\mathfrak{g} \), then a bilinear form \(B\) on \(V\) is invariant of \(B(\rho (x)v_1,v_2)+B(v_1,\rho(x)v_2)=0\) 

Special case: for adjoint representation \(\ad,\mathfrak{g} \) a bilinar form \(B\) on \(\mathfrak{g} \) is ad-invariant if \(B([x,y],z)+B(y,[x,z])=0\).

Example: we can take \(\mathfrak{g} =\mathfrak{gl}(n,\mathbb{C} ) \). We can take \(B(X,Y)=\Tr(XY)\). This is symmetrc, nondegenerate, invariant. 

B is \(GL(n,\mathbb{C} )\) invariant. [Didn't understand what was written on the blackboard]

Generalization:

If \((\rho ,V)\) is a representation of \(\mathfrak{g} \), we can define a bilinear form \(B\) on \(\mathfrak{g} \) by \(B_V(X,Y)=\Tr_V(\rho(X)\rho (Y))\) for all \(X,Y\in \mathfrak{g} \) 

This is symmetric, invariant. But it might be degenerate.

Invariance:

\(B_V([X,Y],Z)+B_V(Y,[X,Z])=\Tr_V(\rho(X)\rho (Y)\rho (Z)-\rho (Y)\rho (X)\rho (Z)+\rho (Y)\rho (X)\rho (Z)-\rho (Y)\rho (Z)\rho (X))=0\)

\begin{theorem}
    Let \((\rho ,V)\) be a representation of \(\mathfrak{g} \) and \(B_{V} =\Tr_V(\rho (X)\rho (Y))\) be the corresponding bilinear form. If \(B_V\) is non-degenerate, then \(\mathfrak{g} \) is reductive according to the standard definition.
\end{theorem}

There is a drawback, an uncertainty, we don't know what the representation might be. This get simpler in the semisimple case.

Now take \((\rho ,V)\) to be the adjoint representation \((\ad,\mathfrak{g} )\). Then the corresponding bilinear form is called the Killing Form.

It is defined by \(K(X,Y)=\Tr_{\mathfrak{g} }(\ad X\ad Y)\) 

This is symmetric invariant.

Warning: This notation is ambigious. If \(\mathfrak{h}\subset \mathfrak{g} \) is a subalgebra and we have \(X,Y\in \mathfrak{h} \) then \(K(X,Y)\) may denote \(\Tr_{\mathfrak{g} }(\ad X\ad Y)\) or \(\Tr_{\mathfrak{h}}(\ad X\ad Y) \)

\begin{theorem}
    Cartan's Criterion for Semisimplicity: The lie algebra \(\mathfrak{g} \) is semisimple if and only if its Killing form is non-degenerate.
\end{theorem}

Killing form is discussed in the exercises. In the homework we have to show one direction of this.

\begin{theorem}
    Cartan's criterion for solvability: A lie algebra \(\mathfrak{g} \) is solvable if and only if \(K(X,Y)=0\) for all \(X\in [\mathfrak{g},\mathfrak{g}  ]\) and \(Y\in \mathfrak{g} \)   
\end{theorem}

Last theorem for today.

\begin{theorem}
    Let \(G\) be a connected real lie group. Then \(G\) is compact with finite center if and only if the Killing form on \(\Lie(G)\) is negative definite.
\end{theorem}

Keep in mind: \(G_1=\Pi ^n=(S^1)^n\) (compact), \(G_2=(\mathbb{R} ^n,+)\) (not compact), but both have same lie algebra and Killing form is identically \(0\). So Killing form doesn't necessarily say anything about the lie group.

Next time: Cartan Subalgebra

\hrule
\hfil

Class 3: 01/16

\section*{Cartan Subalgebra}

Goal: Learn about reps and structure of \(\mathfrak{g} \)

Know well reps of abelian lie algebras/groups.

Want this subalgebra to be as large as possible.

\begin{definition}
    Cartan Subalgebras: If \(\mathfrak{g} \) is a complex semisimple lie algebra, then a Cartan Subalgebra of \(\mathfrak{g} \) is a complex subspace \(\mathfrak{h} \) so that:

    \begin{itemize}
        \item \([H_1,H_2]=0 \forall H_1,H_2\in \mathfrak{h}\)
        \item If for some \(X\in \mathfrak{g} \) we have \([H,X]=0\forall H\in \mathfrak{h} \) then \(X\in \mathfrak{h} \) 
        \item For all \(H\in \mathfrak{h} \) we have \(\ad_H:\mathfrak{g} \to \mathfrak{g} \) is diagonalizeable 
    \end{itemize}

\end{definition}

Note, existence would imply, i implies \(\mathfrak{h} \) is abelian, ii implies \(\mathfrak{h} \) is max commutative, and iii implies restriction of a representation of \(\mathfrak{g} \) to \(\mathfrak{h} \) decomposes into \(\oplus \) irreducible 1-dim subrepresentations of \(\mathfrak{h} \). 

If \(K_{\mathbb{R}}\subset G_{\mathbb{R}}\) where \(G_{\mathbb{R} }\) is non-compact real semisimple lie group and \(K_{\mathbb{R} }\) is maximal compact

\begin{proposition}
    (Cartan Subalgebras Exist) Let \(\mathfrak{g} = \mathfrak{k}_{\mathbb{C}}\) be a complex semisimple lie algebra where \(\mathfrak{k} \) is a compact real form of \(\mathfrak{g} \) and lie algebra of a compact lie group \(K\) and let \(\mathfrak{t} \) be the max commutative subalgebra of \(\mathfrak{k} \) . Then \(\mathfrak{h} =\mathfrak{t} _{\mathbb{C} }=\mathfrak{t} +i\mathfrak{t} \) is a Cartan Subalgebra of \(\mathfrak{g} \)  
\end{proposition}

\begin{proof}
    i is automatic.

    ii: Suppose \(X\in \mathfrak{g} \) commutes with every element with \(\mathfrak{h} \). Write \(X=X_1 + i X_2, X_1,X_2\in \mathfrak{k} \).
    
    \(X\) commutes with every \(H\in \mathfrak{t} \subset \mathfrak{h} \) 

    So, \(0=[H,X]=[H,X_1 + i X_2]=[H,X_1]+i[H,X_2]\) 

    Thus, \([H,X_1]=[H,X_2]=0\)
    
    Since \(\mathfrak{t} \) is max commutative we have \(X_1,X_2\in \mathfrak{t} \) so \(X=X_1 + iX_2\in \mathfrak{h} \)
    
    iii: Fix \(H\in \mathfrak{h} \). We need to prove that \(\ad_H\) is diagonalizeable.
    
    \(K\) is compact with lie algebra \(\mathfrak{k} \)

    \(K\) acts on \(\mathfrak{k} \) as well as \(\mathfrak{k} _{\mathbb{C} }\) via \(\Ad\).
    
    Since \(K\) is compact, we can choose a \(K\)-invariant inner product on \(\mathfrak{k} \).

    So, the adjoint action of \(K\) on \(\mathfrak{g} \) is unitary.

    Hence, for each \(H\in \mathfrak{t} \), \((\ad_H)^{\star} =-\ad_H\) [skew adjoint] and hence diagonalizeable.
    
    Finally, if \(H\in \mathfrak{h} \) then \(H=H_1 + i H_2, H_1,H_2\in \mathfrak{t}   \). \(H_1,H_2\) commute thus \(\ad_{H_1},\ad_{H_2}\) commute. So, \(\ad_{H_1},\ad_{H_2}\) can be diagonalized simultaneously.

    So, \(\ad_H\) is diagonalizeable where \(H=H_1 + i H_2\)  

\end{proof}

\begin{definition}
    The rank of \(\mathfrak{g} \) is \(\dim_{\mathbb{C} }\mathfrak{h} \) where \(\mathfrak{h} \) is the cartan subalgebra.
\end{definition}

Example: let \(\mathfrak{g} =\mathfrak{sl}(n,\mathbb{C} )\) which is a complex semisimple (actually simple) lie algebra.

\(\mathfrak{g} =\mathfrak{su}(n)_{\mathbb{C}} \) and \(\mathfrak{t} =\left\{ \text{diagonal matrices in \(\mathfrak{su}(n) \)}  \right\} \)

Then \(\mathfrak{h}=\left\{ \text{diagonal matrices in \(\mathfrak{sl}(n,\mathbb{C})\)}  \right\} \) 

Therefore, the rank of \(\mathfrak{sl}(n,\mathbb{C})=n-1\)

\section*{Roots and Root spaces}

Let \(\mathfrak{g} \) be a complex semisimple lie algebra.

\(\mathfrak{k} \) be the compact real form

\(\mathfrak{g} =\mathfrak{k} _{\mathbb{C} }\) 

\(\mathfrak{k} \) is lie algebra of a compact group \(K\)

Make this choice of Cartan subalgebra. Let \(\mathfrak{t} \subset \mathfrak{k} \) be max commutative and let \(\mathfrak{h} = \mathfrak{t} _{\mathbb{C} }=\mathfrak{t} +i \mathfrak{t} \).

Each \(\ad_H:\mathfrak{g} \to \mathfrak{g} \) is diagonalizeable and all of them commute with each other.

\begin{definition}[root]

    If \(X\in \mathfrak{g}, X\neq 0 \) is a simultaneous eigenvector, then the corresponding eigenvalues depend linearly on \(H\in \mathfrak{h}\). Hence, each eigenvalue can be regarded as an element of \(\mathfrak{h} ^{\star} \).  If it is non-zero, it is called a root.

\end{definition}

If we fix a \(K\)-invariant inner product on \(\mathfrak{g} \) and we have \(\mathfrak{h} \simeq \mathfrak{h} ^{\star} \) (conjugate linear). The element of \(\mathfrak{h} \) corresponding to roots in \(\mathfrak{h}^{\star}  \) are also called roots.

We can define another way.

\begin{definition}
    An element \(\alpha \in \mathfrak{h} , \alpha \neq 0\) is a root (for \(\mathfrak{g} \) relative to \(\mathfrak{h} \) ) if there exists \(X\in \mathfrak{g} , X\neq 0\) so that \(\ad_H(X)=[H,X]= \langle \alpha ,H \rangle X\) for all \(H\in \mathfrak{h} \).
\end{definition}

Notation: \(R\) is the set of all roots \(R \subset \mathfrak{h} \)

\begin{proposition}
    Each root \(\alpha \in i \mathfrak{t} \subset \mathfrak{h} \).
\end{proposition}

\begin{proof}
    By the construction of the inner product, it is real valued on \(\mathfrak{t} \). To prove \(\alpha \in i \mathfrak{t} \) it is sufficient to show that each eigenvalue of every \(\ad_H, H\in \mathfrak{t}\) is purely imaginary. Since \(H\) is skew-adjoint, the eigenvalues are pure imaginary. 
\end{proof}

\begin{definition}
    If \(\alpha \) is a root, then the root space \(\mathfrak{g} _\alpha \) is the space of all \(X\in \mathfrak{g} \) such that \(\ad_H(X) = \langle \alpha ,H \rangle X\) for all \(H\in \mathfrak{h}\). Non-zero elements in \(\mathfrak{g} _\alpha \) are called root vectors for \(\alpha\).
    
    More generally, if \(\alpha\) is any element of \(\mathfrak{h}\),
    
    define \(\mathfrak{g}_\alpha \left\{ X\in \mathfrak{g} : \ad_H(X) = [H,X] = \langle \alpha , H \rangle X \forall H\in \mathfrak{h}  \right\} \)  

\end{definition}

Note that \(\mathfrak{g}_\alpha\) can fail to be a root space for two reasons: i: \(\alpha =0\) and ii \(\mathfrak{g}_\alpha =\{0\} \)  

If \(\alpha = 0, \mathfrak{g}_0 = \left\{ X\in \mathfrak{g}:[X,H]=0 \forall H\in \mathfrak{h}   \right\}  =\mathfrak{h} \) 

If \(\alpha \neq 0\) and \(\alpha \) is not a root, then \(\mathfrak{g}_\alpha =\{0\} \)   

\begin{proposition}
    As a vector space, \(\mathfrak{g}=\mathfrak{h}\oplus \bigoplus_{\alpha \in R}^{} \mathfrak{g}_\alpha\) where \(R\) is the set of all roots.
\end{proposition}

\begin{proposition}
    i- For any \(\alpha ,\beta \in \mathfrak{h} \) we have \([\mathfrak{g}_\alpha , \mathfrak{g}_\beta] \subset \mathfrak{g}_{\alpha + \beta} \) 

    Furthermore, if \([\mathfrak{g}_\alpha ,\mathfrak{g}_{-\alpha }  ] \subset \mathfrak{h} \) since \(\mathfrak{g}_0 = \mathfrak{h}  \)  

    ii- If \(\alpha +\beta \neq 0\) and \(\alpha +\beta \notin R\) then \([\mathfrak{g}_\alpha ,\mathfrak{g}_\beta   ]= \left\{ 0 \right\} \)  

\end{proposition}

\begin{proof}
    We use Jacobi identity.

    \[
        [H,[X,Y]] = [[H,X],Y]+[X,[H,Y]]
    \]

    Let \(H\in \mathfrak{h},X\in \mathfrak{g}_\alpha , Y\in \mathfrak{g}_\beta   \) 

    Then, \([[H,X],Y]= [\langle \alpha ,H \rangle X, Y] \) 

    \([X,[H,Y]]=[X,\langle \beta ,H \rangle Y ]\) 

    Thus, \([H,[X,Y]]\in \mathfrak{g_{\alpha +\beta }} \) 

\end{proof}

\begin{proposition}
    (i) If \(\alpha \in \mathfrak{h} \) is a root, so is \(-\alpha \). Specifically, if \(X\in \mathfrak{g}_{\alpha }, X^{\star} \in \mathfrak{g}_{-\alpha }  \)
    
    (ii) The roots span \(\mathfrak{h} \) (\(\mathfrak{g} \) is semisimple! ) 
    
\end{proposition}

\begin{proof}
    Notation: If \(X=X_1+iX_2,\overline{X}=X_1 -iX_2 \).
    
    Since \([\mathfrak{k},\mathfrak{k}  ]\subset \mathfrak{k} \), if \(H\in \mathfrak{t} \subset \mathfrak{k}  \) and \(X\in \mathfrak{g} \) we have,
    
    \(\overline{[H,X]} = [H,X_1] - i[H,X_2] = [H,\overline{X} ] \) 

    Now, suppose \(X\) is a root vector with root \(\alpha \in i \mathfrak{t} \) then \(\forall H\in \mathfrak{t}, [H,\overline{X} ] = \overline{[H,X]}=\overline{\langle \alpha , H \rangle X } = -\langle \alpha ,H \rangle \overline{X}  \) 

    By linearity in \(H\in \mathfrak{h} \), \([H,\overline{X} ]=-\langle \alpha ,H \rangle \overline{X}  \)  

    Thus, \(\overline{X}\) is a root vector corresponding to the root \(-\alpha\) and so is \(-\overline{X}=X^{\star}\).
    
    Thus \(X^{\star} \in \mathfrak{g}_{-\alpha} \) 

\end{proof}

\hrule
\hfil

Class 4: 01/18

Today: Roots, Root Spaces

Last time, we did Cartan Subalgebras.

\(\mathfrak{g} \) is a complex semisimple lie algebra

\(\mathfrak{h} \), the cartan subalgebra is the maximal abelian subalgebra of \(\mathfrak{g} \) so that for \(H\in \mathfrak{h} \) we have \(\operatorname{Ad}_h\) is diagonalizeable.

Watch out: if \(\mathfrak{g} =\mathfrak{sl} (2,\mathbb{C} )\) and consider the subalgebra \(\begin{pmatrix}
    0 &  z \\
    0 &  0 \\
\end{pmatrix},z\in\mathbb{C} \) is maximal abelian but not Cartan.

Primary reason, this is not of the form \(\mathfrak{t}+i\mathfrak{t} \) where \(\mathfrak{t} \) is the maximal abelian subalgebra of \(\mathfrak{su} (2,\mathbb{C} )\) which is compact real form of \(\mathfrak{sl}(2,\mathbb{C}) \) 

In general, if \(\mathfrak{g}=\mathfrak{k}_\mathbb{C}    \) where \(\mathfrak{k} \) is the lie algebra of a compact group \(K\). Choose \(\mathfrak{t} \) to be the maximal abelian subalgebra of \(\mathfrak{k} \) then \(\mathfrak{t}_\mathbb{C} \) is a Cartan Subalgebra of \(\mathfrak{g} \)

Fix \(\mathfrak{h} \) [Cartan Subalgebra not unique.]

For each \(\alpha \in \mathfrak{h}\)

\[
    \mathfrak{g}_\alpha = \left\{ X\in \mathfrak{g} : \operatorname{ad}_H(X)=[H,X]=\left\langle \alpha ,H \right\rangle X, \forall H\in \mathfrak{h}   \right\}  
\]

If \(\mathfrak{g}_\alpha \neq \{0\},\alpha \neq 0 \) then \(\mathfrak{g}_\alpha  \) is called a root space and \(\alpha \) is called a root. \(R \subset \mathfrak{h} \) is called the set of all roots.

Note an alternate formation, we can define \(\left\langle \alpha , - \right\rangle \in \mathfrak{h}^{\star} \) to be the root.

If \(\alpha =0\) [not a root] we say \(\mathfrak{g}_0=\mathfrak{h}  \). This is apparent by definition, since \([H,H^{\prime} ]=0\) for \(H,H^{\prime} \in \mathfrak{h} \)  

We have the root space decomposition as a vector space [not as a lie algebra]

\[
    \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\alpha \in R}^{} \mathfrak{g}  _\alpha    
\]

We also have, \([\mathfrak{g}_\alpha ,\mathfrak{g}_\beta  ] \subset \mathfrak{g}_{\alpha +\beta } \)  

\begin{proposition}
    i: If \(\alpha\) is a root then so is \(-\alpha\). Specifically, if \(X\in \mathfrak{g}_\alpha \) then \(X^{\star} \in \mathfrak{g} _{-\alpha } \)
    
    ii: The roots span \(\mathfrak{h} \) [For this we need \(\mathfrak{g} \) to be semisimple]

\end{proposition}

Proof of Part ii: We use contradiction. Suppose the roots do not span \(\mathfrak{h} \) then exists an element \(H_0\in \mathfrak{h} \) that is nonzero and \(\left\langle \alpha , H_0 \right\rangle =0 \forall \alpha \in R\) 

Then, \(\forall X\in \mathfrak{g} _\alpha ,[H_0,X] = \left\langle \alpha , H_0 \right\rangle X = 0\)

\(\implies H_0\in\) center of \(\mathfrak{g} \)

Since \(\mathfrak{g} \) is semisimple the center is just \(\left\{ 0 \right\} \) which is a contradiction. 

\begin{theorem}
    For each root \(\alpha \) we can find \(3\) linearly independent elements \(E_\alpha \in \mathfrak{g} _\alpha, F_\alpha \in \mathfrak{g} _{-\alpha}, H_\alpha \in \mathfrak{h} \) so that \(H_\alpha \) is a scalar multiple of \(\alpha \) and \([H_\alpha , E_\alpha ] = 2E_\alpha \), \(\left[ H_\alpha , F_\alpha  \right] = -2F_\alpha \) and \(\left[ E_\alpha ,F_\alpha  \right] = H_\alpha \) (standard gen relations in \(\mathfrak{sl} (2,\mathbb{C} )\) ) and furthermore, \(F_\alpha \) can be chosen to be \(E_\alpha ^{\star} \). 
\end{theorem}

This basically means the point \(\left\{ \alpha ,-\alpha  \right\} \mapsto \) copy of \(\mathfrak{sl} (2,\mathbb{C} ) \subset \mathfrak{g}\)

Observe: if \(E_\alpha ,F_\alpha ,H_\alpha \) are in the theorem, then \([H_\alpha , E_\alpha ] = 2E_\alpha , [H_\alpha , E_\alpha ]=\left\langle \alpha ,H \right\rangle E_\alpha \) so we can conclude that \(\left\langle \alpha ,H_\alpha  \right\rangle = 2\) and since \(H_\alpha \) is a multiple of \(\alpha \) we have to choose \(H_\alpha =\dfrac{2\alpha}{\left\langle \alpha ,\alpha  \right\rangle }\)  

\begin{definition}
    \(H_\alpha \) is called the coroot associated to the root \(\alpha \) 
\end{definition}

Corrolary: Let \(E_\alpha ,F_\alpha ,H_\alpha \) be as in the theorem, with \(F_\alpha = E_\alpha ^{\star} \) then consider the following elements:\(X_1^\alpha =\frac{i}{2}H_\alpha , X_2^\alpha =\frac{i}{2}(E_\alpha +F_\alpha)\) and \(X_3^\alpha =\frac{1}{2}\left( F_\alpha -E_\alpha  \right) \) then they are linearly independent of \(\mathfrak{k} \) and satisfy the following relations:

\(\left[ X_1^\alpha ,X_2^\alpha  \right] =X_3^\alpha , \left[ X_2^\alpha ,X_3^\alpha  \right] =X_1^\alpha ,\left[ X_3^\alpha ,X_1^\alpha  \right] =X_2^\alpha \).

These are the relations satisfied by the generators of \(\mathfrak{su} (2)\), the pauly matrices:

\[
    X_1=\frac{1}{2}\begin{pmatrix}
        i &  0 \\
        0 &  -i \\
    \end{pmatrix}, X_2 = \frac{1}{2}\begin{pmatrix}
        0 &  i \\
        i &  0 \\
    \end{pmatrix}, X_3=\frac{1}{2}\begin{pmatrix}
        0 &  -1 \\
        1 &  0 \\
    \end{pmatrix}
\]

So, Span of \(X_1^\alpha ,X_2^\alpha ,X_3^\alpha \) is a subalgebra of \(\mathfrak{k} \) isomorphic to \(\mathfrak{su}(2) \) 

\begin{proof}
    Since \(\alpha \in i\mathfrak{t} \subset \mathfrak{h} \) [This is a consequence of the fact \(\operatorname{ad}_H \) is skew-symmetric which means it has imaginary eigenvalues], then \(H_\alpha \) is a real multiple of \(\alpha \) and thus \(X_1^\alpha\) is an imaginary multiple of \(\alpha \). Therefore, \(X_1^\alpha \in \mathfrak{k}\)
    
    Also, \((X_2^\alpha )^{\star} = - X_2^\alpha ,(X_3^\alpha )^{\star} =-X_3^\alpha \) 
    
    The map \(X\mapsto X^{\star} \) has two eigenspaces:

    \(+1\) eigenspaces given by \(i\mathfrak{k} \) 
    \(-1\) eigenspaces given by \(\mathfrak{k} \) 

    Since \(X_1^\alpha ,X_2^\alpha ,X_3^\alpha \) are linearly independent [Since \(E_\alpha ,F_\alpha ,H_\alpha \) are linearly independent and the transformation matrix is invertible] their span is a 3-dimensional subspace of \(\mathfrak{k} \)

\end{proof}

Now we prove the theorem.

\begin{proof}

    Suppose \(X\in \mathfrak{g}_\alpha , Y\in \mathfrak{g} _{-\alpha }, H\in \mathfrak{h}  \). Then, \([X,Y]\in \mathfrak{h} \) and \(\left\langle [X,Y],H \right\rangle = \left\langle \alpha ,H \right\rangle \left\langle Y,X^{\star}  \right\rangle \) 
        
    Proof of this: \([g_\alpha ,g_{-\alpha }] \subset \gamma _0=\mathfrak{h} \implies [X,Y]\in \mathfrak{h}  \) 

    
    Proposition 7.4: \(\left\langle \operatorname{ad}_X(Y),Z  \right\rangle = \left\langle Y,\operatorname{ad}_{X^{\star}  }(Z)  \right\rangle  \) 

    So, \(\left\langle [X,Y],H \right\rangle = \left\langle \operatorname{ad}_X(Y), H  \right\rangle = \left\langle Y,\operatorname{ad}_{X^{\star} }(H)  \right\rangle = - \left\langle Y,[H,X^{\star} ] \right\rangle   \) 

    So, \(X\in \mathfrak{g}_\alpha ,X^{\star} \in \mathfrak{g}_{-\alpha }\implies [H,X^{\star} ]=-\left\langle \alpha ,H \right\rangle X^{\star}  \) 

    Which implies the lemma:

    \[
        \left\langle [X,Y],H \right\rangle = \left\langle \alpha ,H \right\rangle \left\langle Y, X^{\star}  \right\rangle  
    \]


\end{proof}


Proof of the theorem:

Pick any \(X\in \mathfrak{g}_\alpha, X\neq 0 \) then \(X^{\star} \in \mathfrak{g}_\alpha ,X^{\star} \neq 0 \) 

Then, apply the lemma with \(Y=X^{\star} \) 

\[
    \left\langle [X,X^{\star} ],H \right\rangle    = \left\langle \alpha ,H \right\rangle \left\langle X^{\star} ,X^{\star}  \right\rangle  
\]

So, \([X,X^{\star} ]\) is perpendicular to every \(H\in \mathfrak{h} \) that is perpendiular to \(\alpha \)

This implies \([X,X^{\star} ]\) is a scalar multiple of \(\alpha \) 

If \(\left\langle \alpha ,H \right\rangle \neq 0\) then \(\left\langle [X,X^{\star} ],H \right\rangle \neq 0\) since \(\left\langle X^{\star} ,X^{\star}  \right\rangle \neq 0\) 

Thus, \([X,X^{\star} ]\neq 0\) 

Set \(H=[X,X^{\star} ]\) and substitute this to the equation:

\[
    \left\langle [X,X^{\star} ],[X,X^{\star} ] \right\rangle = \left\langle \alpha ,[X,X^{\star} ] \right\rangle \left\langle X^{\star} ,X^{\star}  \right\rangle 
\]

Since the first and last ones are \(>0\) we conclude that \(\left\langle \alpha ,[X,X^{\star} ] \right\rangle \) is real and positive.

Define \(H_\alpha =\frac{2}{\left\langle \alpha ,H \right\rangle }H,E_\alpha =\sqrt{\frac{2}{\left\langle \alpha ,H \right\rangle }}X, F_\alpha =\sqrt{\frac{2}{\left\langle \alpha ,H \right\rangle }} X^{\star} =E_\alpha ^{\star} \)

Then \(H_\alpha\in \mathfrak{h}, E_\alpha \in \mathfrak{g}_\alpha  , F_\alpha \in \mathfrak{g}_{-\alpha }  \) 

and also,

\([H_\alpha ,E_\alpha ]= \langle \alpha ,H_\alpha  \rangle E_\alpha =2E_{\alpha }, [H_\alpha ,F_\alpha] = \langle -\alpha ,H_\alpha  \rangle F_\alpha =-2F_\alpha  \) 

\([E_\alpha ,F_\alpha ]= \frac{2}{\langle \alpha ,H \rangle }[X,X^{\star} ]=\frac{2}{\langle \alpha ,H \rangle }H= H_\alpha \) 

Linear independence: \(E_\alpha ,F_\alpha ,H_\alpha \) are eigenvectors for \(\operatorname{ad}_{H_\alpha} \) with eigenvalues \(2,-2,0\) so they are linearly independent.

Next big result:

Lemma:If \(\alpha \) and \(c \alpha\) are both roots, then \(c=\pm \frac{1}{2}, \pm 1, \pm 2\) are the only possible values.

Note: \(c=\pm \frac{1}{2},\pm 2\) never happens in complex semisimple lie algebras. But may happen in ``abstract'' roots systems.

There are multiple goals. One goal is to classify all simple and semisimple lie algebras, other is to study all the algebras themselves.

Root systems and abstract root systems are used to classify simple lie algebras.

For abstract root systems, just because things happen in them that doesn't happen in complex semisimple lie algebras doesn't mean they are useless. They can happen in real simple lie algebras for example.

\begin{proof}
    Let \(\mathfrak{s}^\alpha\) be the span of \(E_\alpha ,F_\alpha ,H_\alpha \) which is a lie subalgebra of \(\mathfrak{g} \) that is isomorphic to \(\mathfrak{sl}(2,\mathbb{C} ) \)
    
    Then, \(\mathfrak{s}^\alpha \) acts on \(\mathfrak{g} \) via \(\operatorname{ad}|_{\mathfrak{s}^\alpha} \)  

    Then \(H_\alpha =\frac{2\alpha}{\langle \alpha ,\alpha  \rangle }\in \mathfrak{h} \) [the coroot associated to \(\alpha \) ]

    Then \(\langle \alpha ,H_\alpha  \rangle =2\)
    
    If \(\beta =c \alpha \) is a root and \(X\in \mathfrak{g}_\beta ,X\neq 0 \) then,
    
    \([H_\alpha ,X]= \langle \beta ,H_\alpha  \rangle X = \langle c \alpha ,H_\alpha  \rangle X=\overline{c} \langle \alpha ,H_\alpha  \rangle X=2\overline{c} X\) 

    Then \(2\overline{c} \) is an eigenvalue of \(H_\alpha \) for \(\operatorname{ad}|_{\mathfrak{s} ^\alpha } \) 

    All such eigenvalues/weights of \(H_\alpha \) must be integers [From the \(\mathfrak{sl}(2,\mathbb{C} ) \) isomorphism]

    So \(2\overline{c} \) and \(\frac{2}{\overline{c} }\) [by swapping the roles of \(\alpha\) and \(\beta\)] must be both integers.

    So, \(c=\pm\frac{1}{2},\pm 1,\pm 2\)

\end{proof}

Next big result:

\begin{theorem}
    i: For each root \(\alpha\) the only multiples of \(\alpha \) that are roots are \(\alpha \) and \(-\alpha \). The \(\pm \frac{1}{2},\pm 2\) never happen for complex semisimple lie algebras.
    
    ii: If \(\mathfrak{g} \) is complex semisimple, then for each root \(\alpha \) we have \(\dim \mathfrak{g}_\alpha   =1 \) 

\end{theorem}

\hfil
\hrule

Class 05: 01/22

Let \(\mathfrak{g} \) be a complex semisimple lie algebra. Now we don't have to work with reductive lie algebras anymore.

Suppose \(\mathfrak{g} =\mathfrak{k} _\mathbb{C} \) for some compact lie algebra \(\mathfrak{k} _\mathbb{C} \) and \(\mathfrak{t} \) is the maximal abelian ideal of \(\mathfrak{k} \). Then, \(\mathfrak{h} =\mathfrak{k} _\mathbb{C} \) is the Cartan Subalgebra (CSA) of \(\mathfrak{g} \).

Also, for \(\forall \alpha \in \mathfrak{h} \) we define \(\mathfrak{g}_\alpha =\left\{ X\in \mathfrak{g} ; \operatorname{ad}_H( X) = [H,X] = \langle \alpha ,H \rangle X \forall H\in \mathfrak{h}    \right\}  \) 

If \(\alpha \neq 0,\mathfrak{g}_\alpha \neq \{ 0 \}  \) then \(\alpha \) is a root and \(\mathfrak{g}_\alpha \) is a root spaces.

As a vector space, \(\mathfrak{g} \) decomposes into root spaces:

\(\mathfrak{g} = \mathfrak{h}\oplus \bigoplus_{\alpha \in R}^{} \mathfrak{g}_\alpha   \) 

Note that \(\mathfrak{h}=\mathfrak{g}_0  \) 

Also, if \(\alpha \in R,-\alpha \in R\) 

For each \(\alpha ,-\alpha \in R\) we have a copy of \(\mathfrak{sl}_2(\mathbb{C} )\subset \mathfrak{g}  \) 

Which is given by the span of \(H_\alpha ,E_\alpha ,F_\alpha \) 

Where \(E_\alpha \in \mathfrak{g}_\alpha , F_\alpha =E_\alpha a^{\star} \in \mathfrak{g}_\alpha , [E_\alpha ,F_\alpha ]=H_\alpha \in \mathfrak{h}, [H_\alpha ,E_\alpha ]=2E_\alpha , [H_\alpha ,F_\alpha ]=2F_\alpha    \) 

Lemma: If \(\alpha \) and \(c \alpha \) are both roots then \(c=\pm \frac{1}{2},\pm 1,\pm 2\) 

Also, \(\pm \frac{1}{2},\pm 2\) cannot happen so \(\pm 1\) is the only possible case.

Theorem: i: \(\forall \alpha \in R\) the only multiples of \(\alpha \) that are roots is \(\pm \alpha \) 

ii: \(\forall \alpha \in R\), \(\operatorname{\dim }(\mathfrak{g}_\alpha ) = 1 \)

If \(\alpha \neq 0\) and not a root then dimension is \(0\) and if \(\alpha = 0\) then dimension is the dimension of Cartan Subalgebra which can be very big.

\begin{proof}
    There are only finitely many roots. WLOG, if \(c \alpha \) is a root, then \(\vert c \vert \geq 1\) [we take the smallest root]. Then, by lemma, \(c=\pm 1,\pm 2\). Let \(s^\alpha =\operatorname{span}(E_\alpha ,F_\alpha ,H_\alpha ) \subset \mathfrak{g} \) which is a subalgebra of \(\mathfrak{sl} (2,\mathbb{C} )\) and also assume \(F_\alpha =E_\alpha ^{\star} \).

    \(V^\alpha =\operatorname{span}\left\{ H_\alpha ,\mathfrak{g}_\beta : \text{where \(\beta \) is a root proportional to \(\mathfrak{\alpha } \) }   \right\} \) .

    So, \(\beta \) may be \(\pm \alpha ,\pm 2\alpha \) 

    Draw picture:

    \(-2\alpha\)---\(-\alpha \)---\(H_\alpha \)---\(\alpha \)---\(2\alpha \) 

Claim: \(V^\alpha \) is a subalgebra of \(\mathfrak{g} \) 

\begin{proof}
    Let \(\beta \) be a root proportional to \(\alpha \). Consider \(X\in \mathfrak{g} _\beta ,Y\in \mathfrak{g}_{-\beta} \)
    
    Thus, \([X,Y]\in \mathfrak{h} \) and \(\forall H\in \mathfrak{h} \) we have \(\langle [X,Y],H \rangle = \langle \beta , H \rangle \langle Y,X^{\star}  \rangle  \) 

    Since \(H\perp \beta \) we have \(H \perp [X,Y]\) 

    This implies \([X,Y]\) is proportional to \(\beta \) which is proporitonal to \(\alpha\) so it is proportional to \(H_\alpha \) 

    If we have coroots \(\beta ,\beta^{\prime} \) proportional to \(\alpha \) and \(\beta +\beta ^{\prime} \neq 0\) then \([\mathfrak{g}_\beta , \mathfrak{g}_{\beta ^{\prime}}  ] \subset \mathfrak{g}_{\beta +\beta ^{\prime} } \) 

    So \(\beta +\beta ^{\prime} \) would be a non-zero multiple of \(\alpha\). So, \(V^\alpha \) is indeed a \textbf{subalge} subalgebra. 

\end{proof}

Now we can finish the previous proof.

\(s^\alpha \) is a subalgebra of \(V^\alpha\) 

\(V^\alpha \) is invariant under the adjoint action of \(s^\alpha \) 

\(s^\alpha \) is \(\operatorname{ad}(s^\alpha ) \) invariant subspace.

Take \(s^{\alpha \perp} \) in \(V^\alpha\) then this is also \(s^\alpha \) invariant subspace.

\begin{proof}
    Note that \(s^\alpha\) is invariant under the map \(X \mapsto X^{\star} \) 

    \(E_\alpha ^{\star} =F_\alpha \) by assumption, \(F_\alpha ^{\star} =E_\alpha \) and \(H_\alpha \) is a real multiple of \(\alpha \in i \mathfrak{t} \) so \(H_\alpha ^{\star} =H_\alpha \) 

    Proposition 7.4 tells us:

    \(\langle \operatorname{ad}_X Y,Z  \rangle = \langle Y,\operatorname{ad}_{X^{\star} } Z \rangle \) 

    Apply this with \(X\in s^\alpha , Y\in S^{\alpha \perp}, Z\in s^\alpha \) 

    This gives us \(\operatorname{ad}_X Y\in s^{\alpha \perp} \) 

    So, \(s^{\alpha \perp}\) is \(s^\alpha \) invariant.

\end{proof}

Note that \(s^\alpha \) is not the same as \(V^\alpha \) if and only if \(\mathfrak{g}_{2\alpha } \) or \(\mathfrak{g}_{-2\alpha} \) is non-zero or \(\operatorname{\dim }(\mathfrak{g}_\alpha ) > 1 \)  or \(\operatorname{\dim }(\mathfrak{g}_{-\alpha} )  > 1\) 

Since \(\langle \alpha ,H_\alpha  \rangle = 2, H_\alpha \) acting on \(V^\alpha \) has eigenvalues (weights):

\(0\) from \(H_\alpha \) 

\(\pm 2\) from \(\mathfrak{g}_{\pm \alpha} \) 

\(\pm 4\) possibly from \(\mathfrak{g}_{\pm 2\alpha } \) 

Weights of \(H_\alpha \) acting on \(V^\alpha \) are a subset of \(\{ 0,\pm 2,\pm 4 \} \) which are all even numbers.

Look at weights of \(s^{\alpha \perp}\): they are also even, subset of \(\{ 0,\pm 2,\pm 4 \} \) 

From the theory of representation of \(\mathfrak{sl} (2,\mathbb{C} )\) \(s^{\alpha \perp}\) must have a \(0\) weight. If \(s^{\alpha \perp} \neq  \{ 0 \} \) 

This means, \(\mathbb{C} H_\alpha \in s^{\alpha \perp}\) since this is the only thing that produces \(0\) weight.

But \(H_\alpha \in s^\alpha\) 

So, \(H_\alpha \in s^\alpha \cap s^{\alpha \perp}\) 

This is a contradiction.

So, \(s^{\alpha \perp}\) being non-zero is not possible.

So, \(s^{\alpha \perp}\) is zero.

So, \(s^\alpha = V^\alpha \) 

This means the \(2\alpha \) and \(-2\alpha \) parts don't exist, and \(\alpha ,-\alpha \) parts are one dimensional.

\end{proof}

\section*{Weyl Group}

Recall that \(\mathfrak{h} = \mathfrak{t}_\mathbb{C} , \mathfrak{t}  \) is the maximal abelian subalgebra in \(\mathfrak{k} \)  

\(R\) is the set of roots \(\mathfrak{g} \) related to \(\mathfrak{h} \) so \(R \subset \mathfrak{h} \) 

\begin{definition}
    For each root \(\alpha \) denote a linear map \(s_\alpha : \mathfrak{h} \to \mathfrak{h}   \) by the formula:

    \[
        s_\alpha (H) = H - 2 \frac{\langle \alpha ,H \rangle }{\langle \alpha ,\alpha  \rangle } \alpha 
    \] 

    This is a reflection.

\end{definition}

To see this, if \(\alpha \perp H\) then \(s_\alpha (H) = H\) 

If \(\alpha = H, s_\alpha (H)=H-2H=-H\) 

So, this is essentially a reflection through the hyperplane perpendicualr to \(\alpha \) 

\begin{definition}
    The \underline{Weyl Group} of \(R\) denoted \(W\) is the subgroup of \(GL(\mathfrak{h} )\) generated by all the reflections \(s_\alpha\) with \(\alpha \in R\)
    
\end{definition}

Note that \(\mathfrak{h}\) is a complex vector space. Since \(\langle , \rangle \) is real valued on \(\mathfrak{k} \) it s also real valued on \(\mathfrak{t} \) and \(i\mathfrak{t} \). It follows that, \(s_\alpha \) maps \(i\mathfrak{t} \to i\mathfrak{t} \) and hence \(s_\alpha \in O(i \mathfrak{t} )\) and thus \(W \subset O(i\mathfrak{t} )\) 

Fact: if \(K\) is a compact Lie group with lie algebra \(\mathfrak{k} \) such that the complexification of \(\mathfrak{k} \) is \(\mathfrak{g} \) then the quotient group \(\frac{\text{normalizer of \(\mathfrak{h}\) in \(K\)}}{\text{centralizer of \(\mathfrak{h}\) in \(K\)}}=W\) 

In some textbooks this is also used as the definition of the Weyl Group.

\begin{theorem} [Weyl Group maps roots to roots]
    The action of \(W\) on \(\mathfrak{h}\) (or \(i\mathfrak{t} \)) preserves \(R\). That is, if \(\alpha\) is a root then \(w\cdot \alpha \) is also a root for all \(w \in W\)  
\end{theorem}

\begin{proof}

    For each \(\alpha \in R\) consider an invertible linear operation \(S_\alpha : \mathfrak{g} \to \mathfrak{g} \) given by \(S_\alpha=e^{\ad_{E_\alpha}}e^{-\ad_{F_\alpha }}e^{\ad_{E_\alpha }}\)  

    This operators maps one root space into another.

    This will map root space \(\mathfrak{g}_\beta \) into root space \(\mathfrak{g}_{s_\alpha \cdot \beta} \)  

\end{proof}

\hfil
\hrule

Class 06: 01/25

For matrix groups / lie algebra:

\(\mathfrak{g} \subset \mathfrak{gl} (n,\mathbb{C})\) 

For \(X,Y\in \mathfrak{g} \) typically \(XY\notin \mathfrak{g} \) 

But \([X,Y]=XY-YX\in \mathfrak{g} \) 

Suppose \(G\) is an abstract lie group. This means it is a \(C^{\infty}\) manifold with group operations \(G\times G\to G\) and \(G\to G\) given by multiplication and inverse, and they are smooth.

Then, associated lie algebra \(\mathfrak{g} = T_e G \simeq \) Left invariant vector fields on \(G\) 

So, \(a\in G\) we have the smooth operation left multiplication by \(a\) given by \(L_a:G\to G\) which gives a new vector field \(\mathfrak{g}\to \mathfrak{g}\)

If \(X,Y\) are two vector fields, \(XY-YX\) is a vector field.

Every lie algebra \(\mathfrak{g} \subset \mathcal{U}(\mathfrak{g} )\) the universal enveloping lie algebra.

For all \(X,Y\in \mathfrak{g} \) we have \([X,Y]=XY-YX\) where this product is evaluated in the universal enveloping lie algebra.

Lets go back to Weyl Groups.

\(\mathfrak{g} \) is a complex semisimple lie algebra

\(\mathfrak{g=\mathfrak{k} _\mathbb{C}}\) where \(\mathfrak{k} \) is the lie algebra of a compact lie group \(K\) 

\(\mathfrak{t} \) is a maximal commutative subalgebra of \(\mathfrak{k} \) 

\(\mathfrak{h} = \mathfrak{t} _\mathbb{C} \) is the Cartan Subalgebra (CSA) of \(\mathfrak{g} \) 

\(R=\) roots \(\subset i\mathfrak{t} \subset \mathfrak{h} \) 

So that if \(X\) is an eigenvector of one \(\ad_H\) and hence all \(\ad_H\) then \([H,X]= \langle \alpha ,H \rangle X\) for all \(H\in \mathfrak{h} \). \(\alpha \in H\) are the roots of \(H\)

\begin{definition}
    For each \(\alpha \in R\), define linear maps on \(\mathfrak{h} \) [reflection] denoted \(s_\alpha\) 
    
    \[
        s_\alpha \cdot H = H - 2 \frac{\langle \alpha , H \rangle }{\langle \alpha ,\alpha \rangle }
    \]

    when \(H\in \mathfrak{h} \) 

    The \underline{Weyl Group} of \(R\) denoted by \(W\) is the subgroup of \(GL(\mathfrak{g} )\) generated by \(s_\alpha,\alpha \in R\) 

\end{definition}

Since each \(s_\alpha : i\mathfrak{t} \to i \mathfrak{t} \)

we have \(w: i\mathfrak{t} \to i \mathfrak{t}, \forall w\in W \) 

Therefore, \(W \subset O(i \mathfrak{t} )\) 



\begin{theorem}

    The action of \(W\) on \(\mathfrak{h} \) (or \(i\mathfrak{t} \) ) preserves \(R\). In other words, \(\forall \alpha ,\beta \in R\) we have \(s_\alpha \cdot \beta \in R\) 

\end{theorem}

\begin{proof}

    Consider an operator \(S_\alpha: \mathfrak{g} \to \mathfrak{g} \) given by:

    \[
        S_\alpha = e^{\ad_{E_\alpha}}e^{-\ad_{F_\alpha}}e^{\ad_{E_\alpha}}
    \]

    We want to prove that \(S_\alpha\) somehow maps the root space of \(\beta\) to the root space of \(s_\alpha \cdot \beta\) 

    Claim: \(S_\alpha \ad_H S_\alpha^{-1}=\ad_{s_\alpha \cdot H}\) for all \(H\in \mathfrak{h} \) 

    Proof: Consider first \(H\in \mathfrak{h} \) so that \(\langle \alpha ,H \rangle = 0\) 

    Then, \([H,E_\alpha] = \langle \alpha , H \rangle E_\alpha  = 0  \) 

    \([H,F_\alpha ] = -\langle \alpha ,H \rangle F_\alpha =0\) 

    So \(H \) commutes with \(E_\alpha ,F_\alpha\) 

    So \(\ad_H\) commutes with \(\ad_{E_\alpha }\) and \(\ad_{F_\alpha}\) 

    So \(\ad_H\) commutes with \(S_\alpha \) 

    So, \(S_\alpha \ad_H S_\alpha ^{-1}=\ad_H\) 

    Again, if \(H\in \mathfrak{h} \) and \(H\perp \alpha\) meaning \(\langle \alpha ,H \rangle = 0\) we have \(s_\alpha \cdot H = H\) so we have prooved this case.

    If \(H=\alpha\) [or proportional]

    By theorem 4.34(iii) applied to \(\mathfrak{s}^\alpha =span(E_\alpha ,F_\alpha ,H_\alpha ) \simeq \mathfrak{sl}(2,\mathbb{C})  \) and adjoint representations on \(\mathfrak{g} \) we have

    \(S_\alpha \ad_{H_\alpha }S_\alpha ^{-1} = -\ad_{H_\alpha}\) 

    This proves the claim for proportional case, and subsequently for all of \(\mathfrak{h}\) 

    Suppose \(\beta \in R\) and let \(X\in \mathfrak{g} ,X\neq 0\) 

    Consider \(S_\alpha ^{-1} (X)\in \mathfrak{g} \) 

    Then, \(\ad_H(S_\alpha ^{-1} (X)) = S_\alpha ^{-1}  [S_\alpha \ad_H S_\alpha ^{-1}] (x)\) 

    \(=S_\alpha ^{-1} \ad_{s_\alpha \cdot H}(X)\) 

    \(= \langle \beta , s_\alpha \cdot H \rangle S_\alpha^{-1}(X)\) 

    \(= \langle s_\alpha ^{-1}\cdot \beta , H \rangle S_\alpha^{-1}(X) \) 

    \(= \langle s_\alpha \cdot \beta ,H \rangle S_\alpha ^{-1}(X)\) 

    So, \(S_\alpha ^{-1}(X)\) is a root vector with root \(s_\alpha \cdot \beta\) 

    Note that, \(s_\alpha ^2=Id\) and so \(s_\alpha \cdot s_\alpha \cdot \beta =\beta \) which implies \(s_\alpha \) is bijetive.

    So, \(R\) gets mapped to \(R\) 

\end{proof}

Corollary: the Weyl Group is finite.

This is how the textbook states it. But not only is it finite, it is a subgroup of the permutation group.

So, better way of saying it is \(W\) is a subgroup of the permutation group on \(R\) 

\begin{proof}
    roots of \(R\) span \(\mathfrak{h} \) since \(\mathfrak{g} \) is semisimple.

    So, as a linear transformation \(\mathfrak{h} \to \mathfrak{h} \) each \(w\) is uniquely determined by its action on \(R\).

    Thus, the weyl group is a subgroup of permutation group on \(R\) which is finite.

\end{proof}

\section*{Root Systems}

For each \(\alpha \in R\) the element \(H_\alpha \in \mathfrak{h} \) , we have \(H_\alpha = \frac{2\alpha}{\langle \alpha ,\alpha  \rangle }\), the coroot associated to the root \(\alpha\) 

\begin{proposition}
    \(\forall \alpha ,\beta \in R\) we have:

    \[
        \langle \beta , H_\alpha \rangle = 2 \frac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle } \in \mathbb{Z}
    \] 

\end{proposition}

\begin{proof}
    
    Let \(\mathfrak{s}^\alpha = span(E_\alpha ,F_\alpha ,H_\alpha) \) the lie subalgebra of \(\mathfrak{g} \) that is isomorphic to \(\mathfrak{sl}(2,\mathbb{C}) \)  

    Where \(H_\alpha  \leftrightarrow \begin{bmatrix}
        1 &  0 \\
        0 &  -1 \\
    \end{bmatrix}\) 

    Let \(X\in \mathfrak{g}_\beta \) (Let \(X\) be a root vector associated to \(\beta\) ) 

    Then \([H_\alpha ,X] = \langle \beta ,H_\alpha \rangle X \) 

    Therefore, \(\langle \beta ,H_\alpha \rangle \) is an eigenvalue / weight for the adjoint representation of \(\mathfrak{sl}(2,\mathbb{C}) \) on \(\mathfrak{g}\).

    Weights/Eigenvalues of \(H\) are integers.

    Therefore, \(\langle \beta , H_\alpha \rangle \in \mathbb{Z}\) 


\end{proof}

\underline{Geometric Interpretation:}

The orthogonal projecton of \(\beta \) onto \(\alpha \) is:

\[
    \frac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle }\alpha = \frac{1}{2} \langle \beta ,H_\alpha  \rangle \alpha
\]

Since \(\langle \beta ,H_\alpha \rangle \in \mathbb{Z}\) we see that the projection of \(\beta \) onto \(\alpha \) must be of the form \(\frac{n}{2}\alpha\).

Also:

\[
    s_\alpha \cdot \beta = \beta - 2 \frac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle } \alpha
\]

\[
    \implies \beta - s_\alpha \cdot \beta = 2\frac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle }\alpha = n \alpha
\] 

Think of roots \(R\) as subset of real vector space \(E = i \mathfrak{t} \)

\begin{theorem}
    The set of roots \(R\) is a finite set of non-zero elements of a real vector space \(E\) such that:

    \begin{itemize}
        \item \(R\) spans \(E\) 
        \item If \(\alpha \in R\) then \(-\alpha \in R\) and the only multiples of \(\alpha \) that are in \(R\) are \(\pm \alpha\) 
        \item If \(\alpha ,\beta \in R\) then so is \(s_\alpha \cdot \beta \) 
        \item \(\forall \alpha ,\beta \in R, 2\dfrac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle }\in \mathbb{Z}\) 
    \end{itemize}

\end{theorem}

Any collection of vectors satisfying these conditions is called a \underline{root system}.

We can also have abstract root systems.

Their main purpose is classifying simple and semisimple lie algebras.

\section*{Simple Lie Algebras}

The main point of this section: A root system ``knows'' when your lie algebra can be decomposed into ideals.

In other words, the root system contains all the information we need to know about the lie algebra.

\underline{Recall:} A lie algebra (real or complex) is \underline{simple} if it doesn't have any proper ideals and is not commutative.

So, the ideals of a simple lie algebra \(\mathfrak{g}\) are \(\{ 0 \} \) and \(\mathfrak{g} \) and \(\mathfrak{g} \) is not commutative, which means \(\dim \mathfrak{g} \geq 2 \) 

\begin{proposition}
    Suppose \(\mathfrak{g} \) is a real lie algebra, and it's complexification \(\mathfrak{g}_\mathbb{C} \) is simple. Then \(\mathfrak{g} \) is simple as well. 
\end{proposition}

\begin{proof}
    Trivial.
    
    \(\mathfrak{g} \) is not commutative since \(\mathfrak{g}_\mathbb{C} \) cannot be commutative.

    If \(\mathfrak{g} \) is not simple it has a proper ideal \(\mathfrak{h} \) and then \(\mathfrak{h}_\mathbb{C} \) is a proper ideal of \(\mathfrak{g}_\mathbb{C} \) which is a contradiction.    

\end{proof}

Now think of the converse. If \(\mathfrak{g} \) is simple is \(\mathfrak{g}_\mathbb{C} \) simple? The answer is not always!!!

Consider \(\mathfrak{so}(3,1)\) which is a simple lie algebra [lie algebra of the orthogonal group of transformations that preserves the minkowski metric.] It's complexification is not a simple lie algebra! The complexification is \(\mathfrak{so}(4,\mathbb{C})\simeq \mathfrak{sl}(2,\mathbb{C})\oplus \mathfrak{sl}(2,\mathbb{C} )   \)  direct sum of ideals.

Note that, \(\mathfrak{so}(3,1)\) is the underlying real lie algebra of \(\mathfrak{sl}(2,\mathbb{C}) \). 

So we started with something that is real simple, then complexify it. We get something that is complex simple. We consider that as simple, complexify again and it doesn't remain simple.

So, \(\mathfrak{g}_\mathbb{C}\) simple \(\implies \mathfrak{g} \) is simple

But \(\mathfrak{g}\) simple \(\not\implies \mathfrak{g}_\mathbb{C}  \)  simple.

The bulk of this section is dancing around this statement, given \(\mathfrak{g}\) is simple, when is \(\mathfrak{g}_\mathbb{C} \) simple?


\begin{definition}
    A real lie algebra \(\mathfrak{g} \) admits a \underline{complex structure} if there is a complex lie algebra \(\mathfrak{g}^{\prime} \) and an isomorphism of real lie algebras \(\mathfrak{g} \simeq \mathfrak{g}^{\prime}  \)  
\end{definition}

\textbf{Lemma:} Suppose \(K\) is a compact (matrix) lie group whose lie algebra \(\mathfrak{k} \) is not commutative. Then \(\mathfrak{k}\)  does \underline{not} admit a complex structure.

\begin{proof}
    We use contradiction. Suppose \(\mathfrak{k} \) admits a complex structure. Then \(\mathfrak{k} \simeq \mathfrak{k^{\prime}} \) for some complex lie algebra \(\mathfrak{k}^{\prime}  \). Let \(J\) denote the multiplication by \(i\) on \(\mathfrak{k}\).
    
    Fix an \(\Ad K\)-invariant inner product on \(\mathfrak{k} \) then \(\forall H\in \mathfrak{k} \) we have \(\ad_H\) is skew-adjoint and hence diagonalizeable in \(\mathfrak{k}_\mathbb{C} \)  with purely imaginary eigenvalues.

    If \(H\in \mathfrak{k} \setminus \mathfrak{z}  \) where \(\mathfrak{z} \) is the center of \(\mathfrak{k} \) then \(\ad_H \neq 0\) and it has at least one non-zero eigenvalue in \(\mathfrak{k}_\mathbb{C} \) and thus it is not nilpotent.
    
    Consider \(\ad_H\) acting on a complex vector space \(\mathfrak{k} \simeq \mathfrak{k}^{\prime}  \) 

    \(\ad_H\) is not nilpotent and thus there is at least one non-zero eigenspace. Let that be \(\lambda = a + i b \in \mathbb{C}\) 

    Let \(x\) be a corresponding eigenvector.

    Then \(\ad_H(X)=[H,X]=\lambda x = aX+bJX\) 

    Consider \(\overline{\lambda }H = aH-bJH\in \mathfrak{k}  \) 

    Then \(\ad_{H^{\prime} }X=[H^{\prime} ,X]=\vert \lambda \vert^2 X=(a^2 + b^2)X \)
    
    So, \(H^{\prime} \in \mathfrak{k} \) has non-zero real eigenvalues which is a contradiction. 

\end{proof}

\hfil
\hrule

Class 07: 01/30

\begin{theorem}
    The set \(R\) of roots is a finite set of non-zero elements of a real vector space \(E\), and \(R\) satisfies:

    \begin{enumerate}
        \item \(R\) spans \(E\) 
        \item If \(\alpha \in R\) then \(-\alpha \in R\) and the only multiples of \(\alpha \) that are in \(R\) are \(\pm \alpha \) 
        \item If \(\alpha ,\beta \in R\) then so is \(s_\alpha \cdot \beta \) and \(\forall \alpha ,\beta \in R\) the quotient \(2\frac{\langle \alpha ,\beta  \rangle }{\alpha ,\alpha }\in \mathbb{Z}\) 
    \end{enumerate}

\end{theorem}

Lemma: Suppose \(K\) is a compact matrix lie group whose lie algebra \(\mathfrak{k} \) is \underline{not} commutative. Then \(\mathfrak{k} \) does not admit a complex structure.

\(\mathfrak{g}\rightsquigarrow \mathfrak{g}_\mathbb{C}  \)

\(\mathfrak{g}_\mathbb{C} \) simple \(\implies \mathfrak{g} \) simple

\(\mathfrak{g} \) simple \(\not\implies \) \(\mathfrak{g} _\mathbb{C} \) simple in general.

Canonical example: Consider real lie algebra \(\mathfrak{so}(3,1)\), its complexification is \(\mathfrak{sl}(2,\mathbb{C})\oplus \mathfrak{sl}(2,\mathbb{C})  \) so not simple. This is because \(\mathfrak{so}(3,1)\cong \mathfrak{sl}(2,\mathbb{C})  \) and so \(\mathfrak{so}(3,1) \) admits a complex structure. 

\begin{theorem}
    Suppose \(K\) is a compact matrix lie group whose lie algebra \(\mathfrak{k} \) is simple as a real lie algebra then the complexification \(\mathfrak{g}=\mathfrak{k}_\mathbb{C}  \) is simple as a complex lie algebra. 
\end{theorem}

\begin{proof}
    We use the lemma. The proof is by contradiction.

    Suppose \(\mathfrak{g}=\mathfrak{k}_\mathbb{C}  \) is \underline{not} simple.
    
    So we can decompose \(\mathfrak{g}=\mathfrak{z} \oplus \) simple ideals = \(\bigoplus_{j}^{} \mathfrak{g} _j\) 
    
    Since \(\mathfrak{k} \) is simple there is no center. There are thus at least two simple ideals.

    Note that if \(\mathfrak{g} _j\) is an ideal then \(\overline{\mathfrak{g} _j} \) is an ideal as well.

    Uniqueness of decomposition implies \(\overline{\mathfrak{g} _j} = \mathfrak{g} _k\) 

    Case 1: we have some \(\overline{\mathfrak{g} _j} =\mathfrak{g} _j\) and in that case \(\cap \mathfrak{g} _j\cap \mathfrak{k} \) is a proper ideal of \(\mathfrak{k} \) which is impossible since \(\mathfrak{k} \) is simple and thus we have a contradiction.

    Case 2: we have \(\overline{\mathfrak{g}_j }=\mathfrak{g}_k\). In that case \(\mathfrak{g}_j \oplus \overline{\mathfrak{g}_j }  \) is a non-zero ideal of \(\mathfrak{k} \) which means it is all of \(\mathfrak{k} \) 
    
    So, \(\mathfrak{g} \) has exactly \(2\) ideals \(\mathfrak{g_1},\mathfrak{g}_2 \) with \(\overline{\mathfrak{g}_1 }=\mathfrak{g}_2   \)  

    Define a real linear map \(\phi:\mathfrak{g}_1 \to \mathfrak{k} \) given by \(\phi(x)=x+\overline{x} \) 
    
    If \(X\in \mathfrak{g}_1,\overline{X} \in \mathfrak{g}_2  \)  and \([X,\overline{X} ]=0\) 

    So \([\phi(X),\phi(Y)]=[X+\overline{X},Y+\overline{Y}  ]=[X,Y]+[\overline{X},\overline{Y}]=\phi([X,Y])\) 

    So \(\phi\) is a homomorphism.

    Also, \(\phi\) is injective since \(\mathfrak{g}_1\cap \mathfrak{g}_2=\{ 0 \}   \) 

    Also, \(\phi\) is surjective since \(\mathfrak{k} \) has real dimension \(n\) and \(\mathfrak{g} \) has complex dimension \(n\) and \(\mathfrak{g}_1,\mathfrak{g}_2 \) both have real dimension \(n\) 
    
    So \(\phi\) is an isomorphism of real lie algebras \(\mathfrak{g}_1 \to \mathfrak{k} \) 

    So, \(\mathfrak{k} \) has a compelx structure [that it inherits from \(\mathfrak{g} _1\)]. But this is a contradiction since \(\mathfrak{k} \) cannot have a complex structure by the Lemma.

\end{proof}

\underline{Informal Statement:}

Decompositions of \(\mathfrak{g} \) into direct sum \(\mathfrak{g}_1 \oplus \mathfrak{g}_2  \)  corresponds to (\(\leftrightsquigarrow\)) decomposition of root systems \(R=R_1\sqcup R_2\) 

\begin{theorem}
    Let \(\mathfrak{g} = \mathfrak{k} _\mathbb{C} \) be a complex semismiple lie algebra and let \(\mathfrak{t} \) be a maximal commutative subalgebra of \(\mathfrak{k} \) and let \(\mathfrak{h}=\mathfrak{t}_\mathbb{C} \) be the associated Cartan Subalgebra.

    Let \(R \subset \mathfrak{h} \) be the root system for \(\mathfrak{g} \) relative to \(\mathfrak{h} \). If \(\mathfrak{g} \) is \underline{not} simple, then \(\mathfrak{h} \) decomposes as a orthogonal direct sum of subspaces \(\mathfrak{h}_1,\mathfrak{h}_2  \) in such a way that every element of \(R\) is either in \(\mathfrak{h}_1 \) or \(\mathfrak{h}_2 \).

    Conversely, if \(\mathfrak{h} \) decomposes this way, then \(\mathfrak{g} \) is not simple.

\end{theorem}

\underline{Alternative Formulation:} Under the same conditions, \(\mathfrak{g} \) is simple if and only if there \underline{does not} exist an orthogonal decomposition of \(\mathfrak{h} \) as \(\mathfrak{h}_1 \oplus \mathfrak{h}_2  \) with \(\dim \mathfrak{h}_1,\dim \mathfrak{h}_2 > 0\) in such a way that every root is either in \(\mathfrak{h}_1\) or \(\mathfrak{h} _2\).

Root systems are classified by \underline{Dynkin Diagrams}.

\(\mathfrak{g} \) is simple \(\iff \) Dynkin Diagram of its root system is connected.

\(\mathfrak{g} \) is not simple \(\iff \) Dynkin Diagram of its root system is not connected.

\underline{Idea}: For \(\implies\) suppose \(\mathfrak{g}=\mathfrak{k}_\mathbb{C}  \) is not simple. Then, \(\mathfrak{k} \) is not simple. Then, \(\mathfrak{k}=\mathfrak{k}_1 \oplus \mathfrak{k}_2   \). Let \(\mathfrak{g}_1 = \mathfrak{k}_{1\mathbb{C}}, \mathfrak{g}_2=\mathfrak{k}_{2\mathbb{C}}    \) then \(\mathfrak{g}=\mathfrak{g}_1 \oplus \mathfrak{g} _2   \) 

Let \(\mathfrak{h} _1\) be a Cartan Subalgebra of \(\mathfrak{g} _1\) and \(\mathfrak{h} _2\) be a Cartan Subalgebra of \(\mathfrak{g} _2\) 

The bulk of the argument on the textbook shows that \(\mathfrak{h} = \mathfrak{h} _1 \oplus \mathfrak{h} _2\)  is a Cartan Subalgebra of \(\mathfrak{g} \). The reason it takes so much effort is because the definition is different from the usual formulation. But the statement is intuitive, this is exactly what we'd expect.

It follows that each root space \(\mathfrak{g}_\alpha \) is a subspace of \(\mathfrak{g}_1 \) or \(\mathfrak{g}_2 \).

It follows that \(R = R_1 \sqcup R_2\) where \(R_1\) is a root system of \(\mathfrak{g}_1 \) relative to \(\mathfrak{h}_1 \) and \(R_2\) is a root system of \(\mathfrak{g}_2 \) relative to \(\mathfrak{h}_2 \)   

That takes care of one direction.

Conversely, if \(\mathfrak{h} = \mathfrak{h}_1 \oplus \mathfrak{h}_2 \), \(R=R_1 \sqcup R_2\) where \(R_1 \subset \mathfrak{h}_1, R_2 \subset \mathfrak{h}_2  \)  the define \(\mathfrak{g}_i = \mathfrak{h}_i \bigoplus_{\alpha \in R_i}^{} \mathfrak{g} _\alpha  \) 

Then \(\mathfrak{g}=\mathfrak{g}_1 \oplus \mathfrak{g}_2   \) as vector space. We can further prove that \(\mathfrak{g}_1,\mathfrak{g}_2  \) are ideals. So \(\mathfrak{g}=\mathfrak{g}_1 \oplus \mathfrak{g}_2   \) as direct sum of lie algebras so \(\mathfrak{g} \) is not simple.

\section*{Root Systems of Classical Lie Algebras}

The book discusses root systems of \(\mathfrak{sl}(n,\mathbb{C}), \mathfrak{so}(n,\mathbb{C}), \mathfrak{sp}(n,\mathbb{C} )   \) 

This gives us \(4\) infinite families of root systems [\(\mathfrak{so} \) is different based on whether \(n\) is even or odd]

\(\mathfrak{sl}(n+1,\mathbb{C}) \leftrightsquigarrow A_n\) root system 

\(\mathfrak{so}(2n+1) \leftrightsquigarrow B_n\) root system

\(\mathfrak{sl}(2n) \leftrightsquigarrow D_n\) root system

\(\mathfrak{sp}(n,\mathbb{C}) \leftrightsquigarrow C_n\) root system

Exceptional root systems/lie algebras: \(E_6,E_7,E_8,F_4,G_2\) 

Lets look up \(\mathfrak{sl} (n+1,\mathbb{C} )\) in detail. This is \(\mathfrak{su} (n+1)_\mathbb{C}\), this is our \(\mathfrak{k} \)

Then \(\mathfrak{t} \) is traceless diagonal matrices with pure imaginary entries.

Then \(\mathfrak{h} = \mathfrak{t} _\mathbb{C}\) is traceless complex diagonal matrices, this is the ``standard'' Cartan Subalgebra of \(\mathfrak{sl}(n+1,\mathbb{C}) \) 

Let \(H = \begin{pmatrix}
    \lambda_1 & 0 &  0 \\
    0 & \ddots &  0 \\
    0 & 0 &  \lambda _n \\
\end{pmatrix}\) 

Let \(E_{jk}\) be matrix with \(jk\) entry \(1\) and \(0\) elsewhere.

Then \([H,E_{jk}]=(\lambda_j -\lambda _k)E_{jk} \forall H\in \mathfrak{h}  \) 

So \(E_{jk}\) is a root vector, spans root space if \(j \neq k\) 

These are all the root spaces since \(\mathfrak{sl}(n+1,\mathbb{C}) \) decoposes into direct sum of \(\mathfrak{h}\oplus \bigoplus_{j\neq k}^{} \mathbb{C} E_{jk} \)  

So, \(\lambda _i - \lambda _k\) identifies the roots of \(\mathfrak{sl}(n+1,\mathbb{C}) \) as elements of \(\mathfrak{h}^{\star} \)

To identify them as elements of \(\mathfrak{h} \) we need a \(SU(n+1)\)-invariant inner product on \(\mathfrak{sl} (n+1,\mathbb{C})\) 

We choose the product \(\langle X,Y \rangle =\Tr (X^{\star} Y)\) 

Where \(X^{\star} \) is the conjugate transpose.

Note that Schur's Lemma says that any other inner product is a multiple of this. For simple lie algebra we have this.

Let \(e_k\) be diagonal matrix with \(1\) at \(k,k\) place and \(0\) elsewhere.

Then the root corresponding to \(\mathbb{C} E_{jk}\) root space is \(\alpha _{jk}=e_j - e_k, 1 \leq j,k \leq n+1, j\neq k\) 

So we have a root. Each root \(\alpha_{jk}\) has length \(\sqrt{2} \) 

Compute \(\langle \alpha_{jk} , \alpha_{j^{\prime} k^{\prime} } \rangle \). We're applying the trace inner product to two diagonal matrix. Depending on the overlap between \(\{ j,k \} , \{ j^{\prime} ,k^{\prime}  \} \) it can be \(0,\pm 1,\pm 2\).

Next Up: Weyl Group

\hfil
\hrule

Class 08: 02/01

Recall:

``Standard Cartan Subalgebra'' of \(\mathfrak{sl}(n+c,\mathbb{C}) \) consists of \(\mathfrak{h} = \) complex diagonal matrices with trace \(0\) 

Root vectors are \(E_{jk}\), matrices in \(1\) in \(jk\), \(0\) elsewhere.

If \(H=\begin{pmatrix}
    \lambda _1 &  &   \\
     & \ddots &   \\
     &  &  \lambda_{n+1} \\
\end{pmatrix}\) 

Then \([H,E_{jk}]=(\lambda_j - \lambda_k)E_{jk}\) 

So \(E_{j,k}\) are root vectors.

\(\mathfrak{sl}(n+1,\mathbb{C}) = \mathfrak{h} \oplus  \bigoplus_{j \neq k}^{} \mathbb{C} E_{jk} \) as vector space. 

\underline{The root system of \(\mathfrak{sl} (n+1,\mathbb{C} )= \mathfrak{su}(n+1,\mathbb{C}),A_n\) }

We need \(SU(n+1)\)-invariant inner product on \(\mathfrak{sl} (n+1,\mathbb{C})\), we choose \(\langle X,Y \rangle = \Tr (X^{\star} Y)\) where \(X^{\star} \) is the conjugate transpose of \(X\) 

Other invariant inner products are proportional to this.

Let \(e_k\) be the diagonal matrix with \(1\) in \(kk\) and all others \(0\) 

Then \(e_j - e_k \in \mathfrak{h} \) 

The root corresponding to the root space \(\mathbb{C} E_{jk}\) is \(\alpha _{jk} = e_j - e_k\) where \(j \neq k\) 

Since \([H,X]=(\lambda_j - \lambda_j)X\) and \(\langle H, \alpha_{jk} \rangle = \lambda _j - \lambda _k \)  

Each root \(\alpha _{jk}\) has length \(\sqrt{2} \) 

Thus \(\langle \alpha _{jk} , \alpha_{j^{\prime} k^{\prime} } \rangle = 0, \pm 1, \pm 2\) depending on whether they have \(0,1\) or \(2\) elements in common.

If we look at the cosine of the angle between the roots, then we have the formula:

\(\cos \theta = \frac{\langle \alpha _{jk},\alpha _{j^{\prime} k^{\prime} } \rangle }{\vert \vert \alpha _{jk} \vert  \vert \vert \vert \alpha _{j^{\prime} k^{\prime} } \vert  \vert } = 0, \pm \frac{1}{2}, \pm 1\)

\(\cos \theta =\pm 1\) happens when \(\alpha_{jk}=\pm \alpha_{j^{\prime} k^{\prime} }\) 

So the possible angles between rots are \(\frac{\pi}{2},\frac{\pi}{3},\frac{2\pi}{3}\) aka 90, 60, 120 degrees.

The reflection \(s_{ \alpha_{jk} } \cdot H \mapsto H - \langle e_j - e_k, H \rangle (e_j - e_k )  \) 

So it basically switches \(j\) and \(k\) th entries of \(H\) 

Conclusion: the Weyl Group [generated by reflections] is actually generated by switches.

So, the Weyl Group of \(\mathfrak{sl}(n+1,\mathbb{C}) \) is the group of permutations of \((n+1)\) objects.

The objects are the diagonal entries of \(H\) 

\[
    W \subset S_{\text{roots}} =S_{(n+1)^2 - (n+1) }
\]

\[
    W \cong S_{n+1}
\]

\section*{Abstract Root Systems}

Now we think about root systems on their own without thinking about their associated lie algebras.

\begin{definition}
    A (reduced) \underline{root system} \((E,R)\) is a finite dimensional real vector space \(E\) with an inner product \(\langle  \rangle \), together with a finite collection \(R\) ofnon-zero vectors in \(E\) satisying:

    \begin{enumerate}
        \item The vectors in \(R\) span \(E\) 
        \item (optional) If \(\alpha \in R\) and \(c\in\mathbb{R} \) then \(c \alpha \in R\) if and only if \(c=\pm 1\) 
        \item To each root \(\alpha \in R\) define a linear transformation [reflection] on \(E\) so that \(a_\alpha \cdot v = v - 2 \frac{\langle v,\alpha \rangle }{\langle \alpha ,\alpha  \rangle }\alpha\). Then \(\alpha ,\beta \in R \implies s_\alpha \cdot \beta \in R\) 
        \item For all \(\alpha ,\beta \in R\) we have \(2 \frac{\langle \beta ,\alpha  \rangle}{\langle \alpha ,\alpha  \rangle } \in \mathbb{Z} \)  
    \end{enumerate}

\end{definition}

But all abstract root systems actually come from lie algebras!

Condition \(2\) can be dropped, then we get the definition of non-reduced root-systems.

Then \(\alpha , c \alpha \in R \implies c = \pm \frac{1}{2}, \pm 1, \pm 2\)

\begin{definition}

    The dimension of the vector space \(E\) is called the \underline{rank} of the root system and elements of \(R\) are called the roots.

\end{definition}

Property 3 implies that if \(\alpha \) is a root then so is \(-\alpha \). This also follows from property 2, but even if property 2 is dropped we see that this follows from property 3.

\begin{definition}
    [Weyl Group] If \((E,R)\) is a root system, the \underline{Weyl Group} \(W\) of \(R\) is the subgroup of the orthogonal group of \(E\) [\(O(E)\)] generated by the reflection \(s_\alpha, \alpha \in R\) 
\end{definition}

Then \(W \subset O(E)\), \(W\) maps \(R\) into itself, \(R\) spans \(E\) 

Every \(w\in W,\) as a linear transformation of \(E\) is determined by its action on \(R\) .

So \(W\) is a subgroup of the group of permutations of \(R\) 

So \(W\) must be finite.

\begin{proposition}
    Suppose \((E,R)\) and \((F,S)\) are two root systems. Consider \(E \oplus F\) with natural inner product determined by the inner products of \(E\) and \(F\) [\(\langle (x,y),(p,q) \rangle = \langle x,y \rangle + \langle p,q \rangle  \) ]. Then \(R \cup S\) is a root system of \(E \oplus F\) called the \underline{direct sum} of \(R\) and \(S\).
\end{proposition}

\underline{Clarification:} we identify element \(e\in E\) with \((e,0)\) in \(E \oplus F\), \(f\in F\) with \((0,f)\) in \(E \oplus F\) 

So we can think of \(R \cup S\) as a subset of \(E \oplus F\) 

Elements of the form \( (\alpha ,\beta )\) with \(\alpha \in R,\beta \in S\) are not in \(R \cup S\) 

\begin{definition}
    A \underline{root system} \((E,R)\) is called reducible if there exists an orthogonal decomposition \(E = E_1 \oplus E_2\) with \(\dim E_1 >0\) and \(\dim E_2 > 0\) so that every element of \(R\) is either in \(E_1\) or \(E_2\) 

    If no such decomposition exists we call the function irreducible.

\end{definition}

If \((E,R)\) is reducible, then by definition we have \(E = E_1 \oplus E_2\). Let \(R_1 = E_1\cap R\) and \(R_2 = E_2\cap R\) then \(R_1 ,R_2\) are roots of \(E_1\) and \(E_2\) exactly.

\((E,R)\) is reducible \(\iff\) \((E,R)\) can be realized as a direct sum of two other root system.

\begin{theorem}
    [Hall 7.35] A complex semisimple Lie algebra \(\mathfrak{g} \) is \underline{simple} \(\iff \) its root system is irreducible.
\end{theorem}

The idea is, for any two roots, you can find intermediate roots and jump to the other root this way.

\begin{definition}
    Two root systems \((E,R)\) and \((F,S)\) are said to be isomorphic if there exsits an invertible linear transformation \(A: E\to F\) such that \(A\) maps \(R\) onto \(S\) such that \(\forall \alpha \in R\) and \(v\in E\), we have \(A(s_\alpha \cdot v) = s_{A \alpha}\cdot (Av)\).

    A map \(A\) with this property is an \underline{isomorphism}

    Note that this does not necessarily preserve inner product. It only preserves reflections of the roots.

\end{definition}

Rescaling inner product: Suppose you rescale the inner product. Note that \(s_\alpha \cdot v = v - \frac{\langle v,\alpha  \rangle }{\langle \alpha ,\alpha  \rangle} \alpha\) so as long as ratio of inner product is satisfied reflection is satisfied. So rescaling doesn't change reflection. More complicated: if \(E = E_1 \oplus E_2\) then we can have different rescaling in \(E_1\) and \(E_2\). 

\begin{proposition}
    Suppose \(\alpha ,\beta \) are two roots that are not multiples of each other. For simplicity assume that \(\langle \alpha ,\alpha  \rangle \geq \langle \beta ,\beta  \rangle \). Then one of the following must hold:
    
    \begin{enumerate}
        \item \(\langle \alpha ,\beta  \rangle = 0\) 
        \item \(\langle \alpha ,\alpha  \rangle = \langle \beta ,\beta  \rangle \) and the angle between \(\alpha ,\beta \) is either \(\frac{\pi}{3}\) or \(\frac{2\pi}{3}\) 
        \item \(\langle \alpha ,\alpha  \rangle = 2\langle \beta ,\beta  \rangle \) and the angle is \(\frac{\pi}{4}\) or \(\frac{3\pi}{4}\) 
        \item \(\langle \alpha ,\alpha  \rangle = 3 \langle \beta ,\beta  \rangle \) and the angle is \(\frac{\pi}{6}\) or \(\frac{5\pi}{6}\) 
    \end{enumerate}

    Conclusion: Possingle angles are \(\frac{\pi}{2}, \frac{\pi}{3}, \frac{2\pi}{3}, \frac{\pi}{4}, \frac{3\pi}{4},\frac{\pi}{6},\frac{5\pi}{6}\) 

\end{proposition}

The main purpose of this result is simplification of root systems.

[insert picture of possibilities]

\(\alpha ,\beta \) generate lattices in \(\mathbb{R}^2\)

\begin{proof}
    Let \(m_1 = 2 \frac{\langle \alpha , \beta  \rangle }{\langle \alpha ,\alpha  \rangle }, m_2 = 2 \frac{\langle \beta ,\alpha  \rangle}{\langle \beta,\beta  \rangle }\) 

    Since \(\alpha \) is the larger root, \(\vert m_2 \vert \geq \vert m_1 \vert\) 

    Assume \(\alpha ,\beta \) are not perpendicular, since in that case \(m_1=m_2=0\) and not interesting.

    Then, \(\frac{m_2}{m_1}= \frac{\langle \alpha ,\alpha  \rangle }{\langle \beta ,\beta  \rangle } \geq 1 \) 

    And \(m_1 m_2 = 4 \frac{\langle \alpha ,\beta \rangle^2 }{\langle \alpha ,\alpha  \rangle \langle \beta ,\beta  \rangle }=4\cos^2 \theta\) 

    So \(m_1 m_2 = 1,2,3,4\)
    
    \(m_1 m_2=4 \implies \cos ^2\theta = 1\) so \(\alpha ,\beta \) are proportional

    Other cases the angles give us lattices.

\end{proof}

\hfil
\hrule

Class 09: 02/06

The plan for today is \underline{root systems, rank 2 systems, dual root systems}

\begin{definition}
    A (reduced) \underline{root system} \((E,R)\) is a finite-dimensional real vector space \(E\) with an inner product \(\langle , \rangle \) together with a finite collection \(R\) of non-zero vectors in \(E\) so that:

    \begin{enumerate}
        \item \(R\) spans \(E\) 
        \item If \(\alpha \in R\) and \(c\in\mathbb{R}\) then \(c \alpha \in R\) if and only if \(c=\pm 1\) 
        \item \(\forall \alpha \in R\), define a reflection \(s_\alpha \) so that \(s_\alpha \cdot v = v - 2 \frac{\langle v,\alpha \rangle }{\langle \alpha ,\alpha  \rangle }\alpha ,v\in E\). Then, for all \(\alpha ,\beta \in R,s_\alpha \cdot \beta \in R\) 
        \item \(\forall \alpha ,\beta \in R\) we have \(2 \frac{\langle \beta ,\alpha  \rangle }{\langle \alpha ,\alpha  \rangle }\in\mathbb{Z}\)  
    \end{enumerate}

\end{definition}

Originally they were introduced for classifying complex semisimple lie algebras. They can be repurposed to classify real semisimple lie algebras [drop condition 2]. This is also useful for sphere packing, data transmission, error correcting codes [because these provide multi-dimensional lattices].

Weyl Group \(\subset O(E)\) 

Weyl group \(\subset\) permutation group of \(R\) which tells us Weyl Group is finite.

We can take direct sum of root systems \((E,R)\oplus (F,S)\) 

We also have reducible and irreducible root systems. Essentially, if root system decomposes into direct sum of two root system it is called reducible, otherwise it is called irreduible.

Important result: A complex semisimple lie algebra is \underline{simple} if and only if its root system is \underline{irreducible}.

\underline{Isomorphism of Root Systems:} It's a little beat counterintuitive. It is a linear map between the vector space, and a bijection between roots, but we \underline{do not} require it to preserve inner product. We only require it to preserve the reflections.

\underline{Question:} What are the possible angles between two roots?

They are \(\frac{\pi}{2},\frac{\pi}{3},\frac{2\pi}{3},\frac{\pi}{4},\frac{3\pi}{4},\frac{\pi}{6},\frac{5\pi}{6}\).

In degrees: \(90^\circ, 60^\circ,120^\circ, 45^\circ, 135^\circ, 30^\circ,150^\circ\) 

\begin{proposition}
    Suppose \(\alpha ,\beta \) are two roots and they are not proportional. WLOG suppose \(\langle \alpha ,\alpha  \rangle \geq \langle \beta ,\beta  \rangle \). Then one of the following must be true.

    \begin{enumerate}
        \item \(\langle \alpha ,\beta  \rangle = 0\) [not much to say]
        \item \(\langle \alpha,\alpha  \rangle = \langle \beta ,\beta  \rangle \) and the angle between \(\alpha ,\beta \) is \(\frac{\pi}{3}\) or \(\frac{2\pi }{3}\) 
        \item \(\langle \alpha ,\alpha  \rangle = 2 \langle \beta ,\beta  \rangle \) and angle is \(\frac{\pi}{4}\) or \(\frac{3\pi }{4}\) 
        \item \(\langle \alpha ,\alpha  \rangle = 3 \langle \beta ,\beta  \rangle \) and angle is \(\frac{\pi}{6}\) or \(\frac{5\pi}{6}\) 
    \end{enumerate}

\end{proposition}

\begin{proof}

    [Insert Picture of the acute angles here. Draw \(\alpha ,\beta \) and drop perpendicular from tip of \(\beta\) to \(\alpha\)]

    \(m_1 = 2 \frac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle },m_2=2\frac{\langle \beta ,\alpha  \rangle }{\langle \beta ,\beta  \rangle }\) 

    Then we have \(\vert m_2 \vert \geq \vert m_1 \vert \) 

    Suppose \(\langle \alpha ,\beta  \rangle \neq 0\) then,

    \(\frac{m_2}{m_1}=\frac{\langle \alpha ,\alpha  \rangle }{\langle \beta ,\beta  \rangle }\geq 1\) 

    \(m_1 m_2=4 \frac{\langle \alpha ,\beta  \rangle ^2}{\langle \alpha ,\alpha  \rangle^2 \langle \beta ,\beta  \rangle ^2}=4\cos^2\theta \) 

    Where \(\theta =\) angle between \(\alpha ,\beta \) 

    Thus \(1 \leq m_1 m_2 =4 \) 

    So \(m_1 m_2 = 1,2,3,4\) 

    \(m_1 m_2 = 4 \implies \cos\theta = \pm 1 \) so \(\alpha ,\beta \) are perpendicular, we assumed it won't be that.

    \(m_1 m_2 = 1 \implies \cos^2\theta = \frac{1}{4}\) so \(\theta = \frac{\pi}{3}\) or \(\frac{2\pi}{3}\) 

    \(m_1 = m_2 = 1\) or \(m_1 = m_2 = -1\) so \(\frac{m_2}{m_1}=\frac{\langle \alpha ,\alpha  \rangle }{\langle \beta ,\beta  \rangle }=1\)
    
    If \(m_1 m_2 = 2\) then \(\cos^2\theta = \frac{1}{2}\) so \(\theta = \frac{\pi}{4}\) or \(\frac{3\pi}{4}\) so \(m_1=1,m_2=2 \) or \(m_1=-1,m_2=-2\) 

    \(\frac{m_2}{m_1}=\frac{\langle \alpha ,\alpha \rangle }{\langle \beta ,\beta  \rangle }=2\) 

    If \(m_1 m_2 = 3\) then \(\cos^2\theta = \frac{3}{4}\) so \(\theta = \frac{\pi}{6}\) or \(\frac{5\pi}{6}\) 

    \(m_1 =1, m_2=3\) or \(m_1 = -1,m_2=-3\) so \(\frac{m_2}{m_1}=\frac{\langle \alpha ,\alpha  \rangle }{\langle \beta ,\beta  \rangle }=3\) 

\end{proof}

One of the main application of this corollary is to show that a root has a basis, a set of `positive' roots.

\underline{Corollary:} Suppose \(\alpha ,\beta \in R\), not proportional. If the angle between them is strictly obtuse (\(> \frac{\pi}{2}\) ), then \((\alpha + \beta )\) and \(-(\alpha +\beta )\) are roots. If the angle between them is strictly acute (\(<\frac{\pi}{2}\)), then \((\alpha-\beta)\) and \((\beta -\alpha )\) are roots.

Note that we only work on the non-perpendicular case.

\begin{proof}
    Assume \(\alpha \) is the longer root [\(\langle \alpha ,\alpha  \rangle > \langle \beta ,\beta  \rangle \)]

    Draw picture of the acute case. Drawing a perpendicular from \(\beta \) to \(\alpha\) always results in intersecting at \(\frac{\alpha}{2}\). So, \(s_\alpha\cdot;b = \beta - \alpha\) so \(\beta -\alpha \) is a root and thus \(\alpha - \beta \) is a root too.

    The obtuse case is similar, only the perpendicular drops at \(-\frac{\alpha}{2}\) 

\end{proof}

There is not much to say about rank 1 systems but we can talk about rank 2 systems.

\section*{Root Systems of Rank 1 and 2}

\underline{Rank 1:} In a straight line, we have \(0,\alpha ,-\alpha\). So, the only reduced root system is \(R = \{ \alpha ,-\alpha \} \). This is realized by \(\mathfrak{sl}(2,\mathbb{C})\). This is called \(A_1\), and the Weyl Group has only two elements: identity and reflection so it's \(W=C_2\)

\underline{Rank 2:} In 2d, we can have \(\alpha ,-\alpha \) and perpendicular to them \(\beta,-\beta\) [they don't have to be the same length]. So, \(R = \langle \alpha ,-\alpha ,\beta ,-\beta  \rangle \) so it is \(A_1\times A_1\) so it is reducible and decomposes. \(W=C_2\times C_2\). This is uninteresting.

More interesting is the one of \(\mathfrak{sl}(3,\mathbb{C})\). Let \(\alpha,\beta\) have an angle of \(\frac{2\pi}{3}\) between them, then they make a hexagon and thus our root system is \(\{ \alpha ,-\alpha ,\beta ,-\beta ,\alpha +\beta,-\alpha -\beta\} \). It is of type \(A_2\), and \(W=D_3\) [not \(D_6\) since it doesn't contain rotations by \(\frac{\pi}{3}\)].

Angle \(\frac{3\pi}{4}\) gives us a `square' of roots. Our root space is then \(R=\{ \alpha ,-\alpha ,\beta ,-\beta ,\alpha +\beta ,-\alpha -\beta ,\alpha +2\beta ,-\alpha -2\beta  \} \). This is of type \(B_2\) and \(W = D_4\) 

Angle \(\frac{5\pi}{6}\) gives us the star of david. The roots are \(R=\{\alpha ,-\alpha ,\alpha +\beta ,-\alpha -\beta ,\alpha +2\beta ,-\alpha -2\beta ,\alpha +3\beta ,-\alpha -3\beta,2\alpha +3\beta ,-2\alpha -3\beta\}\). This is of type \(G_2\), and we have two hexagons [outer and inner], and the reflections preserve the hexagons. We have \(W = D_6\) 

Note that we have integer coefficients, and all coefficients are positive or negative. No mixing!

\begin{proposition}
    Every rank 2 root system is isomorphic to one of these: \(A_1\times A_1,A_2, B_2\) or \(G_2\). The subscript refer to the rank of the system.
\end{proposition}

\begin{proof}
    Assume \(E=\mathbb{R}^2\) with standard inner product. So the root system \(R \subset\mathbb{R}^2\) 

    Let \(\theta\) be the smallest angle between any two roots in \(R\).

    We know that \(R\) spans \(\mathbb{R}^2\) so we can find two roots \(\alpha ,\beta \) that are linearly independent, and look at the angle between them.

    If \(\langle (\alpha ,\beta ) \rangle \) [angle between \(\alpha ,\beta \)] is \(>\frac{\pi}{2}\) then \(\langle (\alpha ,-\beta) \rangle < \frac{\pi}{2}\).

    So \(\theta\leq\frac{\pi}{2}\) 

    So \(\theta=\frac{\pi}{2},\frac{\pi}{3},\frac{\pi}{4},\frac{\pi}{6}\).

    Note: \(-s_\beta \cdot \alpha \in R\) and \(\beta \) bisects the angle between \(\alpha,-s_\beta \cdot \alpha \) 

    So, we can keep reflecting and keep getting roots in angles \(\theta ,2\theta ,3\theta ,\cdots\) and by the nature of \(\theta \) we come back to the original root. We get a picture of alternating long root, short root andon and on. Also, angle between the roots determine the ratio of their lengths.

    So, we must end up in one of the mentioned cases.

\end{proof}

\begin{proposition}
    [A statement about Weyl Groups] Basically the Weyl Groups, with the exception of the first case, we get dihedral groups or product of two groups of order 2. This is proved by working case by case. It is clear that it must be a subgroup of a dihedral group.
\end{proposition}

This concludes the story of the rank 2 systems.

\section*{Dual Root Systems}

\begin{definition}
    [coroot] Let \((E,R)\) be a root system. For each root \(\alpha \in R\) define the \underline{coroot} \(H_\alpha  = \frac{2\alpha}{\langle \alpha ,\alpha  \rangle }\). The set of all coroots is denoted by \(R^\vee\) and this is called the dual root system.
\end{definition}

\begin{proposition}
    If \(R\) is a root system, then so is \(R^\vee\). \((R^\vee)^\vee\cong R\)  
\end{proposition}

\begin{proof}
    Each \(H_\alpha \) is proportional to \(\alpha \) so \(H_\alpha \) span \(E\). The only scalar multiple of \(H_{\alpha} \) in \(R^\vee\) are \(H_\alpha \) and \(H_{-\alpha}=-H_\alpha\).

    \(s_\alpha = s_{H_\alpha}\) so the Weyl Group must be the same.

    Now some calculation:

    \(\langle H_\alpha ,H_\alpha \rangle = 4\frac{\langle \alpha ,\alpha  \rangle }{\langle \alpha ,\alpha  \rangle ^2}=\frac{4}{\langle \alpha ,\alpha  \rangle }\)
    
    \(2 \frac{H_\alpha}{\langle H_\alpha ,H_\alpha \rangle }= 2 \frac{H_\alpha}{\langle H_\alpha,H_\alpha \rangle }=2\frac{2\alpha}{\langle \alpha ,\alpha \rangle }\frac{\langle \alpha ,\alpha  \rangle }{4}=\alpha\) 

    So \((R^\vee)^\vee = R\)
    
    \(2\frac{\langle H_\alpha,H_\beta \rangle}{\langle H_\alpha ,H_\alpha \rangle }= \langle \alpha ,H_\beta \rangle = 2\frac{\langle \alpha ,\beta \rangle }{\langle \beta ,\beta  \rangle }\in\mathbb{Z}\) 

    Interestingly, \((H_\alpha ,H_\beta )\leftrightsquigarrow (\beta ,\alpha)\) 

    \(s_{H_\alpha}\cdot H_\beta =s_{\alpha}\cdot H_\beta=2\frac{s_\alpha \cdot \beta}{\langle \beta ,\beta  \rangle }=2 \frac{s_\alpha\cdot \beta}{\langle s_\alpha \cdot \beta , s_\alpha \cdot \beta \rangle }=H_{s_\alpha\cdot \beta}\in R^\vee\) 

\end{proof}

Example: If all roots in \(R\) has the same length (type \(A_n\)) then \(R^\vee\cong R\) by rescaling.

If the roots are of different lengths \(R^\vee\) may or may not be isomorphic to \(R\)

\hfil
\hrule

Class 10: 02/08

Plan for today's class: Bases and Weyl Chambers

Recap: root system has dual root system \(\alpha \mapsto H_\alpha = \frac{2\alpha }{\langle \alpha ,\alpha  \rangle }\). Long roots become short, short roots become long.

If all roots in \(R\) have the same length [\((A_n)\)] then \(R^\vee \cong R\)

If there are roots of different length, this may or many not be true.

For a case where we have an isomorphism, look at \(B_2\)

[insert picture of \(B_2\) and \(B_2^\vee\)]

Last class we classified all root systems of rank 2.

\section*{Bases and Weyl Chambers}

We want classification of irreducible representation \((\pi,V)\) 

If \(\mathfrak{g}\) is a simple lie algebra, then the adjoint representation \((\ad,\mathfrak{g})\) is irreducible.

\begin{definition}
    If \((E,R)\) is a root system, a subset \(\Delta\) of \(R\) is called a \underline{base} of \(R\) if the following conditions are satisfied:

    \begin{enumerate}
        \item \(\Delta\) is a basis for \(E\) as a vector space
        \item Each root \(\alpha\in R\) can be expressed as a linear combination of roots in \(\Delta\) where the coefficients are integers, and all coefficients have the same sign.
    \end{enumerate}

\end{definition}

Will prove the existence soon.

Now we have notions of \underline{positive roots} \(R^+\) relative to \(\Delta\) [the one with positive coefficients] and \underline{negative roots} \(R^-\)  relative to \(\Delta\) the ones with negative sign.

The elements of \(\Delta\) are called \underline{simple positive roots}

\begin{proposition}
    If \(\alpha ,\beta \in \Delta, \alpha \neq \beta\) then \(\langle \alpha ,\beta  \rangle \leq 0\) [the angle between them is right or obtuse] 
\end{proposition}

\begin{proof}
    Since \(\alpha \neq \beta, \langle \alpha ,\beta  \rangle >0 \implies \) acute angle.

    By corollary from previous class, \(\alpha -\beta \in R\) 

    But this root has both negative and positive coefficients. This is a contradiction, since \(\Delta\) is a basis. So acute is not possible, it must be right or obtuse.
\end{proof}

\begin{proposition}
    \(\exists \) hyperplane \(V\) through the origin in \(E\) that doesn't contain any roots.
\end{proposition}

\begin{proof}
    For each root \(\alpha \) define hyperplane \(V_\alpha\) perpendicular to \(\alpha\). So, \(v_\alpha = \{ v\in E : \langle \alpha , v \rangle = 0 \} \) 

    Observe hyperplane \(V\) that is perpendicular to \(v\in E\) does not contain \(\alpha\) if and only if \(v\in V_\alpha\) for each root \(\alpha\).
    
    Let \(v\) be a vector so that it is in none of the \(V_\alpha\). Since \(R\) is finite, we need to avoid finitely many hyperplanes, and \(U=\bigcup_{\alpha \in R}^{} V_\alpha \neq E\). So we can let \(v\in E \setminus U\) 

    The hyperplane perpendicular to \(v\) gives us the desied hyperplane \((V\cap R) = \varnothing\) 
\end{proof}

We will first talk about positive and negative roots and then discuss what to put in our base.

\begin{definition}
    Let \((E,R)\) be a root system, \(V\)- hyperplane through the origin that avoids roots [\(V \cap R = \varnothing \) ]. Choose one ``side'' of \(V\) and let \(R^+\) be the roots on that side, and \(R^-\) be the roots on the other side.
\end{definition}

What do we mean by the same side? Recall that we characterize a hyperplane \(V\) through a vector that is perpendicular to it, lets say \(v\). Then we can choose direction by same or opposite direction as \(v\) 

Let \(V = \{ v\in E, \langle H,v \rangle =0 \} \) .

An element \(\alpha \in R^+\) is \underline{decomposable} if there exists \(\beta ,\gamma \in R^+\) so that \(\alpha = \beta +\gamma\) Otherwise it is called \underline{decomposable}.

The main result in this section [if we start with the root system] is that the indecomposable roots form a base.

\begin{theorem}
    Suppose \((E,R)\) is a root system, \(V\) is a hyperplane passing through the origin that doesn't intersect the roots. Thus we have \(R^+\) and \(R^-\). The set of indecomposable elements of \(R^+\) is a base for \(R\). 
\end{theorem}

If we prove this we have actually proved existence.

\begin{proof}
    Notation: \(\Delta =\) \{indecomposable elements in \(R^+\)\}

    \(H\)-vector in \(R\) such that \(H\perp v\) 

    \(\langle H,\alpha \rangle > 0\) for all \(\alpha \in R^+\) 
    
    \textbf{Part 1}: Every \(\alpha\in R^+\)  can be expressed as a linear combination of elements from \(\Delta\) with \(\geq 0\) integers as coefficient.

    Proof for this is from contradiction. Then suppose some roots can't be chosen, and take the root \(\alpha \) with the smallest possible \(\langle H,\alpha \rangle \). If the same value is achieved by multiple roots pick one randomly.

    Then \(\alpha \notin \Delta\) [otherwise coefficient 1]

    Thus \(\alpha =\beta_1 + \beta_2 \) for \(\beta_1,\beta_2 \in R^+\)
    
    But \(\langle H,\alpha \rangle = \langle H,\beta _1 \rangle + \langle H,\beta _2 \rangle \).

    The latter two are positive and thus we have positive root with smaller inner product, which is a contradiction.

    So all values can be expressed as a linear combination of elements from \(\Delta\) with non-negative coefficients.

    \textbf{Part 2:} If \(\alpha ,\beta \in \Delta , \alpha \neq \beta , \langle \alpha ,\beta  \rangle \leq 0\) .

    Suppose not. Then \(\langle \alpha ,\beta  \rangle >0\) implies \(\alpha - \beta,\beta -\alpha \in R^+\). One of them is in \(R^+\) and we have \(\alpha = (\alpha -\beta )+\beta \) or \(\beta = (\beta -\alpha )+\alpha\)which contradicts the fact that they're indecomposable.

    \textbf{Part 3:} The elements of \(\Delta \) are linearly independent.
    
    Suppose not. Then \(\Sigma_{\alpha \in \Delta}c_\alpha \alpha = 0\).

    Seperate the sum: \(c_\alpha >0\) and \(c_\alpha =-d_\alpha\) when \(c_\alpha < 0\) 

    Then \(\sum c_\alpha  \alpha  = \sum d_\beta \beta  \)
    
    Sums range over disjoint subsets of \(\Delta\).

    Let this be \(u\) 

    \[
        \langle u,u \rangle = \left\langle \sum c_\alpha \alpha  , c_\beta \beta \right\rangle = \sum c_\alpha d_\beta \langle \alpha ,\beta \rangle  
    \]

    Since \(c_\alpha ,d_\beta \geq 0\) and \(\langle \alpha ,\beta  \rangle \leq 0\) we have \(\langle u,u \rangle =0 \implies u=0\) 

    Thus \(\langle H,u \rangle = \sum c_\alpha \langle H,\alpha \rangle = 0 \implies c_\alpha = 0 \) for all \(\alpha\) 

    \textbf{Part 4:} \(\Delta\) is a base.

    \(\Delta\) is linearly independent.

    Every element of \(R^+\) is a linear combination of \(\Delta\) with non-negative integer coefficient.

    If \(\beta \in R^-\) then \(-\beta \in R^+\) which means \(-\beta\) is a linear combination of \(\Delta\) with non-negative integer coefficients and thus \(\beta \) is a linear combination of \(\Delta \) with non-positive integer coefficients.

    Since every element of \(R\) is expressible as a linear combination of \(\Delta\) and \(R\) spans \(E\) we can say that \(\Delta\) spans \(E\).

    So \(\Delta\) is a basis.

\end{proof}

Note that our choice of \(\Delta\) is fairly arbitrary. Must the choice be the ones made by the theorem above?

\begin{theorem}
    For any base \(\Delta\) for \(R,\exists \) hyperplane \(V\) and a side of \(V\) such that \(\Delta\) arises as in the previous theorem.
\end{theorem}

\begin{proof}
    [Sketch]

    Let \(r\) be the rank of the system and let \(\Delta = \alpha _1, \dots , \alpha _r \).
    
    For each \(c_1, \dots ,c_r\in \mathbb{R}\) there exists \(H\in E\) such that \(\langle H,\alpha _i \rangle = c_i\) for \(i = 1,\dots r\) 

    Choose \(c_1 , \dots , c_r > 0\) and solve for \(H\).

    The hyperplane perpendicular to \(H\) works.

    We need to show that each element of \(\Delta\) is indecomposable.

    If not, there exist \(\beta\in \Delta\) so that \(\beta = \beta_1 + \beta_2\) where \(\beta_1,\beta_2 = \sum c_\alpha \alpha , \sum d_\alpha \alpha  \). Linear independence implies \(c_\beta + d_\beta =1\) and other \(c_\alpha +d_\alpha =0\) but \(1\) can't be written as sum of two integers so one of \(\beta _1,\beta _2\) must be \(\beta\) 
\end{proof}

Lemma: Let \(\Delta \) be a base, \(R^+\) be associated positive roots, \(\alpha \in \Delta\). Then \(\alpha\) \underline{cannot} be expressed as a linear combination of elements \(R^+ \setminus \{ \alpha \} \) with non-negative real coefficients.

\begin{proof}
    Let \(\alpha =\alpha _1\) and other elements of \(\Delta \) be \(\alpha _2,\dots \alpha _r\).

    Suppose \(\alpha =\) linear combination of roots \(\beta \in R^+, \beta \neq \alpha\) with non-negative coefficients.

    Each \(\beta \in R^+\) is a linear combination of \(\alpha _1, \dots , \alpha _r\) with non-negative coefficients. Thus,

    \(\alpha _1 = c_1 \alpha _1 + c_2 \alpha _2 + \dots + c_r \alpha _r\) for \(c_j \geq 0\) 

    All \(\alpha\) are linearly independent.

    So \(c_1 = 1, c_j = 0\) for \(j \geq 2\) 

    Thus, each \(\beta\) is proportional to \(\alpha\) 

    So, \(\beta = \pm \alpha\) 

    Since \(-\alpha \in R^-,\beta = \alpha\).

    This contradicts that \(\beta \in R^+ \setminus \{ \alpha \} \)
\end{proof}

\begin{proposition}
    If \(\Delta\) is a base of \(R\), then the set of all coroots \(H_\alpha, \alpha \in \Delta\) form a base for the dual root system \(R^\vee\) 
\end{proposition}

\begin{proof}
    Note that the basis part is trivial since corooting is just scaling.

    We need to show that all coefficients are integer and have the same sign.

    Proof is in the book.
\end{proof}

Next up: Weyl Chambers.

\hfil
\hrule

Class 11: 02/13

HW correction: if \(\mathfrak{g}\) is a \underline{real} simple lie algebra then we can have more than `one' invariant bilinear form.

Suppose \(\mathfrak{g}\) has complex structure, eg \(\mathfrak{g} = \mathfrak{sl} (2,\mathbb{C})_\mathbb{R}=\mathfrak{so}(3,1)\).

Then, \(K(X,Y),K(iX,Y)\) are both invariant bilinear form but not proportional.

Now back to class. Recap:

\begin{definition}
    \(\Delta \subset R\) is a base if:

    \begin{enumerate}
        \item \(\Delta \) is a vactor space basis of \(E\) 
        \item Every \(\beta \in R\) can be expressed as \(\beta = \sum_{\alpha \in \Delta } c_\alpha \alpha \) with coefficients \(c_\alpha \in \mathbb{Z}\) and either all are non-negative or all are non-positive.
    \end{enumerate}
\end{definition}

After we have chosen a base \(\Delta\) we have positive roots \(R^+\) and \(R^-\)

\begin{definition}
    Given \(\Delta\), \underline{Positive roots} \(R^+\) are those with non-negative coefficients, \underline{Negative roots} are those with non-positive coefficients. 
\end{definition}

\begin{theorem}
    Bases exist and can be constructed as follows:

    Choose a hyperplane \(V \subset E\) such that \(V\not\ni\alpha\) for all \(\alpha \in R\). We can do this since \(R\) is finite.

    Any time we have an hyperplane, we describe it with a vector perpenducular to it.

    Let \(R^+\) be roots on one `side' of the hyperplane, and \(R^-\) be the roots on the other side of the hyperplane.

    We call a root in \(R^+\) is \underline{indecomposable} if it cannot be written as a sum of two other positive roots.

    Then \(\Delta =\) indecomposable roots in \(R^+\)

    \underline{Actual theorem:} \(\Delta\) is a base.

    \underline{Actual theorem 2:} Every possible can be constructed this way.

\end{theorem}

Now we talk about \underline{Weyl Chambers}

\begin{definition}
    [Weyl Chambers]

    The open weyl chambers for a root system \((E,R)\) are the connected components of \(E \setminus \bigcup_{\alpha \in R}^{} V_\alpha\) where \(V_\alpha\) is the hyperplane through origin perpendicular to \(\alpha\) [\(V_\alpha = \{ v\in E : \langle \alpha ,v \rangle = 0 \} \) ].
\end{definition}

\begin{definition}
    If \(\Delta = \{ \alpha_1, \dots , \alpha_r \}\) is a base for \(R\), then then it has the \underline{open fundamental weyl chamber}.

    The \underline{open fundamental Weyl chamber} in \(E\) (relative to \(\Delta\) ) is the set of all \(v\in E\) such that \(\langle \alpha_j, v \rangle > 0\) for all \(j = 1, \dots ,r\).

    Note: all other roots are given by linear combination of the base. So, for other roots, inner product with positive roots will be positive, inner product with negative roots will be negative.

\end{definition}

Why should this be non-empty? Recall that \(\Delta\) are roots in one side of a hyperplane, and the side of that hyperplane is given by a vector \(h\) that is perpendicular to the hyperplane, which means \(\langle h, \alpha_j \rangle > 0\). Thus the Weyl Chamber is non-empty.

\underline{Example:} Consider the root system \(A_2\) associated to \(\mathfrak{sl}(3,\mathbb{C}) \). Recall that this means all roots lie on a regular hexagon. Choose a hyperplane, then positive roots are \(\alpha _1,\alpha _2,\alpha _1+\alpha _2\).

Consider the half space on side of \(\alpha_1\), and half space on side of \(\alpha_2\) [given by hyperplanes perpendicular to \(\alpha_1,\alpha _2\)]. Their intersection gives us the Weyl Chamber.

[insert picture]

The `walls' of are \underline{NOT} the roots. Instead, they are the hyperplanes perpendicular to the roots.

\begin{proposition}
    We have a bijection between Weyl Chambers and Bases.

    \(\forall\) open Weyl Chamber \(C\), \(\exists \) a unique base \(\Delta_C\) for \(R\) such that \(C\) is the open fundamental weyl chamber associated to \(\Delta_C\). The positive roots with respect to \(\Delta_C\) are precisely those elements \(\alpha\) of \(R\) such that \(\alpha\) has \(>0\) inner product each element of \(C\).
\end{proposition}

Basically, given \(\Delta\), we have

\(C = \{ v\in E; \langle \alpha_1,v \rangle > 0, \dots , \langle \alpha_r,v \rangle > 0 \} \) 

Given open Weyl Chamber \(C\), we have,

\(R^+_C = \{ \alpha \in R : \langle \alpha ,v \rangle > 0 \forall v\in C \}\) 

\(\Delta_C = \) indecomposable elements in \(R_c^+\) 

\begin{proof}
    Let \(H\in C\) and associated to \(H\) is a hyperplane through origin \(V\) that is perpendicular to \(H\) 

    Since \(H\in\) open Weyl Chamber, we have \(\langle H,\alpha \rangle \neq 0\) for \(\alpha \in R\).

    Thus, \(\alpha\notin V\) for all \(\alpha \in R\) 

    \(H,V\) thus gives us a base \(\Delta\).

    \(R_H^+ = \{ \alpha \in R : \langle \alpha ,H \rangle > 0 \} \) 

    \(R_H^- = \{ \alpha \in R : \langle \alpha ,H \rangle < 0 \} \) 

    Given \(R_H^+\) we have \(\Delta_C\) indecomposable elements. This is our \(\Delta_C\) 

    What is the open fundamental Weyl Chamber associated to \(\Delta_C\)?

    Suppose \(H\) is in this fundamental Weyl Chamber.

    Then, for each root \(\alpha_j\), all \(v\in C\) satisfies \(\langle \alpha_j, v \rangle > 0\).

    Since \(C\) is conneted, it does not change sign. So, the associated Weyl Chamber is just \(C\).

    Finally, we prove that two different bases \(\Delta_c, \Delta ^{\prime} \) cannot have the same open fundamental Weyl chamber.

    Suppose it has. Then, for all \(\alpha \in \Delta^{\prime} \) we have \(\langle \alpha ,H \rangle > 0\). This means the positive roots associated to \(\Delta_C\) are the same as those of \(\Delta^{\prime} \).

    Since base is given by indecomposable elements given a selection of positive roots, \(\Delta_C\) and \(\Delta^{\prime} \) have to be the same.

\end{proof}

Thus we have a bijection between open Weyl Chambers and basis.

\begin{proposition}
    Every root \(\alpha \in R\) is an element of some base. 
\end{proposition}

The proof of this is very technical.

\underline{Idea of Proof:} Consider the hyperplane \(V_\alpha \subset E\) associated to the root \(\alpha\). Then (part of) \(V_\alpha\) appears as the \((n-1)\) dimenstional ``wall'' of some open Weyl Chamber. It actually appears as a face of co-dimension \(1\). Lets call the Weyl Chamber \(C\).

Then, associated to this Weyl Chamber \(C\) we have a base \(\Delta_C\). Thus \(C\) is the fundamental Weyl Chamber of \(\Delta_C\).

Then, \(\alpha \in \Delta_C\) 

\section*{Weyl Chambers and The Weyl Group}

\begin{proposition}
    The following are true:

    \begin{enumerate}
        \item The Weyl Group acts transitively on the set of open Weyl Chambers.
        \item If \(\Delta\) is a base, then \(W\) is generated by \(s_\alpha\cdot\alpha \in \Delta\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Fix a base \(\Delta\), let \(C\) be the associated open fundamental Weyl Chamber and let \(W^{\prime}\) be the subgroup of \(W\) generated by \(s_\alpha,\alpha \in \Delta\).

    To prove (1), it's sufficient to prove that \(W^{\prime} \) acts transitively.

    Let \(D\) be any other open Weyl Chamber. Puck \(H\in C,H^{\prime} \in D\). Choose \(w\in W^{\prime} \) so that \(\vert w\cdot H^{\prime} - H \vert \) is minimal. [Since \(W\) is finite, \(W^{\prime} \) is finite so we're talking about finitely many distances so it is always possible to pick the minimal. If they're multiple pick one at random.]

    If \(w\cdot H^{\prime} \notin C\) there must be some \(\alpha \in \Delta \) such that \(\langle \alpha ,w\cdot H^{\prime}  \rangle < 0\) 

    Then \(\vert w\cdot H^{\prime} - H \vert ^2 - \vert s_\alpha \cdot w\cdot H^{\prime} - H \vert ^2 = \vert H^{\prime} \vert ^2 + \vert H \vert ^2 - 2 \langle w\cdot H^{\prime} ,H \rangle - \vert H^{\prime}  \vert ^2 - \vert H \vert ^2 + 2 \langle s_\alpha \cdot w \cdot H^{\prime} , H \rangle \) 

    \(= 2 \langle s_\alpha \cdot w \cdot H^{\prime}  - w \cdot H^{\prime} , H \rangle = -4 \langle w\cdot H^{\prime} ,\alpha  \rangle \frac{\langle \alpha ,H \rangle }{\langle \alpha ,\alpha  \rangle } > 0\) 

    So, the distance is not actually minimal, there is a shorter distance. So, there is a contradiction, and \(W^{\prime}\) indeed acts transitively.

\end{proof}

\hrulefill

Class 12: 02/15

Recap:

\underline{Open Weyl Chamber:} Connected component of \(E \setminus \bigcup_{\alpha\in R}^{} V_\alpha\) where \(V_\alpha\) is the hyperplane perpendicular to the root \(\alpha\) 

We have a bijection between bases \(\Delta\) of \(R\) and open Weyl Chambers:

Base \(\Delta\) for \(R\) \(\mapsto\) Open fundamental Weyl Chamber associated to \(\Delta\) : \(C = \{ v\in E; \langle \alpha , v \rangle > 0 \forall \alpha \in \Delta \} \) 

And Open Weyl Chamber \(C\) \(\mapsto\)  positive roots \(R_C^+ = \{ \alpha \in R : \langle \alpha ,v \rangle > 0 \forall v\in C \} \) and we get \(\Delta_C\) the base from these positive roots.

We also had the proposition above.

We proved the fact that \(W^{\prime} \subset W\), the subgroup generated by \(s_\alpha, \alpha \in \Delta\) acts transitively.

It remains to show that \(W^{\prime} = W\).

\begin{proof}
    
    \(\#|W| = \#\{ \text{open weyl chamber}  \} = \#\{ \text{all possible bases } \Delta \}  \) 

    Let \(s_\beta \) be reflection associated to the \underline{arbitrary} root \(\beta\in R\).

    We want to show that \(s_\beta \in W^{\prime}\) 

    A result we didnt prove but is in the book because it's too technical: \(\beta\) belongs to some base \(\Delta_D\) which is associated to open weyl chamber \(D\).

    Choose \(w\in W^{\prime}\) so that \(w\cdot D = C\)

    Where \(C\) is the fundamental Weyl Chamber associated to \(\Delta\).

    Thus, \(w\cdot \beta \in \Delta\) and \(s_{w\cdot \beta} = w\cdot s_\beta \cdot w^{-1}\) [conjugating by \(w\) is just a change of basis so it is still a reflection. Both sides are reflection and \(w\cdot \beta \mapsto -w \beta\)].

    Thus, \(s_\beta = w^{-1} \cdot s_{w\cdot \beta}\cdot w \in W^{\prime}\) 

    Thus, every generator of the Weyl Group belongs to \(W^{\prime}\) so \(W^{\prime} = W\).

\end{proof}

\underline{Lemma}: Let \(\Delta \) be a base for \(R\) and let \(C\) be the associated fundamental Weyl Chamber. Let \(w\in W, W\neq I, w = s_{\alpha_1}\cdot s_{\alpha_2}\cdots s_{\alpha _k}\) with \(\alpha_j \in \Delta\) be a minimal expressionfor \(w\). Then, \(C\) and \(w\cdot C\) lie on opposide sides of the hyperplane \(V_{\alpha_1}\) [the hyperplane perpendicular to \(\alpha_1\)]. Basically, only the last reflection applied matters.

The proof is on the book and is quite technical.

\begin{proposition}
    Let \(C\) be a Weyl chamber, and let \(H,H^{\prime} \in \overline{W} \) [closure of \(C\)]. If \(w\cdot H = H^{\prime} \) then \(H = H^{\prime} \) 
\end{proposition}

We will prove this using the preceeding lemma.

\underline{Explanation}: \(C\) is a open Weyl chamber, so the closure is the original open set union it's boundary. We will see soon that if \(H,H^{\prime} \in C\) then \(w\cdot H \in C \implies w\cdot C = C\) and thus \(w\) must be the identity, so \(H = H^{\prime}\). On the other hand, if \(H,H^{\prime} \in \partial C\) then this proposition tells us that \(w\cdot \partial C\cap \partial C \neq \varnothing\) which means \(w|_{w\cdot \partial C \cap \partial C} = I\)  

Now we do the formal proof.

\begin{proof}
    Let \(\Delta _C\) be the base corresponding to the Weyl Chamber \(C\). Write \(w = s_{\alpha_1}\cdot \cdots \cdot s_{\alpha_k}\) with \(\alpha_j \in \Delta _C\) - minimal expression for \(w\). Use induction on \(k\)
    
    If \(k = 0, w = I\)

    Suppose \(k > 0\).

    By the lemma, \(C\) and \(w\cdot C\) are on opposide side of \(V_\alpha\) 

    Thus, \((w\cdot \overline{C}) \cap \overline{C} \subset V_{\alpha_1}\) and \(w\cdot H = H^{\prime} \in V_{\alpha_1}\) 
    
    Apply \(s_{\alpha_1}\) to both sides.

    \(s_{\alpha_1}\cdot w \cdot H = s_\alpha\) and \(H^{\prime} = H\) 

    Thus, \(s_{\alpha_1}w = s_{\alpha_2}\cdot s_{\alpha_3}\cdots s_{\alpha_k}\)
    
    Also maps \(H\) to \(H^{\prime}\).

    By induction, \(H = H^{\prime}\) 

\end{proof}

Recall: \underline{Free action:}

If a group \(G\) acts on the set \(X\), then the action is free if for all \(x\in X\), \(g\cdot x = x \implies x= I\) 

The stabilizer of each \(x\in X\) is \(\{ I_1  \} \) 

\begin{proposition}
    The Weyl Group acts freely on a open set channel \(V\circ W\). If \(H\) belongs to an open weyl chamber \(C\) and \(w\cdot H \in C\) for some \(w\in W\) then \(w=I\).
\end{proposition}

\begin{proof}
    \(W\) maps open Weyl Chambers. \(w\cdot H\in C\) implies \(w\cdot C = C\) and the above proposition implies \(w =\) identity on all of \(C\). \(C\)-non-empty open set. Thus, \(W\) = identity as linear map on \(E\). Thus, \(w = I\) as element of \(W\). 
\end{proof}

\begin{proposition}
    Forany two bases \(\Delta _1, \Delta _2\) for root system \(R\), there exists unique \(w\in W\) such that \(w\cdot \Delta_1 = \Delta _2\) 
\end{proposition}

\begin{proof}
    First: this statement for open weyl chambers. The group action on weyl chambers is transitive and free. Thus, for any two weyl chambers there exists a unique member of \(W\) that maps one to the other. Since the weyl chambers are in bijection with bases and this bijection commutes with action of the weyl group, we have the result.
\end{proof}

\begin{proposition}
    Let \(C\) be a Weyl chamber and \(H\in E\). Let \(W\cdot H\) denote the \(W\)-orbit of \(H\). Then \(W\cdot H \cap \overline{C} = \{ \text{one point}  \} \) 
\end{proposition}

\underline{Clarification} There are two possibilities: generic \(H\) will be in \(E \setminus \bigcup_{\alpha \in R}^{} V_\alpha\), then \(w\in W\) such that \(wH\in \overline{C} \) is unique.

But, if \(H\in \bigcup_{\alpha \in R}^{} V_\alpha\) [possibly \(H = 0\)] then \(w\in W\) is \underline{not} unique, but rather the point of intersection of the \underline{orbit} of \(W\cdot H\) and \(\overline{C} \) is unique.

\begin{proof}
    \underline{Existence}:

    We want to show there exists \(w\in W\) so that \(w\cdot H\in \overline{C}\) 

    Weyl chambers are open connected components, so their union is the whole \(E\) except the boundaries, and thus union of their closure is the whole \(E\). So, \(H\in \overline{D} \) for some open Weyl chamber \(D\).

    Suppose there exsits \(w\in W\) so that \(w\cdot D = C\) 

    Then \(w\cdot \overline{D} = \overline{C}\)
    
    Thus \(w\cdot H \in \overline{C}\) 

    The uniqueness of the intersection follows from the proposition [8.25 in the hall, 1 from the beginning of this lecture]

\end{proof}

\begin{proposition}
    Suppose we have \(\Delta\)-base of \(R\) and positive roots \(R^+\). Let \(\alpha \in \Delta , \beta \in R^+, \beta \neq \alpha \). Then, \(s_\alpha \cdot \beta \in R^+\) 
\end{proposition}

\underline{Restatement}: Reflection \(s_\alpha\) maps \(\alpha \mapsto -\alpha\), and \(R^+ \setminus \{ \alpha  \} \mapsto R^+ \setminus \{ \alpha \} \) 

\begin{proof}
    Write \(\beta = \sum_{\gamma \in \Delta}^{} c_\gamma \gamma \), \(c_\gamma \in \mathbb{Z} , c_\gamma \geq 0, \beta \neq \alpha \implies\) some \(c_{\gamma_0}> 0\) for some \(\gamma_0 \neq \alpha \).

    By definition, \(s_\alpha \cdot \beta = \beta - \frac{\langle \alpha ,\beta  \rangle }{\langle \alpha ,\alpha  \rangle }\alpha = \beta - n \alpha\)

    Thus, \(s_\alpha \cdot \beta = \sum_{\gamma \in \Delta }^{} c_\gamma^{\prime} \gamma\) where \(c_\alpha ^{\prime} = c_\alpha - n\) and \(c_\gamma ^{\prime} \geq 0\) if \(\gamma \neq \alpha\) 

    Importantly, \(c_{\gamma_0}\) doesn't change, it's still positive. Thus, the root must be positive.

    Thus, \(s_\alpha \cdot \beta \in R^+\) 
\end{proof}

\hrulefill

Class 13: 02/20

\section*{Dynkin Diagrams (8.6)}

Start with a root system \((E,R)\) and fix a base \(\Delta \). A particular choice of base is not essential.

\underline{Recall:} If \(\alpha ,\beta \in \Delta , \alpha \neq \beta \) then the angle between \(\alpha ,\beta \) is \(\geq \frac{\pi}{2}\).

Possible angles: \(\frac{\pi}{2},\frac{2\pi}{3},\frac{3\pi}{4},\frac{5\pi}{8}\).

We associate \((E,R)\) to a graph called \underline{Dynkin Diagram} as follows:

Its vertices \(\{ v_1,\cdots,v_r \} \) correspond to elements of \(\Delta = \{ \alpha _1,\cdots,\alpha _r \} \)

And edges: if \(v_i, v_j\) are two distinct vertices \(i \neq j\), then there is:

\begin{itemize}
    \item no edge if \(\alpha_i\perp \alpha_j\)
    \item 1 edge if \(\angle(\alpha_i,\alpha_j)=\frac{2\pi}{3}\), no arrows since roots have equal length.
    \item 2 edges if \(\angle(\alpha_i,\alpha_j)=\frac{3\pi}{4}\), place an arrow (or \(>\)) from the vertex associated to the longer root to the vertex associated to the shorter root.
    \item 3 edges if \(\angle(\alpha_i,\alpha_j)=\frac{5\pi}{6}\), place an arrow (or \(>\)) from the vertex associated to the longer root to the vertex associated to the shorter root. 
\end{itemize}

Put examples of dynkin diagrams here.

In class: \(A_1\times A_1\) [disconnected], \(A_2, B_2\)

\begin{definition}
    Two Dynkin Diagrams are \underline{isomorphic} if there is a bijective map of the vertices of one dynkin diagram to the vertices of the other such that the map preserves the number of edges between vertices and the direction of the arrows.
\end{definition}

Let \(\Delta_1,\Delta_2\) be two different bases of the same root system. Then, they are related by an element of the Weyl Group. So \(w\cdot \Delta_1 = \Delta_2\).

Then \(w\) induces an isomorphism of respective dynkin diagrams. Since \(w\) is generated by reflections, it preserves angles and length.

\begin{proposition}
    The following statements are true:

    \begin{enumerate}
        \item A root system is irreducible \(\iff\) its Dynkin diagram is connected.
        \item A root system is determined (upto isomorphism) by its Dynkin diagram. If \(R_1,R_2\) are two root systems with isometric Dynkin diagrams, then \(R_1\cong R_2\).
    \end{enumerate}
\end{proposition}

\textbf{Sketch of Proof:}

1: Suppose \(R\) is reducible. This happens iff we have \(R=R_1 \sqcup R_2\), and \(E = E_1 \oplus E_2\) with \(E_1\perp E_2\) so that \(R_1 \subset E_1,R_2 \subset E_2\).

Then, \(\Delta = \Delta_1\sqcup \Delta _2\) so that they are bases of \(R_1\) and \(R_2\) respectively. Since \(\Delta_1\perp \Delta_2\). Note that this is equivalent to the fact that we have a disconnected Dynkin Diagram.

The other direction requires more work. We need to show that if \(\Delta =\Delta_1 \sqcup \Delta_2\) then \(R\) is either in \(E_1\) or \(E_2\).

Skipping some details, let \(W_1,W_2\) be the weyl groups generated by \(\Delta_1\) and \(\Delta_2\) then \(W = W_1 \times W_2\), since \(W_1\) leaves \(E_2\) unchanged and \(W_2\) leaves \(E_1\) unchanged. Every root in \(R\) belongs to some base, and \(w\cdot \Delta = w_1\cdot \Delta_1 \sqcup w_2\cdot \Delta_2\). Depending on which of these \(R\) is in, it is in either \(E_1\) or \(E_2\).

So this is equivalent to having a disconnected dynkin diagram.

Contrapositive gives us the proof.

2: By part 1 we can deal with each connected component seperately. So assume the root systems are irreducible and the Dynkin diagrams are connected.

Suppose we have \(\Delta_1 = \{ \alpha_1,\cdots,\alpha_r \} \) and \(\Delta_2 = \{ \beta_1,\cdots,\beta_r \} \), bases for \(R_1\) and \(R_2\) ordered so that the isomorphism of Dynkin diagram takes the vertex associated \(\alpha_k\) to the vertex associated to \(\beta_k\).

Rescale inner products on \(E_1,E_2\) so that \(\lVert \alpha _1 \rVert = \lVert \beta_1 \rVert \).

Define linear map \(A:E_1 \to E_2\) so that \(A(\alpha_j)=\beta_j\) for all \(1 \leq j \leq r\). This is well defined since \(\Delta_1\) is a vector space basis for \(E_1\) and \(\Delta_2\) is a vector space basis for \(E_2\).

We claim that it is an isometry.

Note that not only \(\lVert \alpha_1 \rVert = \lVert \beta _1 \rVert \) but also \(\lVert \alpha_j \rVert = \lVert \beta_j \rVert = \lVert A \alpha_j \rVert \). This is because the Dynkin diagram is connected, and since Dynkin diagram fixes the lengths. It also fixes the lengths, so \(\angle (\alpha_j,\alpha_k)=\angle (\beta_j,\beta_k)=\angle(A \alpha_j, A \alpha_k)\).

Since the lengths and angles are same, we can say that \(A\) preserves the inner product between the basis vectors.

Thus, \(A\) is an isometry.

Thus, it preserves reflections. So \(S_{\alpha_j}\leftrightarrow s_{\beta_j}\) means \(AS_{\alpha_j}=S_{A \alpha_j}, A = s_{\beta_j}A\) 

\(W_1\) is generated by \(s_{\alpha_j},\alpha _j\in \Delta_1\) 

\(W_2\) is generated by \(s_{\beta_j},\beta_j\in \Delta_2\) 

So \(W_1\cong W_2\) 

So \(AS_{\alpha}=s_{A \alpha}A\) for all \(\alpha \in R\).

\underline{Corollary}: A compslex semisimple lie algebra \(\mathfrak{g}\) is simple \(\iff\) its root system is irreducible \(\iff\) the associated Dynkin diagram is connected.

\section*{Irreducible Root System [8.10]}

Pictures.

\(A_n\) is realized by \(\mathfrak{sl}(n+1,\mathbb{C})\) 

\(B_n\) is realized by \(\mathfrak{so}(2n+1,\mathbb{C}),n\geq 2\)

\(C_n\) is realized by \(\mathfrak{sp}(n,\mathbb{C}),n\geq 3 \) 

\(D_n\) is realized by \(\mathfrak{so}(2n,\mathbb{C}),n\geq 4 \) 

These are the Classical Root Systems.

\underline{Corollary}: These lie algebras are simple:

\(\mathfrak{sl}(n+1,\mathbb{C}),n\geq 1; \mathfrak{so}(2n+1,\mathbb{C}),n\geq 1; \mathfrak{sp}(n,\mathbb{C}),n\geq 1; \mathfrak{so}(2n,\mathbb{C}),n\geq 3    \) 

Mismatch between indexing is because for some small values it is just duplicate of previous.

In Helgason's book there is a detailed treatment of isomorphism of lie algebra.

Insert pictures of \(E_6, E_7, E_8, F_4, G_2\)

\begin{theorem}

    1. The graphs \(E_6, E_7, E_8, F_4, G_2\) can be realized as Dynkin diagrams of some root system.

    2. Every irreducible root system is isomorphic to exactly  one of the above root system.

\end{theorem}

Main theorem of this chapter:

\begin{theorem}
    1. If \(\mathfrak{g} \) is a complex semisimple lie algebra, and we have two cartan subalgebras \(\mathfrak{h}_1,\mathfrak{h}_2\) of lie algebra \(\mathfrak{g}\) then there exists an automorphism \(\phi: \mathfrak{g}\to \mathfrak{g} \) so that \(\phi(\mathfrak{h}_1 ) = \mathfrak{h}_2 \)
    
    2. If \(\mathfrak{g}_1,\mathfrak{g}_2\) are complex semisimple lie algebras with isometric root system, \(\mathfrak{g}_1\cong \mathfrak{g}_2 \).
    
    3. Each root system can be realized by some complex semisimple lie algebra.

    4. Every simple complex lie algebra is isomorphic to one algebra from the list.
\end{theorem}

\hrulefill

Class 14: 02/22

\section*{Integral and Dominant Integral Elements (8.7)}

Recall that, associated to each root \(\alpha \in R\) we have a coroot \(H_\alpha = \frac{2\alpha}{\langle \alpha ,\alpha \rangle }\in E\).

\begin{definition}
    An element \(\mu\in E\) is an \underline{integral element} if, \(\forall \alpha \in R\), the quantity \(\langle \mu , H_\alpha \rangle = 2 \frac{\langle \mu ,\alpha  \rangle }{\langle \alpha ,\alpha  \rangle }\in \mathbb{Z} \) 
\end{definition}

\begin{definition}
    Given a base \(\Delta\) of \(R\), an element \(\mu\in E\) is a \underline{dominant element} [relative to \(\Delta\)] if \(\langle \alpha ,\mu  \rangle \geq 0\) for all \(\alpha \in \Delta \). It is called \underline{strictly dominant} if \(\langle \alpha ,\mu  \rangle > 0\) for all \(\alpha \in \Delta \) 
\end{definition}

[Alternative definition: \(\mu \in E\) is strictly dominant relative to \(\Delta\)  if and only if \(\mu\in\) open fundamental weyl chamber \(C\) associated to \(\Delta\). \(\mu\) is dominant if and only if \(\mu\in\) closure of open fundamental weyl chamber \(C\) associated to \(\Delta \) ]

Example: Roots are obviously integral elements. Integer linear combination of roots are also integral elements. However, there are more.

Concrete example: Consider the \(A_2\) root system [the hexagon]. Pick a base \(\alpha_1, \alpha_2\). Define \(\mu_1 \coloneqq \frac{2}{3}\alpha_1 + \frac{1}{3}\alpha_2, \mu_2 \coloneqq \frac{1}{3}\alpha_1 + \frac{2}{3}\alpha_2\).

These are not integer coefficients. But they are integral elements.

\(\langle \mu_1, H_{\alpha_1} \rangle = 1, \langle \mu_1, H_{\alpha_2} \rangle = 0\).

\(\langle \mu_2, H_{\alpha_1} \rangle = 0, \langle \mu_2, H_{\alpha_2} \rangle = 1\) 

\begin{proposition}
    If we have element \(\mu\in E\) that has the property that \(2 \frac{\langle \mu , \alpha  \rangle }{\langle \alpha ,\alpha  \rangle }\in \mathbb{Z}\) for all \(\alpha \in \Delta\) then the same holds for \(\alpha \in R\), and thu \(\mu\) is an integral element.
\end{proposition}

\begin{proof}
    Recall that coroots \(\{ H_\alpha , \alpha \in R \} \) form the dual root system \(R^\vee\) and \(\{ H_\alpha , \alpha \in \Delta  \} \) form a base for \(R^\vee\). Thus, each \(H_\beta ,\beta \in R\), can be expressed as a \(\mathbb{Z}\)-linear combination of \(H_\alpha\) where \(\alpha \in \Delta\). The result follows.
\end{proof}

\begin{definition}
    Let \(\Delta = \{ \alpha _1, \dots ,\alpha _r \} \) be a base. Then the \underline{fundamental weights} [relative to \(\Delta\) ] are the elements \(\mu_1,\cdots,\mu_r\in E\) such that \(2\frac{\langle \mu_j,\alpha_k \rangle }{\langle \alpha_k ,\alpha_k  \rangle } = \delta_{jk} \) for \(j,k=1,\dots ,r\). 
\end{definition}

Note that \(\langle H_{\alpha_j},\mu_k \rangle = \delta_{jk} \) so \(\{ \mu_1 , \cdots, \mu_r \} \) is the dual basis to \(H_{\alpha_1}, \cdots, H_{\alpha_r}\).

Then, the set of dominant integral elements \(\mu\) is equal to the set of linear combinations \(\sum_{j=1}^{r} n_j \mu_j\) with \(n_j\in \mathbb{Z} \) and \(n_j \geq 0\). Each coefficient \(n_j = 2 \frac{\langle \mu, \alpha_j \rangle }{\langle \alpha_j, \alpha_j \rangle }\) 

Dominant integral elements \(\mu\) occur as highest weights of representations.

\begin{definition}
    Let \(\Delta\) be a base for \(R, R^+\) the corresponding set of positive roots and \(\delta\) = half \underline{the sum of positive roots} = \(\frac{1}{2} \sum_{\alpha \in R^+ } \alpha\) 
\end{definition}

This appears in many formulas in representation theory.

\begin{proposition}
    \(\delta\) is a strictly dominant element.

    Also, \(2\frac{\langle \beta ,\delta \rangle }{\langle \beta ,\beta  \rangle } = 1\) for all \(\beta \in \Delta\)
\end{proposition}

\begin{proof}
    It is sufficient to show that \(2 \frac{\langle \beta ,\delta  \rangle }{\langle \beta ,\beta  \rangle} = 1\) 

    If \(\beta \in \Delta\), then by proposition 8.30, \(s_\beta\) permutes \(R^+ \setminus \{ \beta \} \).

    Decompose \(R^+ \setminus B = E_1 \sqcup E_2\) so that \(E_1\) is fixed under \(s_\beta\) [\{\(\gamma \in R^+ : \gamma \perp \beta \}\)] and \(E_2\) gets changed under \(s_\beta \) [\(\{ \gamma \in R^+ \setminus \beta : \gamma \not\perp \beta \} \) ] 

    The elements of \(E_1\) contribute \(0\) to \(2\frac{\langle \beta ,\delta \rangle }{\langle \beta ,\beta  \rangle }\) 

    The elements of \(E_2\) can be split up into pairs \(\{ \alpha ,s_\beta \alpha \} \) and they cancel each other out since \(\langle s_\beta\cdot \alpha , \beta  \rangle = \langle \alpha , s_\beta ,\beta \rangle = - \langle \alpha ,\beta  \rangle \) 

    So we conclude that \(\sum_{\alpha \in E_2}^{} \frac{2}{2}\frac{\langle \beta ,\alpha  \rangle }{\langle \beta ,\beta  \rangle } = 0\) 

    \(2 \frac{\langle \beta ,\delta  \rangle }{\langle \beta ,\beta  \rangle } = 2\frac{\langle \beta ,\beta  \rangle }{\langle \beta ,\beta  \rangle } = 1\) 

\end{proof}

Example:

In \(A_1\), \(\delta = \frac{1}{2}A\) 

In \(A_2\), [hexagon case] \(\delta = \frac{1}{2}(\alpha + \beta + (\alpha + \beta )) =  \alpha_1 + \alpha_2\) 

In \(B_2\), [square case] \(\delta =\frac{1}{2}(\alpha _1 + \alpha_2 + (\alpha_1 + \alpha_2) + 2\alpha_1 + \alpha_2)\) so \(\delta  = 2\alpha _1 + \frac{3}{2}\alpha_2\) 

\section*{The Partial Ordering [8.8]}

Fix a base \(\Delta = \{ \alpha _1, \cdots, \alpha \} \) for a root system \(R\).

\begin{definition}
    Let \(\lambda,\mu_\in E\), we say \underline{\(\mu\) is higher than \(\lambda\)}, write \(\mu \succeq \lambda, \lambda \preceq\mu\) if \(\mu - \lambda = c_1 \alpha_1 + \dots + c_r \alpha_r\) with each \(c_j\in \mathbb{R} , c_j \geq 0\).  
\end{definition}

Equivalently we can define \underline{\(\lambda\) is lower than \(\mu\) }

This gives us a \underline{partial ordering} since all elements \(\lambda,\mu\) cannot be compared with each other.

\begin{proposition}
    If \(\mu\) is dominant them \(\mu \succeq 0\) 
\end{proposition}

\underline{Caution}: The converse is not true. Think of the \(A_2\) root system [hexagon]. Draw perpendicularsto \(\alpha_1, \alpha_2\) and you get a region that contians all the dominant elements. Stuff between \(\alpha_1, \alpha_2\) are \(\succeq 0\) but there are stuff outside the set of dominant elements.

For the proof of this statement, we need a lemma.

\underline{Lemma}: Let \(v_1,\cdots, v_r\) be basis of \(E\). Then we have the dual vector space \(E^{\ast}\) of linear functionals \(E \to \mathbb{R} \) [or \(\mathbb{C}\)]

We have a dual basis for \(E^{\ast}\): \(\{ \xi_1, \cdots, \xi_r \} \subset E^{\ast} \)  so that \(\xi_j(v_k)=\delta_{jk}\) 

In our case, \(E\) has an inner product so we have an isomorphism \(E \cong E^{\ast}\).

Then the \underline{dual basis} for \(E\) is a set \(\{v_1^{\ast} , \cdots, v_r ^{\ast}\}\) so that \(\langle v_j^{\ast} ,v_k \rangle = \delta_{jk}\) 

\underline{Lemma}: If \(\{ v_1, \cdots, v_r \} \) is an obtuse basis for \(E\) [meaning \(\langle v_j, v_k \rangle \leq 0\) for all \(1\leq j,k\leq r, j \neq k\) ] then the dual basis \(\{ v_1^{\ast} , \cdots , v_r^{\ast}  \} \) is acute [\(\langle v_j^{\ast} , v_k^{\ast}  \rangle \geq 0\)]

Proof: Basic linear algebra, in textbook. Check \(r = 2\) directly and then use induction on \(r\).

For each \(m\), \(1 \leq m, \leq r\) define \(E^{\prime} = span(v_1^{\ast} , \cdots, \not{v_m^{\ast}}, \cdots,v_r ^{\ast})\) and apply induction on the basis \(Pv_1, \cdots , \not{P v_m},\cdots, Pv_r\) where \(P:E \to E^{\prime}\) is the orthogonal projection.

Nowwe prove the proposition.

\begin{proof}
    Any \(u\in E\) can be expressed as \(u = \sum_{j=1}^{r} c_j \alpha_j\) with \(c_j = \langle \alpha_j ^{\ast}, u \rangle \).

    Take \(\{ \alpha_1 ^{\ast} , \cdots, \alpha_r ^{\ast} \} \) the dual basis. When \(u = \alpha_j ^{\ast}\) we get:

    \[
        \alpha_j ^{\ast} = \sum_{k=1}^{r} \langle \alpha_k ^{\ast} , \alpha_j ^{\ast}  \rangle \alpha_k
    \]

    If \(\mu\) is dominant, \(\mu = \sum_{j=1}^{r} c_j \alpha _j\). Thereforem

    \[
        c_j = \langle \alpha_j ^{\ast},\mu \rangle = \sum_{k=1}^{r} \langle \alpha_k ^{\ast} , \alpha_j ^{\ast}  \rangle \langle \alpha_k, \mu  \rangle 
    \]

    \(\langle \alpha_k ^{\ast} , \alpha_j ^{\ast}  \rangle \geq 0\) because they form an acute basis, \(\langle \alpha_k, \mu \rangle \geq 0\) because \(\mu\) is dominant.

\end{proof}

\hrulefill

Class 15: 02/27

Today we finish the discussion on partial ordering, section 8.8. Once we're done with that, we will move on to the next chapter.

\begin{proposition}
    If \(\mu\) is dominant, then \(w \cdot \mu \preceq \mu\) for all \(w\in W\).
\end{proposition}

\begin{proof}
    Let \(O = W\cdot\mu\) be the weyl group orbit of \(\mu \). Since weyl group is finite, this is finite.

    Let \(\lambda\in O\) be any maximal elment. So, there does not exist another element \(\lambda ^{\prime} \) in the orbit such that \(\lambda ^{\prime} \succeq \lambda\) where \(\lambda ^{\prime} \neq \lambda\).

    Claim: \(\lambda\) must be dominant, that is to say, \(\langle \alpha,\lambda \rangle \geq 0\) for all \(\alpha \in \Delta\).

    Proof: Suppose otherwise. If some \(\langle \alpha ,\lambda \rangle < 0\), we can apply reflection with respect to \(\alpha\) and still be in \(O\).

    \(s_\alpha \cdot \lambda = \lambda - 2 \frac{\langle \lambda ,\alpha  \rangle }{\langle \alpha ,\alpha  \rangle }\alpha \) 

    Then \(s_\alpha \cdot \lambda \succeq \lambda, s_\alpha \cdot \lambda \neq \lambda\). This contradict the maximality of \(\lambda\).

    Thus, every maximal element in \(O\) is dominant.

    By proposition 8.29, \(\mu\) is the unique dominant element in \(O\). Thus, we must have \(\lambda = \mu\).

    It remains to prove that every \(\lambda^{\prime} \in O\) is lower than \(\mu\).

    For each \(\lambda^{\prime}\) there exists a \(\lambda \in O\) such that \(\lambda ^{\prime} \preceq \lambda\) and \(\lambda\) is maximal in \(O\).  So we must have \(\lambda ^{\prime} \preceq \mu \) 

\end{proof}

\begin{proposition}
    \(\delta = \frac{1}{2} \sum_{\alpha \in R^+} \alpha \) is the minimal strictly dominant integral elment. This means, if \(\mu\) is strictly dominant integral element, then \(\delta \preceq \mu\)
\end{proposition}

\begin{proof}
    If \(\mu\) is integral and strictly dominant, then \(\langle \mu, H_\alpha \rangle \geq 1\). \(\delta\) has the property: \(\langle \delta , H_\alpha \rangle = 1\) for all \(\alpha\in R\).
    
    Therefore, \(\mu - \delta\) is integral and dominant, thus \(\mu - \delta \succeq 0\).

\end{proof}

\underline{Lemma}:

Suppose \(K\) is a compact convex subset of \(E\) and \(\lambda\) is an element of \(E\) and \(\lambda \notin K\). Then there exists an element \(\gamma \in E\) such that \(\langle \gamma , \lambda \rangle \geq \langle \gamma , \eta \rangle \) for \(\eta \in K\).

\underline{Sketch of Proof:}

This essentially says that \(\gamma\) and \(K\) can be seperated by hyperplanes.

Let \(\eta_0\in K\) be the element with minimal distance to \(\lambda\). This exists due to compactness. Let \(\gamma = \lambda - n_0\).

Idea is, if we draw a hyperplane perpendicular to the line between \(\gamma \) and \(\eta_0\) at \(\eta_0\) then the hyperplane seperates \(\gamma\) and \(K\). This hyperplane is characterized by a normal vector, which gives us the result.

If \(\mu \in E\) let \(Conv(W\cdot\mu)\) be the convex hull of the weyl group orbit of \(\mu\).

\underline{Lemma}: If \(\lambda ,\mu \in E\) are dominant and \(\lambda \notin Conv(W\cdot \mu)\), then there exists a \underline{dominant} element \(\gamma\in E\) so that \(\langle \gamma ,\lambda  \rangle > \langle \gamma , w\cdot \mu \rangle \) for all \(w\in W\).

\underline{Sketch of Proof:}

This is similar to the previous lemma with the convex set \(K\) replaced with \(Conv(W\cdot \mu)\). We also require \(\gamma\) to be dominant.

By the previous lemma, there exists some \(\gamma ^{\prime} \) such that it satisfies all the properties, except it might not be dominant.

In particlar, \(\langle \gamma ^{\prime} , \lambda \rangle > \langle \gamma^{\prime} , w\cdot \lambda  \rangle \) for all \(w\in W\).

Our solution is to replace \(\gamma^{\prime} \) with a relevant \(w_0 \gamma ^{\prime} \) where \(w_0\in W\) so that \(w_0\cdot \gamma ^{\prime} \) is dominant, and then argue that \(\langle \gamma ,\lambda  \rangle \geq \langle \gamma ^{\prime} , \lambda \rangle \) and the set of values \(\{\langle \gamma ,w\cdot \mu \rangle | w\in W\}\) and \(\{ \langle \gamma ^{\prime} , w\cdot \mu \rangle | w\in W \} \) are the same. 

Thus, \(\gamma ^{\prime} \preceq \gamma\) and \(\lambda\) makes any non-neg inner product with each simple/base root.

\begin{proposition}
    \begin{enumerate}
        \item If \(\lambda ,\mu \in E\) are dominant, then \(\lambda \in Conv(W\cdot \mu) \iff \lambda \preceq \mu\) 
        \item If \(\lambda ,\mu \in E\) and \(\mu\) is dominant, then \(\lambda \in Conv(W\cdot \mu)\) if and only if \(w\cdot \lambda \preceq \mu\) for all \(w\in W\). 
    \end{enumerate}
\end{proposition}

\begin{proof}

    Regarding 1, recall that \(\lambda \in Conv(W\cdot \mu)\) if and only if \(w\cdot \lambda \in Conv(W\cdot \mu)\) for all \(\mu \in W\), if and only 

    1: is equivalent to: \(Conv(W\cdot \lambda ) \subset Conv(W\cdot \mu)\) 

    \(1^{\prime} \) : If \(\lambda ,\mu \in E\) are dominant, then \(Conv(W\cdot \mu ) \subset Conv (W\cdot \mu )\) if and only if \(\lambda \preceq \mu \)  

    1: Suppose \(\lambda \in Conv(W\cdot \mu)\). Introduce the set \(\Lambda = \{ n\in E : \eta \preceq \mu \} \). Then \(\Lambda \) is convex, contains each \(w\cdot \mu , w\in W\).

    Hence, \(\lambda \in Conv(W\cdot \mu) \subset \Lambda\). Tis proves that \(\lambda \preceq \mu\) 
    
    Conversely, assume \(\lambda \preceq \mu\) and suppose \(\lambda\notin Conv(W\cdot \mu)\). By the lemma, there must exist a dominant \(\gamma \in E\) such that \(\langle \gamma ,\lambda  \rangle > \langle \gamma , w\cdot \mu \rangle \) for all \(w\in W\).

    Subtracting, \(\langle \gamma , \mu - \lambda \rangle < 0\). On the other hand, \(\lambda \preceq \mu\), so each coefficient of \(\mu - \lambda\) is non-negative.

    Thus, \(\mu - \lambda = \sum_{\alpha \in \Delta }^{} c_\alpha \alpha \) for \(c_\alpha \geq 0\). Since \(\gamma\) is dominant, we have \(\langle \gamma , \mu - \lambda  \rangle = \sum_{\alpha \in \Delta }^{} c_\alpha \langle \gamma , \alpha  \rangle \geq 0\)

    For part 2: Now \(\lambda\) is not necessarily dominant. Suppose first that \(w\cdot \lambda \preceq \mu \) for all \(w \in W\), and choose \(w_0\) so that \(w_0 \lambda \) is dominant. Then part 1 applied to \(w_0 \lambda \preceq \mu\) tells us \(w_0 \lambda \in Conv(W\cdot \mu)\). Thus, \(\lambda \in Conv(W\cdot \mu)\) 
    
    Conversely, suppose \(\lambda \in Conv(W\cdot \mu )\). By a previous proposition, each element of \(Conv(W\cdot \mu)\) is lower than \(\mu\). In particular, \(w\cdot \lambda \preceq \mu \) for all \(w\in W\).

\end{proof}

Now we move on to chapter 9.

Main result of chapter 9:

\begin{proposition}
    Let \(\mathfrak{g}\) be a complex semisimple lie algebra. The irreducible finite dimensional repesentations of \(\mathfrak{g}\) are in bijection with dominant integral elements (highest weights).
\end{proposition}

Hard part: given a particlar highest weight, we need to show there is a irreducible representation.

We need: Universal enveloping algebras.

\section*{Universal Enveloping Algebras (9.3)}

Let \(\mathbb{K}\) be a field, \(\mathfrak{g} \) be a lie algebra over \(\mathbb{K}\), not necessarily semisimple.

Goal: to embed \(\mathfrak{g}\) into a (large) assoicative algebra \(\mathcal{U}\) so that \([X,Y] = XY - YX\) for all \(X,Y\in \mathfrak{g}\) in the algebra \(\mathcal{U}\).

We will deal with finite dimensional lie algebra, but \(\mathcal{U}\) will still be infinite dimensional.

First we form tensor algebra \(\bigotimes \mathfrak{g}= \bigoplus_{k=0}^{\infty} \otimes^k \mathfrak{g}\) where \(\otimes ^0 \mathfrak{g} = \mathbb{K} \).

Then \(\bigotimes \mathfrak{g} \) is associative non-commutative algebra over \(\mathbb{K}\) with unit, generated by \(\mathfrak{g} \) . It is universal in the sense that any other one is a quotient of \(\bigotimes \mathfrak{g}\).

Similarly, we can construct the symmetric algebra:

\(\mathcal{S} (\mathfrak{g} ) = \bigoplus_{k=0}^{\infty} \mathcal{S} ^k(\mathfrak{g} )\), where \(S^0(\mathfrak{g} ) = \mathbb{K} \) and \(\mathcal{S} ^k (\mathfrak{g} )\) = \(k\) th symmetric power.

\(\mathcal{S} (\mathfrak{g})\) is the algebra of polynomials over \(g^{\ast}\).

\(\mathcal{S} ^k(\mathfrak{g})\) = homogeneous polynomials of degree \(k\) on \(g^{\ast}\) 

\(\mathcal{S} (g)\) is the associative commutative algebra over \(\mathbb{K}\) with unit. It contains \(\mathfrak{g} \) as a linear subspace and is generated by it. So,it is universal in the sense that any other symmetric algebra must be a quotient of it.

By universal property, \(\mathcal{S}(\mathfrak{g} ) = \bigotimes \mathfrak{g} / I  \) for some ideal \(I\). 

\hrulefill

Class 16: 02/29

Plan for today is to construct the universal enveloping algebra \(\mathcal{U} (\mathfrak{g} )\). Let \(\mathbb{K}\) be a field, and let \(\mathfrak{g} \) be a lie algebra over \(\mathbb{K} \), not necessarily semisimple.

Our goal is to embed \(\mathfrak{g} \) into an associative algeba \(\mathcal{U}\) so that \([X,Y]=XY - YX\) for all \(X,Y\in \mathfrak{g} \) 

We constructed \(\bigotimes \mathfrak{g}\) and \(\mathcal{S}(\mathfrak{g})\).

It is universal in the sense that:

\[
    \begin{tikzcd}
        \mathfrak{g} \ar[rd,hook,swap] \ar[rr,"\text{linear}"] & & \mathcal{A}\\
        & \bigotimes \mathfrak{g} \ar[ur,dotted, "\exists !"] 
    \end{tikzcd}
\]

Also, the symmetric one is universal in the sense that

\[
    \begin{tikzcd}
        \mathfrak{g} \ar[rd,hook,swap] \ar[rr,"\text{linear}"] & & \mathcal{A}\text{, commutative}\\
        & \mathcal{S}(\mathfrak{g}) \ar[ur,dotted, "\exists !"] 
    \end{tikzcd}
\]

We have:

\[
    \mathcal{S}(\mathfrak{g}) = \bigotimes \mathfrak{g} / I 
\]

where \(I\) is a two sided ideal generated by \(XY - YX\) where \(X,Y\in \mathfrak{g}\) 

When characeristic is \(0\) we can also realize \(\mathcal{S} (\mathfrak{g} )\) as a subalgeba of \(\otimes \mathfrak{g}\):

\[
    \sum_{\sigma \in S_n}^{} \frac{1}{n!} X_{\sigma(1)}\cdots X_{\sigma(n)} \leftrightarrow X_1\cdots X_n
\]

We need char 0 since we're dividing by \(n!\).

Both \(\bigotimes \mathfrak{g}\) and \(\mathcal{S} (\mathfrak{g} )\) are \(\mathbb{N}\) graded algebras. Meaning they are direct sum of components of grade in \(\mathbb{N}\) and if we mulitply components of grade \(m\) and compontents of grade \(n\) we get a component of degree \(m+n\) 

\begin{definition}
    \underline{The universal enveloping algebra} of \(\mathfrak{g} \) is \(\mathcal{U}(\mathfrak{g}) = \bigotimes \mathfrak{g} / J \) where \(J\) is the two sided ideal generated by \(\{ XY - YX - [X,Y] ; X,Y\in \mathfrak{g} \} \)  
\end{definition}

Then \(\mathcal{U} (\mathfrak{g} )\) is an associate algebra over field \(\mathbb{K}\) equipped with a \(\mathbb{K} \) linear map \(i:\mathfrak{g} \to \mathcal{U} (\mathfrak{g})\).

The map is given by the image of \(\otimes^1 \mathfrak{g} = \mathfrak{g} \) in \(U(\mathfrak{g})\), the first graded element.

\begin{proposition}
    We have the following properties:

    \begin{enumerate}
        \item \(i\) is a lie algebra homomorpism where \(\mathcal{U} (\mathfrak{g})\) is a lie algebra with \([a,b]=ab - ba\) 
        \item \(\mathcal{U} (\mathfrak{g} )\) is generated by \(i(\mathfrak{g})\) as a \(\mathbb{K} \) albera
    \end{enumerate}
\end{proposition}

Will see: \(i:\mathfrak{g} \to \mathcal{U} (\mathfrak{g} ) \) is an injection, that is, it is 1-1. The easiest way to see this is to use the PBW theorem.

\underline{Caution}: if \(X = \begin{pmatrix}
    0 &  1 \\
    0 &  0 \\
\end{pmatrix}\) in \(\mathfrak{sl} (2,\mathbb{C})\) then \(X^2 = 0\) as a 2 by 2 matrix but \(X^2 \neq 0\) as an element of \(\mathcal{U} (\mathfrak{sl} (2,\mathbb{C}))\). It will be obvious when we see the PBW theorem.

\(\mathcal{U} (\mathfrak{g} )\) is universal in the following sense: let \(\mathcal{A}\) be an associative lie algebra over \(\mathbb{K} \) with unit and let \(j:\mathfrak{g} \to \mathcal{A} \) be a linear map such that \(j(X)j(Y)-j(Y)j(X)=j([X,Y])\) for all \(X,Y\in \mathfrak{g} \). Then, \(j\) can be uniquely extended.

\[
    \begin{tikzcd}
        \mathfrak{g} \ar[rd,"i",swap] \ar[rr,"j"] & & \mathcal{A}\\
        & \mathcal{U}(\mathfrak{g}) \ar[ur,dotted, "\exists !"] 
    \end{tikzcd}
\]

\underline{Corollary}: Any representation of \(\mathfrak{g}\) (not necessarily finite dimensional) has a canonical structure of a \(\mathcal{U}(\mathfrak{g})\)-module.

Conversely, every \(\mathcal{U}(\mathfrak{g})\) module has a canonical structure of a representation of \(\mathfrak{g}\) 

\underline{Reason}: Apply the universality property to \(\pi: \mathfrak{g} \to End(V) \) 

\underline{Restatement}: The categories of representations of \(\mathfrak{g} \) and \(\mathcal{U} (\mathfrak{g} )\) modules are equivalent. Recall that this means morphisms are preserved. In representations of \(\mathfrak{g} \) morphisms are intertwining maps, and morphisms \(\mathcal{U} (\mathfrak{g} )\)-modules are simply module homomorphisms.

\underline{Example}:

\begin{definition}
    \underline{Casimir element} in \(\mathcal{U} (\mathfrak{sl} (2,\mathbb{C} ))\):

    \(\Omega = XY + YX -\frac{1}{2} H^2 \in \mathcal{U} (\mathfrak{sl} (2,\mathbb{C}))\) 

    Where \(X,Y,H\) are the classical generators of \(\mathfrak{sl} (2,\mathbb{C} )\) . ie \(X = \begin{pmatrix}
        0 &  1 \\
        0 &  0 \\
    \end{pmatrix}, Y = \begin{pmatrix}
        0 &  0 \\
        1 &  0 \\
    \end{pmatrix}, H = \begin{pmatrix}
        1 &  0 \\
        0 &  -1 \\
    \end{pmatrix}\) 

\end{definition}

\begin{proposition}
    \(\Omega\) is central in \(\mathcal{U} (\mathfrak{sl} (2,\mathbb{C} 0))\). Basically, for all \(\mathbb{Z}\in \mathcal{U} (\mathfrak{sl}(2,\mathbb{C} ) )\) we have \(\Omega Z = Z \Omega\)
\end{proposition}

\begin{proof}
    Part of homework. But we only need to check it for generators, since \(\mathcal{U} (\mathfrak{sl}(2,\mathbb{C}) )\) is generated by them. So we need t 
\end{proof}o check \(\Omega X = X \Omega , \Omega Y = Y \Omega , \Omega H = H \Omega\) 

Note:

\[
    Y \Omega = YXY + Y^2 X + \frac{1}{2} YH^2
\]

\[
    \Omega Y = XY^2 + YXY + \frac{1}{2} H^2 Y
\]

So, we have

\(YXY + Y^2 X + \frac{1}{2} YH^2\) 

\(= YXY + Y(XY - H) + \frac{1}{2} (2Y+HY)H\)

\(=2YXY + \frac{1}{2} HYH\)

\(=2YXY + \frac{1}{2} H(2Y+HY)\)

\(= YXY - (YX+H)Y + \frac{1}{2} H^2 Y\)

\(= YXY + XY^2 + \frac{1}{2}H^2 Y\) 

So it is indeed in the center.

\begin{proposition}
    \[
        Z \mathcal{U} (\mathfrak{sl} (2,\mathbb{C})) = \mathcal{C} (\Omega)
    \]

\end{proposition}

\begin{proposition}
    Let \((\pi ,V)\) be a representation of \(\mathfrak{sl}(2,\mathbb{C}) \). Then \(\pi(\Omega):V\to V\) commutes with the action of \(\mathfrak{sl}(2,\mathbb{C}) \)  
\end{proposition}

This gives us an intertwining operator. Thus, if \((\pi,V)\) is an irreducible decomposition, we can use Schur's lemma to deduce that \(\pi(\Omega)\) is multiplication by a scalar. If \((\pi ,V)\) is not irreducible, the eigenspaces of \(\pi(\Omega)\) are subrepresentations of \((\pi,V)\) and can be used for decomposition of \((\pi,V)\) into irreducible components.

If \((\pi_1,V_1)\) and \((\pi_2,V_2)\) are two irreducible representations and \(\pi_1(\Omega )\neq \pi_2(\Omega)\) then \(V_1\not\cong V_2\) 

So, casimir element can be used to deduce whether representation is irreducible or not: if it doesn't act by multiplication by scalar then it is not irreducible. We can also use it to decompose into irreducible components.

\begin{proposition}
    Casimir element for \(\Omega \in \mathcal{U} (\mathfrak{g} ) \) when \(\mathfrak{g} \) is semisimple exists. 
\end{proposition}

If \((\pi, v)\) is realized in a space of functions, then \(\pi(\Omega)\) often looks very much like a Laplacian or wave operator.

\underline{Recall}: adjoing representation of \(\mathfrak{g} \) on \(\mathfrak{g} \).

Then \(\mathfrak{g}\ni X \mapsto ad_X = [X,-]:\mathfrak{g} \to \mathfrak{g}   \) 

We can extend \(ad\) to a representation of \(\mathfrak{g} \) on \(\mathcal{U} (\mathfrak{g} )\): \(\mathfrak{g} \ni X \mapsto ad_X : \mathcal{U} (\mathfrak{g} ) \to \mathcal{U} (\mathfrak{g} ) \) where \(ad_X a=Xa-aX\) where \(a\in \mathcal{U} (\mathfrak{g} )\) 

\begin{proposition}
    1. The adjoint action of \(\mathfrak{g} \) on itself can be uniquely extended to an action of \(\mathfrak{g}\) on \(U(\mathfrak{g})\) which satisfies Leibniz rule: \(ad_X(a,b) = (ad_X a)b + a(ad_X b)\). Moreover, \(ad_X a = Xa - aX\).

    2. Let \(Z(\mathcal{U} (\mathfrak{g} ))\) be the center of the universal enveloping algebra. Then \(Z(\mathcal{U} (\mathfrak{g} )) = (\mathcal{U} (\mathfrak{g} )^{ad\mathfrak{g}})\) 

\end{proposition}

\begin{proof}
    Fact 2 follows from the fact that \(\mathfrak{g} \) generates \(\mathcal{U} (\mathfrak{g} )\) 

    1: Define \(ad_X a = Xa - aX\) where \(X\in \mathfrak{g} , a\in \mathcal{U} (\mathfrak{g} )\).

    First we check that it is a representation.

    We need: \(ad_{[X,Y]}a = ad_X ad_Y a - ad_Y ad_X a\) 

    \(ad_{[X,Y]}a = [X,Y]a - a[Y,X] = (XY - YX)a - a(XY - YX)\) 
    
    \(ad_X ad_Y a - ad_Y ad_X a = X(Ya - aY) - (Ya - aY)X - Y(Xa - aX) + X(Ya - aY)\) 

    After all the cancellation, we see that we are done.

    So we have a representation.

    Now we check that the Leibniz rule is satisfied.

    \(ad_X(ab) = Xab - abX\) 

    \((ad_X a)b + a(ad_X b) = (Xa - aX)b + a(Xb - bX) = Xab - abX\) 

    So we have leibniz rule

    Uniqueness of the extension follows from the leibniz rule.

\end{proof}

\section*{Poincar\'e-Birkhoff-Witt / PBW}

\(\mathcal{U} (\mathfrak{g} )\) is a filtered algebra.

\(K = \mathcal{U} _0(\mathfrak{g} ) \subset \mathcal{U} _1 (\mathfrak{g} ) \subset \dots \mathcal{U}_n (\mathfrak{g}) \subset \dots \subset \mathcal{U} (\mathfrak{g}) = \bigcup_{n=0}^{\infty} \mathcal{U} _n(\mathfrak{g} )\) 

Where \(\mathcal{U} (\mathfrak{g}) = \bigotimes \mathfrak{g} / J \), and \(\mathcal{U} _n(\mathfrak{g})= \) image of \(o\sum_{k=0}^n \otimes ^k \mathfrak{g} \) in \(\mathcal{U} (\mathfrak{g})\).

Note that \(\mathcal{U}_m(\mathfrak{g}) \cdot \mathcal{U}_n(\mathfrak{g}) \subset \mathcal{U} _{m + n}(\mathfrak{g})\) 

It is \underline{not} a graded algebra, since stuff can get messed up inside quotient.

\hrulefill

Class 17: 03/05

About homework: Problem 8b:

We needed to show that \(\langle \gamma , \alpha_i ^{\ast}  \rangle \geq 0\).

If \(\alpha_i ^{\ast} \) in \(C\) this statement would be trivial.

This is not true. Actually \(\alpha _i ^{\ast} \in \overline{C} \)

Consider \(\alpha_1 ^{\ast} + \epsilon (\alpha _2 ^{\ast} + \dots + \alpha _r ^{\ast})\)

This is in \(C\) for \(\epsilon > 0\) [inner product either \(1\) or \(\epsilon\)]

Letting \(\epsilon \to 0\) we see the result.

Back to Class:

\[
    \mathcal{U} (\mathfrak{g}) = \bigotimes \mathfrak{g} / J
\]

\(J\) is a two sided ideal generated by \(XY - YX - [X,Y]\) for \(X,Y\in \mathfrak{g}\) 

\(\mathcal{U} (\mathfrak{g} )\) is a filtered algeba. That is to say,

\[
    \mathbb{k} = \mathcal{U}_0(\mathfrak{g}) \subset \mathcal{U} _1 (\mathfrak{g} ) \subset \mathcal{U} _2(\mathfrak{g}) \subset \dots \subset \mathcal{U} _n (\mathfrak{g}) \subset \dots \mathcal{U} (\mathfrak{g}) = \bigcup_{n=0}^\infty \mathcal{U}_n(\mathfrak{g})
\]

We also have

\[
    \mathcal{U} _m(\mathfrak{g}) \mathcal{U}_n(\mathfrak{g}) \subset \mathcal{U}_{m + n}(\mathfrak{g})
\]

Here,

\[
    \mathcal{U}_n(\mathfrak{g}) = \bigoplus_{k=0}^n \otimes^k \mathfrak{g}
\]

in \(\mathcal{U} (\mathfrak{g})\) 

\underline{Caution} \(\mathcal{U} (\mathfrak{g})\) is \underline{not} a graded algebra.

This is because the generators \(XY - YX - [X,Y]\) are not homogeneous.

\begin{proposition}
    \begin{enumerate}
        \item If \(X\in \mathcal{U} _p(\mathfrak{g})\) and \(Y\in \mathcal{U} _  q(\mathfrak{g} )\) then \(XY - YX \in \mathcal{U}_{p+q-1}(\mathfrak{g})\)
        \item Let \(X_1,\cdots,X_n\) be an ordered basis for \(\mathfrak{g}\). Then monomials \((X_1)^{k_1}\cdots (X_n)^{k_n}\) where \(\sum_{i=1} ^n k_i \leq p\) span \(\mathcal{U} _p(\mathfrak{g})\). Note: we have fixed the order of basis elements.  
    \end{enumerate}

\end{proposition}

\begin{proof}
    1: By induction on \(p\). If \(p = 1\) and \(X\in i(\mathfrak{g})\) then,

    \[
        X Y_1 Y_2\cdots Y_q - Y_1 Y_2 \cdots Y_q X = X Y_1 Y_2\cdots Y_1 - Y_1 X Y_2\cdots Y_q + Y_1 X Y_2\cdots Y_1 -\cdots
    \]

    \[
        +Y_1\cdots Y_{q-1}X Y_q - Y_1\cdots Y_q X
    \]

    We can make a telescoping sum this way.

    \[
        = \sum_{i=1}^q Y_1\cdots [X,Y_i]\cdots Y_q
    \]

    Since \([X,Y_i]\) has degree one, this is actually in \(\mathcal{U}_q(\mathfrak{g})\) 
    
    Thus, if \(X\in \mathcal{U}_1(\mathfrak{g})\) and \(Y\in \mathcal{U} _q(\mathfrak{g})\) then \(XY-YX \in \mathcal{U}_q(\mathfrak{g})\)

    In other words, \(XY \equiv YX \pmod{\mathcal{U}_q(\mathfrak{g})}\) 

    Therefore,

    \[
        X_1 \cdots X_p X_{p+1} Y \equiv X_1\cdots X_p Y X_{p+1} \equiv \dots \equiv Y X_1 \cdots X_{p+1} \pmod{\mathcal{U}_q(\mathfrak{g})}
    \]

    Part 1 follows.

    2: We again induct on \(p\).

    \(\mathcal{U}_1(\mathfrak{g})\) is spanned by \(\{ 1, X_1,\cdots, X_n \} \). So we have base case.

    Note that \(\mathcal{U}_{p+1} = \mathcal{U}_1(\mathfrak{g}) \mathcal{U}_p(\mathfrak{g})\). By induction hypothesis,
    
    \(\mathcal{U}_p(\mathfrak{g})= span\{ (X_1)^{k_1} (X_2)^{k_2}\cdots (X_n)^{k_n} : \sum_{i} k_i \leq p \} \) 

    By part i, 

    \(X_i (X_1)^{k_1}\cdots (X_n)^{k_n} - (X_1)^{k_1}\cdots (X_i)^{k_{i+1}}\cdots (X_n)^{k_n}\) is in \(\mathcal{U}_p(\mathfrak{g})\) 

    Thus, \(X_i(X_1)^{k_1}\cdots (X_n)^{k_n}\in span\{ (X_1)^{l_1}\cdots (X_n)^{l_n}, \sum_{i} l_i \leq p+1 \} \) 

    This completes the proof.

\end{proof}

Informally, this means \(\mathcal{U}(\mathfrak{g} )\) is almost commutative.

\underline{Corollary}: The associated graded algebra \(Gr \mathcal{U} (\mathfrak{g}) = \bigoplus_{p=0}^{\infty} \mathcal{U}_p(\mathfrak{g}) / \mathcal{U}_{p-1}(\mathfrak{g})\) is commutative. This is because \(XY-YX\in U_{p-1}(\mathfrak{g})\) so quotient makes it \(0\).

We have \(\mathcal{S}(\mathfrak{g})\) the universal associated commutative algebra generated by \(\mathfrak{g}\). We thus get a unique map of algebras:

\[
    \mathcal{S}(\mathfrak{g}) \to Gr \mathcal{U}(g)
\]

Such that for all \(X\in \mathfrak{g}, \mathcal{S}(\mathfrak{g})\ni X \mapsto i(X) \in Gr \mathcal{U}(\mathfrak{g})\) 

Now we go to Poincare Birkhoff Witt formula.

\begin{theorem}
    [PBW] THe map \(\mathcal{S}(\mathfrak{g}) \to Gr \mathcal{U} (\mathfrak{g})\) is an isomorphism of algebras for any field \(\mathbb{K} \)  even of nonzero characteristic.
\end{theorem}

\underline{note}: Clearly, this map is onto. We essentially just proved that. The hard part is proving that it is actually one to one / injective. Equivalently:

\begin{theorem}
    The ordered monomials \((X_1)^{k_1}\cdots (X_n)^{k_n}; \sum_{i} k_i \leq p\) form a vector space basis of \(\mathcal{U}_p(\mathfrak{g})\). We have already shown span, the hard part is linear independence!
\end{theorem}

\begin{proof}
    In the book.
\end{proof}

\underline{Corollary}: The map \(i:\mathfrak{g} \to \mathcal{U} (\mathfrak{g}) \) is one to one. Thus, we can regard \(\mathfrak{g}\) as a vector subspace of \(\mathcal{U} (\mathfrak{g})\).  

\underline{Corollary}: If \(\mathfrak{h} \subset \mathfrak{g} \) subalgebra then \(h\hookrightarrow \mathfrak{g}\) induces \(\mathcal{U}(\mathfrak{h}) \hookrightarrow \mathcal{U} (\mathfrak{g})\). Noreover, \(\mathcal{U} (\mathfrak{g})\) is free as a left \(\mathcal{U} (\mathfrak{h})\) module.

\underline{Corollary}: If \(\mathfrak{h} _1, \mathfrak{h} _2 \subset \mathfrak{g}\) are lie subalgebra such that \(\mathfrak{g} = \mathfrak{h} _1 \oplus \mathfrak{h} _2\) as vector spaces. \(\mathfrak{h}_1\) and \(\mathfrak{h}_2\) need not commute.

Then the multiplication map \(\mathcal{U}(\mathfrak{h}_1) \otimes \mathcal{U}(\mathfrak{h}_2) \to \mathcal{U}(\mathfrak{g})\) is a vector space isomorphism.

\underline{Corollary}: \(\mathcal{U}(\mathfrak{g})\) has no zero divisors.

eg \(X = \begin{pmatrix}
    0 &  1 \\
    0 &  0 \\
\end{pmatrix}\in \mathfrak{sl} (2\mathbb{C})\) then \(X^n\neq 0\) in \(\mathcal{U} (\mathfrak{sl} (2,\mathbb{C}))\) 

\subsection*{Weights of Representations (9.1)}

\underline{Setup:} Let \(\mathfrak{g} = \mathfrak{k} _\mathbb{C} \) be a complex semisimple lie algebra with compact real form \(\mathfrak{k}\) 

Then \(\mathfrak{k}\) is the lie algebra of a compact lie group \(K\) and \(\mathfrak{g} = \mathfrak{k} \otimes \mathbb{C} \)

Let \(\mathfrak{t} \subset \mathfrak{k}\) be a maximal commutative subalgebra.

Then \(\mathfrak{h} = \mathfrak{t} _\mathbb{C}\) is the cartan subalgebra.

\(R \subset i \mathfrak{t}\) are the roots of \(\mathfrak{g}\) with respect to \(\mathfrak{h}\) 

Let \(\langle , \rangle \) be a \(k\)-invariant inner product on \(\mathfrak{g}\).

Let \(\Delta \) be fixed base for \(R\).

Let \(R^+, R^-\) be positive and negative roots relative to \(\Delta\) 

The coroots \(H_\alpha = \frac{2\alpha}{\langle \alpha ,\alpha  \rangle }\in \mathfrak{h}\) for each root \(\alpha \in R\) and let \(W\) be the weyl group.

\begin{definition}
    Let \((\pi ,V)\) be representation of \(\mathfrak{g}\) (possibly infinite dimensional). Then an element \(\lambda \in \mathfrak{h}\) is a \underline{weight} of \(\pi\) if \(\exists v\in V, v\neq 0\) such that:

    \[
        \pi (H)v = \langle \lambda , H \rangle v
    \]

    For all \(H\in \mathfrak{h}\) 

    The \underline{weight space} corresponding to \(\lambda \) is \(\{ v\in V : \pi (H)v = \langle \lambda ,H \rangle v \text{ for all } H\in \mathfrak{h} \} \)

    So \(v\) is the space of the common eigenvectors of of \(\pi (H)\) 

    The \underline{multiplicity} of \(\lambda\) is the dimension of the space above.

\end{definition}

Compare with roots and root spaces for \((ad, \mathfrak{g})\) 

What are the weights of the adjoint representation?

They are the roots and 0, \(R \cup \{ 0 \} \) 

The weight spaces of \((ad, \mathfrak{g} )\) are root spaces \(\cup \mathfrak{h}\) where \(\mathfrak{h} \) is associated to \(0\)

multiplicity is \(1\) if \(\lambda\in R\) and \(rank  \mathfrak{g} = \dim \mathfrak{h}\) if \(\lambda =0\) 

\begin{proposition}
    If \(\pi ,V\) is a \underline{finite dimensional} representation of \(\mathfrak{g}\) then every weight is an integral element. Caution: not true for infinite dimensional representation.
\end{proposition}

\begin{proof}
    Simple, just reduce to \(\mathfrak{sl}(2,\mathbb{C})\) 

    Let \(v\in V\) be a weight vector with weight \(\lambda\).
    
    To show that it is integral, we need to show it has integer inner product with every coroot.

    Thus, we need to show \(\langle \lambda ,H_\alpha \rangle \in \mathbb{Z}\) for all \(\alpha \in \mathbb{R}\) 

    For each coroot \(\alpha \in R\) let \(s^\alpha = span\{ X_\alpha , Y_\alpha , H_\alpha \} \) whhich is a subalgebra of \(\mathfrak{g} \) isomorphic to \(\mathfrak{sl} (2,\mathbb{C} )\). Restrict \(V\) to \(s^\alpha\) 

    Basic facts of finite dimensional representations of \(\mathfrak{sl}(2,\mathbb{C} ) \) is the representation of \(X_\alpha ,Y_\alpha ,H_\alpha \): the weights are integers.

    \[
        \pi (H)v = \pi (H_\alpha)v = \langle \lambda , H_\alpha ,v \rangle 
    \]

    This tells us \(\langle \lambda ,H_\alpha \rangle \in \mathbb{Z}\) 


\end{proof}

\begin{theorem}
    If \((\pi ,V)\) is a finite dimensional representation of \(\mathfrak{g} \) the weight of \(\pi \) and their multiplicities are invariant under the action of te weyl group on \(\mathfrak{h}\). Not true for infinite dimensions.
\end{theorem}

\begin{proof}
    See the textbook. Proof is same for corresponding result for roots.

    Essentially saying: \(W\) maps \(R\) into itself.

    Done by constructing: for each root \(\alpha \in R\) an invertible operator \(\mathcal{S}_\alpha : V \to V\) that maps weight vectors with weight \(\lambda\) into weight vector with weight \(\mathcal{S} _\alpha \cdot \lambda\) 
\end{proof}

This is the main result of this chapter, the \underline{Theorem of the Highest Weight}

\begin{theorem}
    [Theorem of Highest Weight for Complex Semisimple Lie Algebras]

    Here we only consider finite dimensional representations.

    \begin{enumerate}
        \item Every irreducible representation of \(\mathfrak{g}\) is the direct sum of its weight spaces.
        \item Every irreducible representation of \(\mathfrak{g}\) has a highest weight, the multiplicity of the highest weight is one.
        \item Two irreducible representation of \(\mathfrak{g}\) with the same highest weight are isomorphic.
        \item If \((\pi ,V)\) is an irreduible representation of \(\mathfrak{g}\) with highest weight \(\mu\) then \(\mu\) is dominant integral.
        \item Conversely, if \(\mu\) is dominant integral, there exists an irreducible representation of \(\mathfrak{g}\) with highest weight \(\mu\)
    \end{enumerate}

    Informally, there is a bijection between finite dimensional representations and integral dominant theorem.

    The hard part of the theorem is part v. Parts i through iv are easy - we will prove them next time.

\end{theorem}

\hrulefill

Class 18: 03/07

We begin from before.

There is a one to one correspondence between:

Irreducible finite dimensional representations of \(\mathfrak{g} \) 

And dominant integral elements.

We start the proof of 1-4 today.

\begin{proof}
    1: For each \(\lambda \in \mathfrak{h}\), notation:

    \[
        V_\lambda = \{ v\in V; \pi (H)v = \langle \lambda , H \rangle v, \forall H\in \mathfrak{h} \} 
    \]

    This is the weight space corresponding to \(\lambda\).

    Then, \(\lambda\) is a weight if and only if \(V_\lambda \neq \{ 0 \} \) 

    Then \(\operatorname{Weights}(V) = \{ \text{all weights of } V \} \subset \mathfrak{h} \) 

    We need to show,

    \[
        V = \bigoplus_{\lambda\in \operatorname{Weights}(V)}^{} V_\lambda
    \]

    Note that statement and proof is very similar to root space decomposition:

    \[
        \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\alpha \in R}^{} \mathfrak{g} _\alpha 
    \]

    Let \(\alpha \in R\) be a root from \(\mathfrak{s}^\alpha = \{ X_\alpha , Y_\alpha , H_\alpha \} \) 

    This is a subalgebra of \(\mathfrak{g} \cong \mathfrak{sl} (2,\mathbb{C})\) 

    Regard \(V\) as a representation of this \(\mathfrak{s} ^\alpha \cong \mathfrak{sl} (2,\mathbb{C})\) 

    \(\rho(H_\alpha)\) is diagonalizeable.

    \(H_\alpha\) where \(\alpha \in R\) spans \(\mathfrak{h}\) 

    The sum of two commuting diagonalizeable operators is diagonalisable.

    Thus, each \(\rho (H),H\in \mathfrak{h}\) is diagonalizeable.

    Thus, \(\rho(H), H\in \mathfrak{h}\) are diagonalizeabe and commuting.

    This means we get decomposition:

    \[
        V = \bigoplus_{\lambda \in \operatorname{Weights}(V)} V_\lambda    
    \]

    2: Lemma: let \(\alpha \in R, X\in \mathfrak{g} ^\alpha , \lambda \in \mathfrak{h} , v\in V_\lambda\) then,

    \[
        \pi (X)v \in V_{\lambda + \alpha}
    \]

    If \(\lambda + \alpha\) is not a weight, then \(\pi (X)v = 0\) 
    
    Proof: for all \(H\in \mathfrak{h}\) we have:

    \[
        \pi(H)\pi(X)v = \pi(X)\pi(H)v + \pi([H,X]) v
    \]

    \[
        \pi(H)v = \langle \lambda , H \rangle v, \pi ([H,X])v = \langle \alpha,H \rangle X
    \]

    So, that is equal to:

    \[
        = \langle \lambda + \alpha \rangle \pi(X)v
    \]

    Thus we have proved the lemma. Now we go back to the previous:
    
    \(\dim V < \infty\) means weights(V) is a finite set. Let \(\mu \in \) weights(V) be a max element in the set in the sense that there doesn't any \(\lambda \in\) weights(V) such that \(\lambda \geq \mu, \lambda \neq \mu\) 

    Pick any \(v\in V_\mu, v\neq 0\)
    
    Lemma implies \(\pi(X)v = 0\) for all \(X\in \mathfrak{g} ^\alpha , \alpha \in R^+\) 

    Now, let \(W\) be the \(\mathbb{C}\)-span of:

    \(\{ \pi(Y_{j1}) \pi(Y_{j2})\cdots \pi(Y_{jk}) v ; k = 0,1,2, \text{ each } Y_{jl} \in \mathfrak{g}_{\alpha_{jl}}, \alpha_{jl}\in R^- \} \) 

    Claim: \(W\) is \(\mathfrak{g}\)-invariant. Hence \(W = V\) by irreducibility.

    Proof: Let \(R^+ = \{ \alpha_1,\cdots, \alpha_N \} \) 

    choose a vector space basis for \(\mathfrak{g}:\) \(X_1 \in \mathfrak{g}_{\alpha_1}, \cdots, X_N\in \mathfrak{g}_{\alpha_N} \)
    
    \(Y_1\in \mathfrak{g}_{-\alpha_1},\cdots, Y_n\in \mathfrak{g}_{-\alpha_N} \)
    
    Where \(H_1,\cdots, H_r\) is basis for \(\mathfrak{h} \) 

    Need to show that \(W\) is invariant under each \(\pi (Y_j)\) and \(\pi(H_j)\) and \(\pi(X_j)\) 

    \(Y_j\) and \(H_j\) we have automatically from the definition.

    For \(X_j\):

    \(\pi(X_j)\pi (Y_{j1})\cdots \pi(Y_{jk})v \in W\) by induction on \(k\). \(k=0\) we already have.

    \(\pi (X_j)\pi(Y_{j1})\cdots \pi(Y_{jk})v\)
    
    \(= \pi(Y_{j1})\pi(X_j)\pi(Y_{j2})\cdots \pi(Y_{jk})v\)
    
    \(+ \pi([X_j,Y_{j1}])\pi(Y_{j2})\cdots \pi(Y_{jk})v\) 

    \([X_j, Y_{j1}]\) can be \(0, \in \mathfrak{h} \) or one of the root vectors.

    \(0, \mathfrak{h}\) we're done. Negative root implies this belongs to the span, positive root we have less elements so we can invoke induction hypothesis.

    Or we can apply PBW.

    Analyzing weights of \(W = V\) , we see that \(\forall \lambda \in\) Weights(V), \(\lambda \leq \mu \) and the multiplicity of \(\mu = \dim V_{\mu} \) is \(1\).

    We prove 4 and then come back to 3.

    4: All weights of \(V\) are integral, so \(\mu\) is integral.

    Now the dominant part.

    Theorem 9.3 implies: the weights of \(V\) and their multiplicities are invariant under Weyl group \(W\).

    Proposition 8.29 implies, there exists \(w\in W\) such that \(w \mu \in \overline{C}\) [closure of the fundamental weyl chamber].

    Then \(w \mu\) is dominant and in Weights(V).

    Since \(w \mu\) is dominant, \(\mu \preceq w\cdot \mu \) 

    Since \(\mu\) is maximal, \(\mu = w\cdot \mu\) is dominant.

    Finally, we do part 3.

    Suppose \((\pi_1, V_1)\) and \((\pi_2, V_2)\) are irreducible representation with same highest weight \(\mu \).
    
    Let \(v_1\in V_{1 \mu}, v_1\neq 0, v_2\in V_{2\mu}, v_2\neq 0\) are highest weight vectors of \(V_1, V_2\) 
    
    Form \((\pi_1 \oplus \pi_2, V_1 \oplus V_2)\) and let \(U \subset V_1 \oplus V_2\) be the smallest invariant subspace containing \((v_1,v_2)\in V_1 \oplus V_2\).

    \underline{Claim}: \(U\) is irreducible with highest weight \(\mu\).

    Proof: First observation: \(V_1 \oplus V_2\) is completely reducible since it is direct sum of two irreducibles.

    Proposition 4.26 implies \(U\) is completely reducible as well.

    Hence, \(U = \bigoplus_{j}^{} U_j\) where each \(U_j\) is irreducible.

    Since \(\dim U_\mu = 1\), exactly one \(U_j\) contains \(U_\mu\).

    This implies \((v_1, v_2)\) generates this particular \(U_j\) and \(U = U_j\).

    So, irreducible and the highest weight of \(U_j = U\) is \(\mu\) 

    We can have two projections \(P_1,P_2\) from \(V_1 \oplus V_2\), projection to \(V_1\) and projection to \(V_2\).

    \(P_1(v,w)=v, P_2(v,w)=w\) 

    these are intertwining maps, and \(P_1|_U, P_2|_U\) are not identically \(0\), \(P(v_1,v_2)\neq 0, P_2(v_1,v_2)\neq 0\) 

    We can invoke Schur's lemma:

    \(U, V_1, V_2\) are irreducible, by Schur's lemma: \(P_1|_U : U\to V_1\) and same to \(V_2\) are isomorphisms.

    So, \(V_1 \cong U \cong V_2\) 

    This proves 3.

\end{proof}

\section*{Highest Weight Cyclic Representations (0.2)}

\begin{definition}
    A representation \((\pi ,V)\) of \(\mathfrak{g}\) [possibly infinite dimensional] is called a \underline{highest highest weight cyclic representation} with highest weight \(\mu\) if there exists \(v\in V, v\neq 0\) such that:

    i: \(\pi(H)v = \langle \mu , H \rangle v\) for all \(H\in \mathfrak{h}\) 

    ii: \(\pi(X)v = 0 \forall X\in \mathfrak{g} _\alpha \) with \(\alpha \in R^+\) 

    iii: The only invariant subspace containing \(v\) is \(V\).

    Example: if \((\pi,V)\) is irreducible finite dimensional with highest weight \(\mu\), then it is a cyclic representation.

\end{definition}

Comments:

\begin{enumerate}
    \item The definition does not require \(\mu\) to be dominant or integral
    \item Highest weight representation may or may not be irreducible
    \item Two highest weight cyclic representations \((\pi_1 ,V_1)\) and \((\pi_2, V_2)\) with the same highest weight need not be isomorphic.
    \item It is true that two \underline{finite dimensional} highest weight cyclic representation with the same highest weight are irreducible and isomorphic.
    \item 
\end{enumerate}

Example:

Let \(\mu \in \mathbb{C}\). Then \(\mathfrak{sl} (2,\mathbb{C})\) has an \(\infty\)-dim highest weight cyclic representation \(\pi_\mu , V_\mu\) such that:

\[
    \begin{tikzcd}
        \cdots & v_3 \ar[l,bend right = 60, "\pi_\mu(Y)",swap] & v_2 \ar[l,bend right = 60, "\pi_\mu(Y)",swap] & v_1 \ar[l,bend right = 60, "\pi_\mu(Y)",swap] & v_0 \ar[l,bend right = 60, "\pi_\mu(Y)",swap]
    \end{tikzcd}
\]

finish diagram later.

\(\pi_\mu(Y)v_j = v_{j+1}, \pi_\mu(H)v_j = (\mu-2j)v_j\)

\(\pi_\mu(X)v_0 = 0, \pi_\mu(X)v_j = j (\mu - (j-1))v_{j-1}\) 

If \(\mu = m, m\in \mathbb{Z} , m \geq 0\), \(\pi_\mu(X)v_{m+1} = 0\) 

Insert another picture here.

The quotient \(V_m / V_{-m-2} \) - finite dimensional irreducible weight as \(V_m\)  

\hrulefill

Class 19: 03/19

Recall that we have a bijection

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
           Integral dominant \(\mu \in \mathfrak{h} \)   & \(\leftrightarrow\) & irreducible finite dimensional representation of \(\mathfrak{g}\)   \\
        \bottomrule
    \end{tabular}
    \caption{bijection}
    \label{tab:label}
\end{table} 

Hard part: starting with \(\mu\) find / construct the corresponding irreducible representation of \(\mathfrak{g}\).

\(\mathfrak{g}\) is a complex semisimple lie algebra.

So we talk about Verma Modules (9.5)

\section*{Verma Modules (9.5)}

Let \(\mathfrak{g}\) be complex semisimple lie algebra, \(\mathfrak{h}\) the cartan subalgebra. We have the root decomposition:

\[
    \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\alpha \in R} \mathfrak{g}_\alpha
\]

Fix a base \(\Delta \subset R\), that gives us positive and negative roots.

\(R = R^+ \sqcup R^-\) 

So we can write:

\(\mathfrak{g} = \mathfrak{n}^+ \oplus \mathfrak{h} \oplus \mathfrak{n}^-  \) 

Where \(\mathfrak{n}^+ = \bigoplus_{\alpha \in R^+}\mathfrak{g}_\alpha, \mathfrak{n}^- = \bigoplus_{\alpha \in R^-} \mathfrak{g}_\alpha\) 

These are (nilpotent) subalgebras.

Fix \(\mu \in \mathfrak{h}\) (any!), consider \underline{left} ideal \(I_\mu \subset \mathcal{U}(\mathfrak{g})\) generated by \(X\in \mathfrak{n} ^+\) abd \(H - \langle \mu ,H \rangle 1, H\in \mathfrak{h}\).

Form a quotient \(W_\mu = \mathcal{U} (\mathfrak{g}) / I_\mu\) - left \(\mathcal{U} (\mathfrak{g})\)-module.

\(\leftrightsquigarrow\) representation \((\pi_\mu , W_\mu)\) of \(\mathfrak{g}\) (\(\infty\)-dim) called the \underline{Verma module} with highest weight \(\mu\).

\begin{theorem}
    Let \(v_0\in W_\mu = \mathcal{U} (\mathfrak{g}) / I_\mu\) be the image of \(1\in \mathcal{U} (\mathfrak{g})\) in the quotient.
    
    Hard part: Prove that \(v_0\neq 0\)

    Easier: \(W_\mu\) is a highest weight cyclic representation with highest weight \(\mu\) and highest weight vector \(v_0\).  
\end{theorem}

\begin{proof}
    (assuming \(v_0\neq 0\)): Need to check the following properties.

    \begin{enumerate}
        \item \(\forall H\in \mathfrak{h}, Hv_0 \equiv H1 \equiv 1 \langle \mu,H \rangle \equiv \langle \mu,H \rangle v_0  \pmod{I_\mu}\). This tells us \(v_0\) has weight \(\mu\).
        \item \(\forall X\in \mathfrak{n}^+, Xv_0 \equiv X \equiv 0 \pmod{I_\mu}\)
        \item \(v_0\) generates \(W_\mu\)    
    \end{enumerate}

    For proving 3: clearly \(1\in \mathcal{U}(\mathfrak{g})\) generates all of \(\mathcal{U}(\mathfrak{g})\) as a left \(\mathcal{U}(\mathfrak{g})\)-module.
    
    Thus, since \(v_0\) is the image of \(1\) in the quotient, \(v_0\) generates all of \(W_\mu\) as a left \(\mathcal{U}(\mathfrak{g})\)-module.
    
    Thus, \(v_0\) generates \(W_\mu\) as a representation of \(\mathfrak{g}\).  

    So, all that is left to be proved is that \(v_0 \neq 0\).

    Let \(\mathfrak{b} = \mathfrak{h} \oplus \mathfrak{n} ^+\), subalgebra of \(\mathfrak{g}\).
    
    Then \(\mathcal{U}(\mathfrak{b}) \subset \mathcal{U}(\mathfrak{g})\) [consequence of PBW theorem].
    
    \underline{Lemma}: Let \(\mathcal{J}_\mu \subset \mathcal{U}(\mathfrak{b})\) be the left ideal generated by \(X\in \mathfrak{n}^+\) and \(H - \langle \mu , H \rangle 1, H\in \mathfrak{h} \). Then \(1 \notin \mathcal{J}_\mu\).
    
    \begin{proof}
        Define a 1-dimensional representation \(\sigma_\mu\) of \(\mathfrak{b}\) by:
        
        \(\sigma(H+X) = \langle \mu , H \rangle \) [think of this as a \(1\times 1\) matrix with entry in \(\mathbb{C}\). Here \(H\in \mathfrak{h}, X\in \mathfrak{n}^+\) ]

        This is a representation of \(\mathfrak{b}\): if \(H_1 + X_1, H_2 + X_2\in \mathfrak{b} = \mathfrak{h} \oplus \mathfrak{n} ^+\) then:
        
        \[
            [H_1 + X_1, H_2 + X_2] \in \mathfrak{n}^+
        \]

        Thus, by definition,

        \[
            \sigma_\mu ([H_1 + X_1, H_2 + X_2]) = 0 = [\sigma_\mu(H_1 + X_1), \sigma_\mu(H_2 + X_2)]
        \]

        This proves that this is indeed a representation.

        This representation of \(\mathfrak{b}\) corresponds to a 1-dimensional \(\mathcal{U}(\mathfrak{b})\)-module \(\mathbb{C}_\mu\).
        
        \[
            \ker(\mathcal{U}(\mathfrak{b}) \to \operatorname{End}(\mathbb{C}_\mu) = \mathbb{C}) \supset \mathfrak{n}^+, H - \langle \mu , H \rangle 1 
        \]

        This implies that this kernel contains \(\mathcal{J}_\mu\)
        
        \(\mathbb{C}_\mu \neq \{ 0 \} \implies \ker \neq \mathcal{U}(\mathfrak{b}) \implies 1\notin \ker \implies 1\notin \mathcal{J}_\mu\) 

    \end{proof}

    Now,

    Fix a vector space basis \(\{ Y_1, \cdots, Y_k \} \) of \(\mathfrak{n}^-\).

    For example, take \(Y_j \in \mathfrak{g}_{\alpha_j}, Y_j \neq, \alpha_j\in R^-\).

    \begin{theorem}
        The elements \(\pi_\mu(Y_1)^{n_1}\pi_\mu(Y_2)^{n_2}\cdots\pi_\mu (Y_k)^{n_k}v_0\) where each \(n_j \geq 0\), form a vector space basis for \(W_\mu\). In particlar, \(v_0 = \pi_\mu(Y_1)^0\cdots\pi_\mu(Y_k)^{0}v_0 \neq 0\)    
    \end{theorem}

    \begin{proof}
        \(\mathfrak{g} = \mathfrak{n}^- \oplus \mathfrak{b}\) as vector spaces.
        
        Choose a vector space basis \(\{ Z_1, \cdots, Z_l \}\) of \(\mathfrak{b}\).
        
        Combine basis: \(\{ Y_1,\cdots, Y_k, Z_1,\cdots,Z_l \}\) vactor space basis for \(\mathfrak{g}\).
        
        PBW implies: every element \(a\) of \(\mathcal{U}(\mathfrak{g})\) can be written uniquely as a (finite) linear combination of elements of the form:
        
        \[
            Y_1 ^{n_1} Y_2 ^{n_2}\cdots Y_k^{n_k} Z_1^{m_1}\cdots Z_l^{m_l}
        \]

        Thereofre, \(a\) can be expressed \underline{uniquely} as :

        \[
            a = \sum_{n_1,\cdots, n_k = 0}^{\infty} Y_1^{n_1}Y_2^{n_2}\cdots Y_k^{n_k} a_{n_1\cdots n_k}
        \]

        each \(a_{n_1\cdots n_k}\in \mathcal{U}(\mathfrak{b})\). There are only finitely many non-zero terms!

        Now, \(a\in I_\mu \iff a\) is linear combination of terms of the form \(\mathfrak{b}X\) and \(\mathfrak{b}(H - \langle \mu , H \rangle )1, \mathfrak{b} \in \mathcal{U}(\mathfrak{g}) \).
        
        Write each \(\mathfrak{b}\) in the unique form. Then \(a\) is a linear combination of terms of the form:
        
        \[
            Y_1 ^{n_1} Y_2 ^{n_2} \cdots Y_k^{n_k} \mathfrak{b}_{n_1\cdots n_k} X_{n_1\cdots n_k}
        \]

        \[
            Y_1 ^{n_1} Y_2 ^{n_2} \cdots Y_k^{n_k} \mathfrak{b}_{n_1\cdots n_k} (H_{n_1\cdots n_k}- \langle \mu , H_{n_1\cdots n_k} \rangle 1 )
        \]

        Where \(X_{n_1,\cdots n_k}\in \mathfrak{n}^+, \mathfrak{b}_{n_1\cdots n_k}\in \mathcal{U}(\mathfrak{b}), H_{n_1\cdots n_k}\in \mathfrak{h}\) 

        So, they both belong to \(\mathcal{J}_\mu\) 

        Conclode: \(a\in I_\mu \if\) each \(a_{n_1\cdots n_k}\) in the unique expression belongs to \(J_\mu\).
        
        If \(a=1\) the uniqueness of expression implies the expression has only 1 term.

        Thus, \(n_1 = \cdots = n_k = 0, a_{0 \cdots 0} = 1\).

        We conclude that \(v_0 \neq 0\) in \(W_\mu = \mathcal{U}(\mathfrak{g}) / I_\mu \) 

    \end{proof}
    
    We still need to prove linear independence of \(\pi_\mu (Y_1)^{n_1}\cdots \pi_\mu(Y_k)^{n_k}v_0\) in \(W_\mu = \mathcal{U}(\mathfrak{g}) / I_\mu\). Suppose not.
    
    \[
        0 = \sum c_{n_1\cdots n_k} \pi_\mu(Y_1)^{n_1}\cdots \pi(Y_k)^{n_k}v_0
    \]

    If and only if:

    \[
        a = \sum \cdots \in I_\mu
    \]

    If and only if:

    \[
        c_{n_1\cdots n_k}\in J_\mu
    \]

    If and only if:

    each \(c_{n_1,\cdots,n_k}=0\) 

    So we have linear independence.

    Note that PBW was crucial for this: the unique expression is a consequence of PBW!

\end{proof}

\section*{Irreducible Quotient Module (9.6)}

\underline{Goal}: Show each Verma module \(W_\mu\) has a proper invariant subspace \(U_\mu \) such that the quotient \(W_\mu / U_\mu\) is irreducible with highest weight \(\mu\). (\underline{All} \(\mu \in \mathfrak{h}\) )

Next section: if \(\mu\) is \underline{dominant integral} then this quotient \(W_\mu / U_\mu\) is finite dimensional.  

\begin{theorem}
    Elements of the form:

    \[
        \pi_\mu (Y_1)^{n_1} \pi_\mu(Y_2)^{n_2}\cdots \pi_\mu(Y_k)^{n_k}v_0
    \]

    \(v_0 = \text{ image of } 1\in \mathcal{U}(\mathfrak{g}) \text{ in } W_\mu = \mathcal{U}(\mathfrak{g}) / I_\mu\)

    \(\{ Y_1,\cdots, Y_k \} \) basis for \(\mathfrak{n}^-\)
    
    \(n_1,\cdots n_k\) non-negative integers

    \underline{Form a basis for \(W_\mu\)}.
    
    Each of these elements is a weight vector of \(W_\mu \)
    
    \underline{Conclude}: \(W_\mu\) is an (infinite) direct sum of weight spaces and the weight space corresponding to \(\mu\) is spanned by \(v_0\) (has multiplicity \(1\)).   
\end{theorem}

Every vector \(v\in W_\mu\) can be expressed uniquely as a finite sum of weight vectors and it makes sense to talk about the component of \(v\) of weight \(\mu\). The `\(v_0\)- component', since \(v_0\) is the only vector in this.

\begin{definition}
    Let \(U_\mu\) be the subspace of \(W_\mu\) consisting of all vectors \(v\in W_{\mu}\) such that the \(v_0\) component of the vector is \(0\) and also such that each \(\pi_\mu(X_1)\cdots \pi_\mu(X_N)v\) also has zero \(v_0\)-component for all \(X_1,\cdots,X_N \in \mathfrak{n}^+\).     
\end{definition}

\begin{proposition}
    \(U_\mu\) is \(\mathfrak{g}\)-invariant subspace of \(W_\mu\)    
\end{proposition}

\begin{proposition}
    \(V_\mu = W_\mu / U_\mu\) is an irreducible representation of \(\mathfrak{g}\).  
\end{proposition}

\hrulefill

Class 20: 03/21

\underline{Irreducible Quotient modules} 

Last time: constructed Verma module \(W_\mu = \mathcal{U}(\mathfrak{g} ) / I_\mu\) with highest weight \(\mu\). Let \(v_0\) be the image of \(1\in \mathcal{U}(\mathfrak{g})\) in this quotient.

We saw that \(W_\mu\) is an (infinite) direct sum of weight spaces and multiplicity of weight space corresponding to \(\mu\) is \(1\), spanned by \(v_0\). For \(v\in W_\mu\), can tlk about \(v_0\)-compound.

\begin{definition}
    Let \(U_\mu\) be the subspace of \(W_\mu\) consisting of all vectors \(v\in W_\mu\) such that the \(v_0\)-component of \(v\) is \(0\) and such that each \(\pi_\mu(X_1)\cdots\pi_\mu(X_N)v\) also has zero \(v_0\)-component for all \(X_1,\cdots,X_n\in \mathfrak{n}^+\) 
\end{definition}

\begin{proposition}
    \(U_\mu\) is a \(\mathfrak{g}\)-invariant subspace of \(W_\mu\). 
\end{proposition}

\begin{proof}
    Choose a vector space basis for \(\mathfrak{g}\).
    
    Let \(\{ X_1,\cdots,X_k \}\)-basis for \(\mathfrak{n}^+\).
    
    Let \(\{ Y_1,\cdots,Y_k \} \)-basis for \(\mathfrak{n}^-\), each \(Y_j\) being a root vector.

    Let \(\{ H_1, \cdots, H_r \} \)-basis for \(\mathfrak{h}\)
    
    Need to show: if \(v\in U_\mu\) then so does \(\pi_\mu(Z)v\) for all \(Z\in \mathfrak{g} \) for all \(X_1,\cdots,X_n\in \mathfrak{n}^+\)  

    We to check that the \(v_0\)-component of

    \(\pi_\mu(X_{j_1})\cdots\pi_\mu(X_{j_N})\pi_\mu(Z)v =\) linear combination of

    \(\pi_\mu(Y_1)^?\cdots\pi_\mu(Y_n)^?\underbrace{\pi_\mu(H_1)^?\cdots\pi_\mu(H_r)^?\underbrace{\pi_\mu(X_1)^?\cdots\pi_\mu(X_k)^?v}_{\text{\(v_0\)-component is \(0\)}}}_{\text{\(v_0\)-component is \(0\)}}\) 

    And the final part also has \(v_0\) component equal \(0\)

\end{proof}

\begin{proposition}
    \(V_\mu = W_\mu / U_\mu\) is an irreducible representation of \(\mathfrak{g}\). 
\end{proposition}

\begin{proof}
    Bijection between:

    Invariant subspaces of \(W_\mu / U_\mu\)
    
    Invariant subspaces of \(W_\mu\) containing \(U_\mu\) 

    Suppose \(\tilde{U} < W_\mu\) is a \(\mathfrak{g}\)-invariant subspace, \(U_\mu \subsetneq \tilde{U}\).

    Then \(\tilde{U}\) contains a vector \(v\) such that:

    \(u = \pi_\mu(X_1)\cdots\pi_\mu(X_n)v\) has a non-zero \(v_0\)-component.
    
    Then, \(u\in\tilde{U}\) 
    
    Claim: \(v_0\in\tilde{U}\) so \(\tilde{U} = W_\mu\).

    Proof: write \(u = v_\mu + v_{\lambda_1}+\cdots+v_{\lambda_l}\).

    \(v_\mu\) is proportional to \(v_0\) 

    each \(v_{\lambda_j}\) is a weight vector of weight \(\lambda_j \neq \mu\) 

    We want to reduce the number of weights one by one. Since the last weight is different from \(\mu\) we can find an element of the cartan algebra such that \(\langle \mu, H \rangle \neq \langle \lambda_l, H \rangle \). Then look at the expression \(\pi_\mu(H) - \langle \lambda_l, H \rangle \mu = \underbrace{\langle \mu - \lambda_l, H \rangle}_{\neq 0} v_\mu + \langle \lambda_1 - \lambda_l, H \rangle v_{\lambda_1} + \dots + \underbrace{\langle \lambda_l - \lambda_l, H \rangle}_{=0} v_{\lambda_l}\)  

    Since the last oe if \(0\) we have fewer weights. We reduce the weight one by one until we end up with only \(\mu\). Induction on \(l\).

    \(v_0\notin U_\mu\) implies its image in quotient \(V_\mu = W_\mu / U_\mu\) os \underline{not} zero.

    Thus, \(V_\mu\) is a highest weight cyclic representation with highest weight \(\mu\), irreducible.

\end{proof}

Finally, we need finite dimensionality.

Let \(\alpha \in \Delta\) and let \(s^\alpha = \{ X_\alpha , Y_\alpha , H_\alpha  \} \)- lie subalgebra of \(\mathfrak{g} \) isomorphic to \(\mathfrak{sl}(2,\mathbb{C})\).

\underline{Lemma:} Suppose \(\langle \mu, H_\alpha \rangle = m\) - non-negative integer. Then the vector \(\pi(Y_\alpha)^{m+1}v_0\in U_\mu\)

\begin{proof}
    Note each \(\pi(Y_\alpha)^j v_0\) is a weight vector for \(\pi(H_\alpha)\) of weight \(m-2j\) \(\pi(H_\alpha)\pi(Y_\alpha)^j v_0 = (m - 2j)\pi(Y_\alpha)^j v_0\).
    
    We conclude that the \(v_0\)-component of each \(\pi(Y_\alpha)^j v_0, j > 0\) is zero.
    
    Argument from proof of theorem 4.32 shows that:

    \(\pi(X_\alpha)\pi(Y_\alpha)^j v_0 = j(m-(j-1))\pi(Y_\alpha)^{j-1}v_0\)
    
    When \(j=m+1\): \(\pi(X_\alpha)\pi(Y_\alpha)^{m+1}v_0 = (m+1)\underbrace{(m-m)}_{=0}\pi(Y_\alpha)^m v_0 = 0\)
    
    On the other hand, if \(\beta \in R^+\) with \(\beta \neq \alpha\) then for \(X_\beta\in \mathfrak{g}_\beta\), 
    
    \(\pi(X_\beta)\pi(Y_\alpha)^{m+1}v_0\) is either zero or weight vector of weight \(\mu - (m+1)\alpha + \beta\).
    
    There's a problem, \(\mu - (m+1)\alpha + \beta \not \preceq \mu\)
    
    Thus, \(\pi(X)\pi(Y_\alpha)^{m+1}v_0 = 0\)
    
    for all \(\in \mathfrak{n}^+\), and in particular, has \(0\) \(v_0\)-component.
    
    \(\implies \pi(Y_\alpha)^{m+1}v_0\in U_\mu\).

\end{proof}

\section*{Finite Dimensionality of \(V_\mu = W_\mu / U_\mu\) [9.7]}

\underline{Observe}: Each weight occurs in Verma module \(W_\mu\) with finite multiplicity (hwk). Therefore, each weight of \(V_\mu\) occurs with finite multiplicity.

Thus, to prove that \(\dim V_\mu < \infty\) it is sufficient to show that \(V_\mu\) has finitely many different weights. 

To prove this, it is sufficient to show that weights of \(V_\mu\) are \(W\)-invariant (\(W\) is the Weyl Group).

Each \(W\)-orbit \(\{ w\cdot \lambda_j, w\in W \} \) (\(\lambda\)-weiht of \(V_\nu\)) is a finite set and has one dominant representative \(\lambda^{\prime} \) (proposition 8.29).

Thus \(X\) is \underline{integral} dominant and \(0 \preceq \lambda ^{\prime}  \preceq \mu\)

The set of \(\{ \eta \in \mathfrak{h} ; \eta \text{ integral dominant and } 0 \preceq \eta \preceq \mu \text{ is finite}  \} \) 

Need exp(operator)- problematic in \(\infty\)-dim.

Workaround: locally nilpotent operators.

\begin{definition}
    A linear operator \(A\) on a vector space is \underline{locally nilpotent} if for each \(v\in V\) there exists \(k(v)\in\mathbb{Z}^+\) such that \(A^{k(v)}v=0\) 
    
    \(V\) finite dimensional implies \(A\) is nilpotent.
\end{definition}

In any cases, \(A\)-locally nilpotent means \(e^A\) can be defined as \(e^A v = \sum_{k=0}^{\infty} \frac{A^k}{k!} v = \sum_{k=0}^{k(v)}\frac{A^k}{k!}v\) - finite sum! 

\begin{proposition}
    For each \(\alpha \in \Delta\), let \(s^\alpha = \{ X_\alpha , Y_\alpha , H_\alpha \} \) be as in theorem 7.19 (\(X_\alpha \in \mathfrak{g}_\alpha , Y_\alpha \in \mathfrak{g} _{-\alpha}, H_\alpha\) is coroot of \(\alpha\) satisfying the \([,]\) relations of \(\mathfrak{sl}(2,\mathbb{C})\)). If \(\mu\) is dominant integral, then \(X_\alpha, Y_\alpha\) act on \(V_\alpha= W_\alpha / U_\alpha\) in a locally nilpotent fashion.    
\end{proposition}

\begin{proof}
    \(\mu\)-integral dominant implies \(\langle \mu, H_\alpha \rangle - m\), where \(m\) is a non-negative integer.
    
    span \(\{ \pi_\mu(Y_\alpha)^k v_0, k = 0,1,2,\cdots\} \subset V_\mu = W_\mu / U_\mu\).
    
    \(s^\alpha\)-invariant subspace of \(V_\mu\) 
    
    previous lemma implies \(\pi_\mu(Y_\alpha)^{m+1} v_0 = 0\)
    
    so this space is finite dimensional \(s^\alpha\)-invariant subspace of \(V_\mu\).
    
    Thus, \underline{\(v_0\) is \(s^\alpha\)-finite}
    
    Call a vector \(v\in V_\mu\) \underline{\(s^\alpha\)-finite} if it lies in a finite-dimensional \(s^\alpha\)-invariant subspace of \(V_\mu\).
    
    Let \(F_\alpha = \{ \text{all } s^\alpha \text{-finite vectors in } V_\mu \} \)
    
    \(v_0\in F_\alpha \implies F_\alpha \neq \{ 0 \} \)
    
    We claim that \(F_\alpha\) is \(\mathfrak{g}\)-invariant. By irreducibility, \(F_\alpha = V_\mu\)
    
    Proof of claim: pick any \(v\in F_\alpha\). Then \(v\) lies in a finite-dimensional \(s^\alpha\)-invariant subspace \(\mathcal{S}\) of \(V_\mu\).
    
    Let \(S^{\prime}=\mathbb{C}\)-span of \(\{ \pi_\mu(X)s; X\in \mathfrak{g} , s\in S \} \subset V_\mu \)
    
    \(\dim (S^{\prime} ) \leq (\dim \mathfrak{g})(\dim S) < \infty\) and \(S^{\prime}\) is \(s^\alpha\)-invariant.
    
    If \(\pi_\mu(x)s\in S^{\prime}, Z\in s^\alpha\) then
    
    [we'll finish the rest later]. 
\end{proof}

\hrulefill

Class 21: 03/26

\underline{Finite-dimensionality of \(V_\mu = W_\mu / U_\mu\) } 

It is sufficient to show that the weight of \(V_\mu\) are \(W\)-invariant.

\begin{definition}
    A linear operator \(A\) on a vector space \(V\) is \underline{locally finite} if for each \(v\in V\) there exists \(k(v)\in\mathbb{Z}^{>0}\) such that \(A^{k(v)}v=0\)  
\end{definition}

Purpose: then \(e^A\) makes sense

\(e^A v=\sum_{k=0}^{\infty} \frac{A^k}{k!}v = \sum_{k=0}^{k(v)} \frac{A^k}{k!}v\) 


\begin{proposition}
    For each \(\alpha \in \Delta \) let \(s^\alpha = span(X_\alpha , Y_\alpha , H_\alpha) \) be as in theorem 7.19. If \(\mu\) is dominant integral, then, \(X_\alpha\) and \(Y_\alpha\) act on \(V_\mu = W_\mu / U_\mu\) in a locally nilpotent fashion.
\end{proposition}

\begin{proof}
    Since \(\mu\) is integral dominant, \(\langle \mu, H_\alpha  \rangle = m\in\mathbb{Z}_{\geq 0} \). Call a vector \(v\in V_\mu\) \(s^\alpha\)-finite if it lies in a finite-dimensional \(s^\alpha\)-finite if it lies in a finite dimensional \(s^\alpha\)-invariant subspae of \(V_\mu\).

    Let \(v_0\in V_\mu\) be a vector of weight \(\mu\).

    \underline{CLaim:} \(v_0\) is \(s^\alpha\)-finite.
    
    proof: \(span\{ \pi_\mu(Y_\alpha)^k v_0 \} \subset V_\mu \) is \(s^\alpha\)-invariant. Lemma from last tome: \(\pi_\mu(Y_\alpha)^{m+1} v_0 = 0,\) so this space is finite. We're done.

    Let \(F_\alpha =\{ \text{all \(s^\alpha\)-finite vectors in \(V_\mu\)}\}  \neq \{ 0 \} \).
    
    \underline{Claim:} \(F_\alpha\) is \(\mathfrak{g}\)-invariant.
    
    And since \(V_\mu\) is irreducible, \(F_\alpha=V_\mu\)
    
    Proof: Pick any \(v\in F_\alpha\). Then \(v\) lies in a finite-dimensional \(s^\alpha\)-invariant subspace \(S\) of \(V_\mu\).
    
    \(S^{\prime} = span\{ \pi_\mu(X)s; X\in \mathfrak{g}, s\in S  \} \)  

    \(\dim S^{\prime} \leq \dim \mathfrak{g} \cdot\dim S < \infty\)
    
    \(S^1\) is \(s^\alpha\)-invariant if \(\pi_\mu(X)s\in S^1, Z\in s^\alpha\) 
    
    \(\pi_\mu(Z)\pi_\mu(X)s=\underbrace{\pi_\mu(X)\underbrace{\pi_\mu(Z)s}_{\in S}}_{\in S^{\prime} }+\underbrace{\pi_\mu(\underbrace{[Z,X]}_{\in \mathfrak{g} })s}_{\in S^{\prime}}\) 

    Thus, \(\pi_\mu(Z)\pi_\mu(X)s\in S^{\prime}\) 

    Thus, \(\pi_\mu(X)v\in F_\alpha\) for all \(X\in \mathfrak{g}\)  

    Let \(v\in V_\mu\) be any vector, then \(v\) is \(s^\alpha\)-finite and
    
    \[
        \pi_\mu(X_\alpha)^{k(v)}v = \pi_\mu(Y_\alpha)^{k(v)}v = 0
    \]

    Which is what we needed to show.

    Thus, the following terms make sense:

    \[
        e^{\pi_\mu(X_\alpha)},e^{\pi_\mu(Y_\alpha)}
    \]

    on \(V_\mu\)
\end{proof}

\begin{proposition}
    If \(\mu\) is dominant integral, the weights of \(V_\mu\) are \(W\)-invariant [Weyl Group]
\end{proposition}

\underline{Sketch}: \(W\) is generated by reflections \(s_\alpha, \alpha \in \Delta\).

In the finite dimension case, use operator \(S_\alpha=e^{\pi_\mu(X_\alpha)}e^{-\pi_\mu(Y_\alpha)}e^{\pi_\mu(X_\alpha)}\) acting on \(V_\mu\).

So, \(S_\alpha\) makes sense.

\underline{Corollary}: \(V_\mu\) is finite dimensional. All weights of \(V_\mu\) are integral. 

Let \(\lambda\) be a weight of \(V_\mu\)

\(W\cdot \lambda\), the orbit under weyl group

\(\lambda\) has unique dominant representation \(\lambda ^{\prime}\)

\(\{ \lambda \text{ - integral dominant } 0 \preceq \lambda^{\prime}  \preceq \mu\}\) is a finite set.

So, \(V_\mu\) is irreducible finite dimensional representation of \(\mathfrak{g}\) with highest weight \(\mu\).

\section*{Weights of Irreducible Finite-Dimensional Representations (10.1)}

Let \(\mu\)-dominant integral element of \(\mathfrak{h}\)

\((\pi_\mu,V_\mu)\) irreducible finite dimensional representation of \(\mathfrak{g}\) with highest weight \(\mu\)

\(W\cdot\mu = \) orbit of \(\mu\) under the action of the Weyl group.

\(Conv(W\cdot \mu)\) be the convex hull of this orbit.

\begin{theorem}
    An integral element \(\lambda\) is a weight of \(V_\mu\) if and only if two conditions are satisfied:
    
    i: \(\lambda \in Conv(W\cdot\mu)\)
    
    ii: \(\mu -\lambda =\) integer linear combination of roots.
\end{theorem}

Konstant multiplicity formula gives the multiplicity of \(\lambda\). Example: \(\mathfrak{g} = \mathfrak{sl}(2,\mathbb{C}), m\)-non-negative integer = irreducible representation of \(\mathfrak{sl} (2,\mathbb{C})\) of dim \(m+1\)    

\(SL(2,\mathbb{C})\)-acts on \(\mathbb{C}^2\) by multiplication by \(g = \begin{pmatrix}
    a &  b \\
    c &  d \\
\end{pmatrix}\in SL(2,\mathbb{C}), \binom{z_1}{z_2}\in\mathbb{C}^2\)

\(g\binom{z_1}{z_2} = \binom{az_1 + bz_2}{cz_1 + dz_2}\) 

\(V = \{ \text{all polynomial functions} f:\mathbb{C}^2 \to \mathbb{C}  \} \) 

\(SL(2,\mathbb{C})\) acts on \(V\) by \([\pi(g)f] = f(g ^{-1}  z)\)  

\(V_m\) is homogenous polynomials on \(V\) of degree \(m\)

so, \(\dim V_m=m+1\) 

Different produces representation \((\pi_m, V_m)\) of \(\mathfrak{sl}(2,\mathbb{C})\)

Weights of \(\pi_M, V_M\) of \(\mathfrak{sl}(2,\mathbb{C}) \)  

\(m-1, m-3,\cdots, -m+3, -m+1\in\) Conve hull of \(\{ m, -m \}\), integral, but not weights of \(V_m\)  

So ii was not satisfied.

\underline{Lemma}: Suppose \((\pi, V)\) is a finite dimensional representation of \(\mathfrak{g}\) and \(\lambda\) is a weight of \(V\) and \(\alpha\) root of \(\mathfrak{g}\) such that \(\langle \lambda ,\alpha  \rangle > 0 \)     

Let \(k= \langle \lambda ,H^\alpha \rangle = 2 \frac{\langle \lambda , \alpha \rangle }{\langle \alpha ,\alpha \rangle }\) 

Then each \(\lambda - j_\alpha\) is a weiht of \(V\) for each integers, \(j\), \(0 \leq j \leq k\)

In part, \(\lambda - \alpha\) os a weight of \(V\)  

\begin{proof}
    \(\mathfrak{s}^\alpha = \{ X_\alpha ,Y_\alpha ,H_\alpha  \} \cong \mathfrak{sl}(2,\mathbb{C})\).
    
    Let \(U = \) the subspae of \(V\) spanned by weight spaces with weights of the form \(\lambda - j \alpha\). \(X_\alpha , Y_\alpha\) shifts weights by \(\pm \alpha \implies U\) is \(s^\alpha\)-invariant.
    
    Since \(\langle \alpha , H_\alpha  \rangle = 2, \langle \lambda - j \alpha , H_\alpha  \rangle = k-2j  \) 

    The weight space of \(V\) corresponding to \(\lambda - j \alpha  =\) eigenspace for \(\pi(H_{\alpha} )\) inside \(U\) corresponding to \(k - 2j\)   

    From the theory of finite dimensional representations of \(\mathfrak{sl}(2,\mathbb{C} )\), when \(0 \leq j \leq k\) these \(\pi(H_\alpha)\) eigenspaces are \(\neq 0\)    

    Conclude: each \(\lambda - j \alpha\) is a weight of \(V\). 

\end{proof}

\begin{proposition}
    Let \(\mu\) be dominant integral. Suppose \(\lambda\) is dominant, \(\lambda \preceq \mu\), and \(\mu - \lambda\)  can be expressed as an integer combination of roots. Then \(\lambda\) is a weight of \((\pi_\mu, V_\mu)\) - the irreducible representation of highest weight \(\mu\).     
\end{proposition}

\begin{proof}
    \(\mu = \lambda + \sum_{j=1}^r k_j \alpha_j, \Delta = \{ \alpha_1,\cdots, \alpha_r \} \)
    
    Since \(\lambda \preceq \mu\) we deduce \(k_j\geq 0\) 
    
    Let \(P = \{ \eta = \lambda + \sum_{j = 1} ^ r l_j \alpha_j ; l_j\in \mathbb{Z} , 0 \leq l_j \leq k_j  \} \) 
    
    `discrete parallelopiped'

    \underline{Caution}: \(P\) need not lie inside \(Conv(W\cdot\mu)\) so not every element of \(P\) must be a weight of \(V_\mu\) [book fig 10.3]
    
    Notation: \(L(\eta)= \sum_{j} l_j\) 

    Think of it as the taxicub distance from \(\eta\) to \(\lambda\)
    
    Clearly \(L(\eta)\in\mathbb{Z}_{\geq 0}\) 

    Starting with \(\eta \in \mu\) we can show that \(\exists \eta ^{\prime} \in P\) such that \(\eta ^{\prime} \) is also a weight of \(V_\mu\) and \(L(\eta ^{\prime} ) < L(\eta)\) .

    \(\eta = \lambda + \sum_{j} l_j \alpha_j\)
    
    \(0 < \langle \sum_{j} l_j \alpha_j, \sum_{k} l_k \alpha_k \rangle = \sum_{k} l_k \langle \sum_{j} l_j \alpha_j, \alpha_k \rangle \) 

    So there must be at least one strictly positive term.

    So, there exists simple root \(\alpha_k\) such that \(l_k > 0, \langle \sum_{j} l_j \alpha_j, \alpha_k \rangle > 0\)
    
    Then \(\langle \eta , \alpha_k \rangle = \underbrace{\langle \lambda, \alpha_k \rangle}_{\geq 0} + \underbrace{\langle \sum_{j} l_j \alpha_j, \alpha_k \rangle }_{>0} > 0 \) 

    Previous lemma; \(\eta ^{\prime} = \eta - \alpha_k\)-weight of \((\pi_\mu,V_\mu)\)  

    We basically have \(l_j^{\prime} = l_j, l_k^{\prime} = l_k - 1\) 

    So distance drops by \(1\) and we still have a weight. Keep dropping and we see that \(\lambda\) is actually a weight of \(\pi_\mu , V_\mu\)  

\end{proof}

\hrulefill

Class 22: 03/28

\((\pi_\mu,V_\mu)\)-irreducible finite-dimensional representation of \(\mathfrak{g}\) of highest weight \(\mu\).

\begin{theorem}
    An integral element \(\lambda\) is a weight of \(V_\mu\) if and only if these conditions are satisfied:
    
    i: \(\lambda \in Conv(W\cdot\mu)\)
    
    ii: \(\mu - \lambda =\) integer linear combination of roots

\end{theorem}

\begin{proposition}
    Let \(\mu\) be a dominant integral element. Suppose that the elemeent \(\lambda\) is dominant, \(\lambda \preceq \mu\) and \(\mu - \lambda\) can be expressed as an integer combination of roots.
    
    Then \(\lambda\) is a weight of \((\pi_\mu, V_\mu)\)  
\end{proposition}

\begin{proof}
    (of the theorem)

    Let \(V^{\prime} \subset V_\mu\) be spanned by all weigt vectors different from \(\mu\) by an integer linear combination of roots.
    
    \(V^{\prime}\) contains \(v_0\) [the highest weight vector], \(\mathfrak{g}\)-invariant.
    
    \(V_\mu\) irreducible \(\implies V^{\prime} =V_\mu\)

    \(\implies\) every weight of \(V_\mu\) satisfies ii.
    
    If \(\lambda\) is a weight of \(V_\mu\) so is each \(w\cdot \lambda, w\in W\) and eac \(w\cdot \lambda \preceq \mu\).
    
    Proposition 8.44 implies:

    \(\lambda \in Conv(W\cdot\mu)\)
    
    This is the easy part. Hard part: showing \(\lambda\) satisfying the conditions is a weight vector.
    
    Conversely, suppose \(\lambda\) satisfies conditions i and ii.
    
    Choose \(w\in W\) so that \(\lambda ^{\prime} = w\cdot \lambda\) is dominant. Then \(\lambda ^{\prime} \in Conv(W\cdot\mu)\)
    
    We need to show that \(\lambda ^{\prime} \) is integral and \(\lambda - \lambda ^{\prime} \) is an integer linear combination of roots.
    
    \(W\) is generated by reflections \(s_\alpha\)s.
    
    Write \(w\) as composition of reflections

    Work with one reflection at a time.

    \(\lambda - s_\alpha \cdot \lambda = \underbrace{\langle H_\alpha , \lambda \rangle}_{\in\mathbb{Z}} \alpha  \) 
    
    Since \(\lambda\) is integral.
    
    So, \(s_\alpha \cdot \lambda\) is integral and \(\lambda - s_\alpha \cdot \lambda\) is an integer linear combination of roots.
    
    By inducting on the number of reflections, \(\lambda^{\prime} \) is integral and \(\lambda - \lambda ^{\prime} \) is an integer linear combination of roots.
    
    So, \(\mu - \lambda ^{\prime} = (\mu - \lambda )+(\lambda - \lambda ^{\prime} )\) equals integer linear combination of roots.
    
    \(\implies \lambda ^{\prime}\) satisfies condition ii and condition of the proposition.
    
    \(\implies \lambda ^{\prime} \) is a weight \(\implies  \lambda = w ^{-1} \cdot \lambda ^{\prime} \) is a weight.  
    
\end{proof}

\section*{Casimir Element (10.2)}

Let \(\mathfrak{g} = \mathfrak{k}_\mathbb{C}\) semisimple lie algebra

\(\mathfrak{k}\) lie algebra of compact group \(K\) 

\(\langle , \rangle \), which is \(\Ad_K\)-invariant 

Let \(\{ X_1,\cdots, X_n \} \) be an orthonormal basis for \(\mathfrak{k}\). The Casimir element \(C\in \mathcal{U}(\mathfrak{g})\) is:

\[
    C = - \sum_{j} X_j ^2
\]

\begin{proposition}
    i: \(C\) is indpendent of the coice of the orthonormal basis for \(\mathfrak{k}\)
    
    ii: \(C\) belongs to center of \(\mathcal{U}(\mathfrak{g})\). 
\end{proposition}

\begin{proof}
    i: Let \(\{ Y_1,\cdots,Y_n \}\) be another orthonormal basis for \(\mathfrak{k}\). Tere exists orthogonal matrix \(R\) such that \(Y_j = \sum_{k} R_{kj}X_k\)
    
    \[
        \sum_{j} Y_j^2 = \sum_{j,k,l} R_{kj}X_k R_{l_j}X_l
    \]

    \[
        =\sum_{k,l} R_{kj}(R^{tr}_{jl})X_k X_l 
    \]

    \[
        = \sum_{k,l} \delta_{k,l} X_k X_l = \sum_{k} X_k^2 
    \]

    ii: \(\mathcal{U}(\mathfrak{g})\) is generated by \(\mathfrak{g}\). So it is sufficient to show that \(C\) commutes with each basis element \(X_k\). We use structure constant of the lie algebra:

    \([X_j,X_k] = \sum_{l} c_{jk}^l X_l\)
    
    The matrix of \(\ad_{X_j}\) has \(lk\) entry \(c_{jk}^l\). So it is a skew-symmetric matrix, since \(\langle , \rangle\) is \(\Ad_K\) invariant.

    Now, \(-[X_j,C]=\sum_{k} [X_j,X_k^2]=\sum_{k} ([X_j,X_k]X_k + X_k[X_j,X_k])=\sum_{k,l} c_{jk}^l X_l X_k + \sum_{k,l} c_{jk}^l X_k X_l=\sum_{k,l}c^l_{jk}X_l X_k - \sum_{k,l} c^{l}_{jk}X_l X_k = 0\)

\end{proof}

\underline{Comments}:

Possible to define \(C\) without reference to \underline{any basis} of \(\mathfrak{g}\), since it is basis independent.

Another \underline{practical} way of defining \(C\) is:

Let \(\{ X_1,\cdots,X_n \} \) be any basis for \(\mathfrak{g}\) [not of \(\mathfrak{k}\), not necessarily orthonormal] and let \(\{ X^1,\cdots,X^n \} \) to be the dual basis [defined relative to the Killing Form, \(K(X_i,X^j)=\delta_{i j}\)].

Recall: \(K(X,Y)=\Tr(\ad X \ad Y)\) 

Then \(C = \sum_{j} X_j X^j\)

Example: \(\mathfrak{g} = \mathfrak{sl}(2,\mathbb{C})\) 

\(\left\{ H = \begin{pmatrix}
    1 &  0 \\
    0 &  -1 \\
\end{pmatrix}, X = \begin{pmatrix}
    0 &  1 \\
    0 &  0 \\
\end{pmatrix}, Y = \begin{pmatrix}
    0 &  0 \\
    1 &  0 \\
\end{pmatrix} \right\} \) 

Orthogonal relations:

\newcommand{\tr}{\operatorname{tr}}

\(K(H,H)=\tr \begin{pmatrix}
    0 & 0 &  0 \\
    0 & 2^2 &  0 \\
    0 & 0 &  (-2)^2 \\
\end{pmatrix}=8\) 

\(K(X,Y)=K(Y,X)=\tr \begin{pmatrix}
    2 & ? &  ? \\
    ? & 2 &  ? \\
    ? & ? &  0 \\
\end{pmatrix}=4\) 

So, dual basis is \(\frac{1}{8}H, \frac{1}{4}Y, \frac{1}{4}X\)

So, casimir is \(\frac{1}{8}H^2 + \frac{1}{4}XY + \frac{1}{4}YX \) 

This is \(\frac{1}{4}\) the previous casimir. 

all other pairings are zero.

Recall: \(X \mapsto X^{\ast} : \mathfrak{g} = \mathfrak{k}_\mathbb{C}, (X_1 + iX_2)^{\ast} = - X_1 + i X_2\)

\(x_1,x_2\in \mathfrak{k}\)

\begin{proposition}
    [7.18] If \(X\in \mathfrak{g}_{\alpha}\) then \(X^{\ast} \in \mathfrak{g}_{-\alpha}\)
    

    
\end{proposition}

\underline{Lemma}: Let \(X\in \mathfrak{g}_{\alpha}\) be a unit vector. THen \([X,X^{\ast}]=\alpha\)

\begin{proof}
    Lemma 7.22 means \(\langle [X,X^{\ast}], H_\alpha \rangle = \langle \alpha , H_\alpha \rangle \langle X^{\ast} ,X^{\ast} \rangle   = \langle \alpha ,H_\alpha \rangle =2\) 

    theorem 7.19 implies \([\mathfrak{g}_\alpha , \mathfrak{g}_{-\alpha} ]=\mathbb{C} H_\alpha\) 

    So \([X,X^{\ast}]\) is a multiple of \(H_\alpha\)
    
    Looking at the previous equation, we deduce that \([X,X^{\ast}]=\alpha\) 

\end{proof}

\begin{proposition}
    Let \((\pi , V)\) be a finite-dimensional irreducible representation of \(\mathfrak{g}\) of highest weight \(\mu\). Extend it to \(\mathcal{U}(\mathfrak{g})\). Then \(\pi(C)=-\sum_{j} \pi(X_j)^2 = c_\mu I\)
    
    Where \(c_\mu = \langle \mu + \delta , \mu + \delta \rangle - \langle \delta ,\delta  \rangle  \) [real constant].

    Furthermore, \(c_\mu \geq 0\) and \(c_\mu=0\) only if \(\mu =0  \)  
\end{proposition}

\begin{proof}
    Since \(C\) is in the center of \(\mathcal{U}(\mathfrak{g})\), \(\pi(C):V\to V\) is an intertwining map. Since \(V\) is irreducible, by Schur's lemma, this must be multiplication by a constant \(c_\mu\) 

    Hard part: finding the value of the scalar.

    To find value of \(c_\mu\) we evaluate \(C\) on the highest weight vector \(v_\mu \in V\)
    
    Firstly, \(\pi(C)v_\mu =c_\mu v_\mu\) 

    Choose orthonormal basis for \(\mathfrak{k}\) as follows: \(\{ H_1. \cdots, H_r \} \)-orthonormal basis for \(\mathfrak{t}\)
    
    For each \(\alpha \in R^+\) choose unit vector \(X_\alpha \in \mathfrak{g}_\alpha\)
    
    Then \(X_\alpha^{\ast}\) is a unit vector in \(\mathfrak{g}_{-\alpha}\)
    
    Consider: \(Y_\alpha = \frac{1}{i \sqrt{2} }(X_\alpha +X_\alpha ^{\ast})\)
    
    \(Z_\alpha = \frac{1}{\sqrt{2}}(X_\alpha - X_\alpha ^{\ast} )\) 

    Satisfies \(Y_\alpha^{\ast} =-Y_\alpha , Z_\alpha ^{\ast} = -Z_\alpha\)
    
    So they belong to \(\mathfrak{k}\)
    
    Root spaces corresponding to different roots are orthogonal, aka \(\mathfrak{g}_\alpha {\perp} g_\beta \) if \(\alpha \neq \beta\) so these vectors are orthonormal.
    
    So, \(\{ H_1,\cdots,H_r,Y_\alpha ,Z_\alpha \} \) is orthonormal basis.

    We use this to compute \(c_\mu\)

    \(Y_\alpha^2 = - \frac{1}{2}(X_\alpha^2 + X_\alpha X_\alpha ^{\ast} + X_\alpha ^{\ast} X + X_\alpha ^{\ast 2})\)
    
    \(Z_\alpha^2 = \frac{1}{2}(X_\alpha ^2 - X_\alpha X_\alpha ^{\ast} -X_\alpha ^{\ast} X_\alpha +X_\alpha ^{\ast 2} )\)
    
    So, \(- Y_\alpha ^2 - Z_\alpha ^2 = X_\alpha X_\alpha ^{\ast} +X_\alpha^{\ast} X_\alpha = 2 X_\alpha ^{\ast} X_\alpha +[X_\alpha ,X_\alpha^{\ast} ]=2X_\alpha ^{\ast} X_\alpha + \alpha\)
    
    So, \(C = \sum_{\alpha \in R^+} (2 X_\alpha ^{\ast} X_\alpha + \alpha) - \sum_{j=1} ^ r H_j^2 \) 

    Note that \(X_\alpha\) annihilates the highest weight vector.
    
    So \(\pi(C)v_\mu = \sum_{\alpha \in R^+} \pi(\alpha)v_\mu - \sum_{j=1} ^r \pi(H_j)^2 v_\mu \) 

    \(= \sum_{\alpha \in R^+} \langle \mu , \alpha \rangle v_\mu - \sum_{j} \langle \mu , H_j \rangle ^2 v_\mu   \) 

    proposition 7.14 tells us: each \(\alpha\in i \mathfrak{t}\) are pure imaginary. So \(\mu \in i \mathfrak{t} \) 

    \(\implies \sum_{j} \langle \mu , H_j \rangle = - \langle \mu , \mu  \rangle  \) 

    Finish the rest later

\end{proof}

\hrulefill

Class 23: 04/02

\(\{ X_1,\cdots,X_n \}\) orthonormal basis for \(\mathfrak{k}\) and \(\mathfrak{g} = \mathfrak{k}_\mathbb{C}\)

Then \(C = - \sum_{j} X_j^2\in \mathcal{U}(\mathfrak{g})\) is the casimir element.

This definition is not satisfactory since \(C\) is independent of basis.

Note, Casimir is important because it is in the center, it commutes with everything.

\begin{proposition}
    Let \((\pi ,V)\) be finite dimensional irreducible representation of \(\mathfrak{g}\) of highest weight \(\mu\). Then \(\pi(C)=-\sum_{j} \pi (X_j)^2=c_\mu I\)
    
    Where \(c_\mu = \langle \mu + \delta , \mu + \delta \rangle - \langle \delta , \delta \rangle  \) real constant
    
    \(\delta = \frac{1}{2}\sum_{\alpha \in R^+} \alpha\) .

    Furtermore, \(c_\mu \geq 0\) with \(c_\mu = 0\) if and nly if \(\mu =0\)  
\end{proposition}

\begin{proof}
    We go back to what we did later.

    \(\pi(C)\) is some scalar \(c_\mu I\).

    We want to compute \(c_\mu\). Apply \(\pi(C)v_\mu = c_\mu v_\mu\) where \(v_\mu\) is the highest weight vector of \(V\).

    Choose an orthonormal basis for \(\mathfrak{t} \subset \mathfrak{k}\) [CSA \(\mathfrak{h} = \mathfrak{t}_\mathbb{C}\) ]
    
    Let the basis be \(\{ H_1,\cdots, H_r \}\) 

    \(\pi(C)v_\mu = \underbrace{\sum_{\alpha \in R^+} \langle \mu , \alpha \rangle}_{2\langle \mu , \delta \rangle } v_\mu \underbrace{-\sum_{j=1} ^r \langle \mu , H_j \rangle ^2}_{\langle \mu , \mu \rangle } v_\mu  \) 

    proposition 7.14 implies for \(\alpha \in i \mathfrak{t} \subset \mathfrak{h} =>\mu \in i \mathfrak{t} \implies \langle \mu , H_j \rangle \in i \mathbb{R}  \) 

    \(\{ H_1,\cdots, H_r \}\) -orthonormal basis for \(\mathfrak{t} \implies \sum_{j} \langle \mu , H_j \rangle ^2 = - \langle \mu , \mu \rangle  \) 
    
    \(\langle \mu , \mu \rangle + 2 \langle \mu , \delta \rangle = \langle \mu  + \delta , \mu + \delta \rangle - \langle \delta , \delta  \rangle  \) 

    So we're done.

    Also, \(2 \langle \mu , \delta \rangle \geq 0 \) and \(\langle \mu , \mu \rangle \geq 0 \) with \(0\) equality iff \(\mu =0\)

\end{proof}

\underline{Infinitesimal Characters}

\(Z(\mathcal{U} (\mathfrak{g}))\) is the center of \(\mathcal{U}(\mathfrak{g})\)  

\underline{Lemma}: Let \((\pi , V)\) be an irreducible finite dimensional representation of \(\mathfrak{g}\). Then each \(z\in Z(\mathcal{U}(\mathfrak{g}))\) acts on \(V\) by a scalar multiple of identity \(\pi(z)=\chi(z)\cdot Id_v, \chi(z)\in \mathbb{C}\) by schur's lemma.

\(\chi : Z(\mathcal{U}(\mathfrak{g}))\to\mathbb{C}\) is an algebra homomorphism.

\(\chi(1)=1, \chi(z_1 + z_2)=\chi(z_1)+\chi(z_2), \chi(z_1 z_2)=\chi(z_1)\chi(z_2)\).

\begin{definition}
    If \((\pi,V)\) is a reprsentation of \(\mathfrak{g}\) (not necessarily finite-dimensional or irreducible) and \(\chi\) is an algebraic homomorphism \(Z(\mathcal{U}(\mathfrak{g}))\to \mathbb{C}\) such that \(\pi(z)v=\chi(z)v\) for all \(z\in Z(\mathcal{U}(\mathfrak{g}))\) for all \(v\in V\), then \(\chi\) is called the \underline{infinitesimal character} of \((\pi , V )\)        
\end{definition}

\section*{Complete Reducibility}

\begin{theorem}
    Every finite dimensional representation \((\pi, V)\) of a complex semisimple lie algebra \(\mathfrak{g}\) is completely reducible (direct sum of irreducible subrepresentations)  
\end{theorem}

\underline{Traditional Proof}: \(\mathfrak{g} = \mathfrak{k}_\mathbb{C}\). The group \(K\) can be chosen to be connected, simply connected.

Then \((\pi , V)\) `lifts' to a representation of \(K\) and can be given unitary structure.

Then \((\pi, V)\) is completely reducible as a representation of \(K\) (if \(U \subset V\) is \(K\)-invariant subspace, then so is \(U ^{\perp} \), \(V = U \oplus U ^{\perp} \)  ) 

\((\pi, V)\) ompletely reducibleas a representation of \(\mathfrak{g}\).

Hard part: connected simply connected compact \(K\) exists. This is very technical, and we need analytic methods. This poses a problem when we don't have access to analysis eg when we are working in finite fields etc.

In the textbook there is a pure lie algebraic proof using casimir elements.

\underline{Lemma}: If \((\pi,V)\) is \(1\)-dim representation of \(\mathfrak{g}\) then \(\pi(X)=\) for all \(X\in \mathfrak{g}\)   

\begin{proof}
    Theorem 7.8 implies \(\mathfrak{g}\) is direct sum of simple algebras \(\mathfrak{g}_i\) with dimension at least \(2\). Restriction of \(\pi|_{\mathfrak{g}_i}:\mathfrak{g}_i \to End(V) \cong \mathbb{C}\), 1-dim. \(\ker \pi |_{\mathfrak{g}_i}\neq \{ 0 \} \)
    
    \(\mathfrak{g}_i\) simplemeans \(\ker \pi |_{\mathfrak{g}_i}= \mathfrak{g}_i\) 

    So, \(\pi |_{\mathfrak{g}_i}=0\)
    
    So, \(\pi = 0\) on \(\mathfrak{g}\)  
    
\end{proof}

\underline{Lemma}: Suppose \((\pi , V)\) is a finite dimensional representation of \(\mathfrak{g}\) and that \(W \subset V\) is an invariant subspace of codimension \(1\). Then \(V\) decomposes as \(W \oplus U\) for some invariant subspace \(U\) of \(V\).

\begin{proof}
    Case 1: \(W\) is irreducible. If \(\dim W = 1\) then by lemma, \(\pi|_W = 0, \dim V = 2\). \(\dim \{ A\in End(V);A|_W = 0 \} = 2 \)

    It follows that \(\pi = 0\) on \(V\).
    
    \(\mathfrak{g} = \oplus\) simple algebras \(\mathfrak{g}_i\)
    
    \(\dim \mathfrak{g}_i \geq 3\) since each \(\mathfrak{g}_i\) contains a copy of \(\mathfrak{sl}(2,\mathbb{C})\) which has dimension \(3\) 
    
    So, \(\pi|_{\mathfrak{g}_i}\) has domain \(\mathfrak{g}_i\) of dimension \(\geq 3\) and range \(2\).
    
    So it has non-trivial kernel.

    \(\mathfrak{g}_i\) simple implies \(\pi|_{\mathfrak{g}_i}=0\)
    
    So \(\pi =0\) on \(\mathfrak{g}\)  
   
    Let \(U\) be any vector space complement of \(W\). \(\underbrace{V}_{2-\dim}=\underbrace{W}_{1-\dim}\oplus\underbrace{U}_{1-\dim}\) 
    
    Second subcase: assume \(\dim W > 1, W\) irreducible. Consider casimir element \(C\in \mathcal{U}(\mathfrak{g})\). Get \(\pi(C)\) acting on \(V\).
    
    \(\pi(C)|_W\) is multiplication by a scalar \(c\).
    
    Since \(\dim W \neq 1\) we have \(c \neq 0\)
    
    On the other hand, \(V / W\) is a trivial one dimensional representation.
    
    Then \(\{ w_1,\cdots, w_m, u \} \)-basis for \(V\) 

    \(\pi(C)\) in this basis has matrix:
    
    \[
        \begin{bmatrix}
            c & 0 & \cdots & 0 & \ast \\
            0 & c & \cdots & 0 & \ast \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & \cdots & c & \ast\\
            0 & 0 & \cdots & 0 & 0  \\
        \end{bmatrix}
    \]

    det \(0\) so \(\pi(C)\) has non-trivial kernel.
    
    Let \(U = \ker \pi(C)\).

    \(U\) is \(\mathfrak{g}\)-invariant. \(W\cap U = \{ 0 \} \)  

    So, \(V = W \oplus U\)
    
    Case 2: \(W\) not irreducible.

    \(W\) has a non-trivial subspace \(W^{\prime}  \). Use induction on \(\dim V\).
    
    \(W / W^{\prime}\) is a codimension \(1\) invariant subspace of \(V / W^{\prime} \)
    
    By induction, \(\exists\) invariant complement \(Y / W^{\prime} \) where \(Y\) is an invariant subspace of \(V\). \(W^{\prime}\) is a codimension \(1\) invariant subspcae of \(Y\). \(\dim Y < \dim V\), by induction, \(Y = W \oplus U\). \(U\)-invariant \(1\)-dimensional subpsace of \(Y\).
    
    \(Y\cap W = W^{\prime} \implies U \cap W = \{ 0 \} \) and \(V = W \oplus U\)  

\end{proof}

\underline{Proof of Complete Reducibility} 

Sufficient to prove that each invariant subspace \(W \subset V\) has an invariant complement \(U \subset V\) such that \(V = W \oplus U\) by recursion.

Let \(W \subset V\) be an invariant subspace. Suppose we can find an intertwining operation \(A: V \to W\) such that \(A|_W =\) non-zero multiple of identity on \(W\).

\(U = \ker A\) is invariant, \(U \cap W = \{ 0 \}, \dim U = \dim V - \dim W\)  

The remainder of the proof is just construction of the operator.

\hrulefill

Class 24, 25: 04/04, 04/09 skipped

\hrulefill

Class 26: 04/11

\section*{Maximal Tori}

\begin{definition}
    A subgroup \(T\) of \(K\) is a \underline{torus} if \(T\) is isomorphic to \((S^1)^k\) for some \(k\).

    A subgroup \(T\) of \(K\) is a \underline{maximal torus} if it is a torus and it is not properly contained in any other torus of \(K\).
\end{definition}

Example: if \(K=SU(n)\) then \(T =\) subgroup of diagonal elements of \(SU(n)\). Then \(T\) is a torus of dimension \(n-1\).

It is maximal torus because \(\{ g\in SU(n); gt=tg  \forall t\in T \} = T\). So, the centralizer of \(T\) is \(T\).

HW P1 hint: Start with \(\mathfrak{g} = \mathbb{C}\).

Back to the lecture.

If \(g\in SU(n)\) has off diagonal entry \(a_{ij}\) then multiplying it with diagonal \(t_k\) matrix, we see that we need \(a_{ij}t_i=a_{ij}t_j\) from \(gt=tg\) which is not always true so it is indeed maximal abelian.

\begin{proposition}
    If \(T\) is a max torus, the lie algebra \(\mathfrak{t}\) of \(T\) is a maximal commutative subalgebra of \(\mathfrak{k}\).
    
    Conversely, if \(\mathfrak{t}\) is a maximal commutative subalgebra of \(\mathfrak{k}\) then the connected lie subgroup of \(K\) with lie algebra \(\mathfrak{t}\) is a maximal torus.     
\end{proposition}

\underline{Restatement}: we have a bijection: maximal tori of \(K\) and maximal commutative lie subalgebra of \(\mathfrak{k}\)  

There is \underline{no bijection} between tori of \(K\) and commutative subalgebra of \(\mathfrak{k}\). We need maximal.

If we start with a torus, its lie algebra is commutative. But if we start with a commutative subalgebra we might not end up with a torus. For example, any one dimensional subalgebra is automatically commutative. But the one-parameter subgroup generated by that need not be compact.

key ingredient of the proof:

\begin{proposition}
    [5.24] Suppose \(G \subset GL(n,\mathbb{C})\) is a matrix group with lie algebra \(\mathfrak{g}\) and \(\mathfrak{h}\) is a maximal commutative subalgebra of \(\mathfrak{g}\). 
    
    Then, the connected lie subgroup \(H\) of \(G\) with lie algebra \(\mathfrak{h}\) is \underline{closed}. 
\end{proposition}

\begin{proof}
    If \(T\) is a maximal torus, \(T\) is commutative so \(\mathfrak{t}\) is commutative. If \(\mathfrak{t}\) is not maximal commutative, then \(\mathfrak{t} \subsetneq S^1\) where \(S^{\prime}\) is commutative.
    
    \(S^{\prime} \subseteq S\) where \(S\) is maximal commutative.
    
    So, \(\mathfrak{t} \subset S\) and thus lie subgroup with lie algebra \(\mathfrak{s}\) where \(S \supset T\).
    
    \(S\)-commutative, closed, compact.

    Theorem 11.2 implies \(S\) is a torus.

    So, \(S=T\) and thus \(\mathfrak{s} = s^{\prime} = \mathfrak{t}, \mathfrak{t} \) is max commutative.
    
    This is one direction. Other direction is easy.

    Conversely, if \(\mathfrak{t}\) is a maximal commutative algebra, let \(T \subset K\) be the connected Lie subgroup with lie algebra \(\mathfrak{t}\).

    \(T\) is commutative, closed, compact.
    
    Theorem 11.2 implies \(T\) is a torus. We want to show that it is a maximal torus.

    If \(T\) is contained in a bigger torus \(S\) then \(\mathfrak{t} \subset \mathfrak{s}\). Since \(\mathfrak{t}\) is maximal commutative, we have \(\mathfrak{t} = \mathfrak{s}\). Since both \(T\) and \(S\) are connected, \(T=S\). So, \(T\) is maximal.   
\end{proof}

\begin{theorem}
    (Torus Theorem) If \(K\) is a connected compact (matrix) lie group, then:

    i: If \(S\) and \(T\) are max tori in \(K\), \(\exists x\in K\) such that \(T = xSx ^{-1}\)
    
    ii: Every element of \(K\) is contained in some max torus.
\end{theorem}

There is often a part iii: every torus is contained in some max torus. Proof is easy: look at lie algebra, it's contained in some other max comm lie algebra and going back to lie group gives us the result.

We accept the torus theorem for now. Proof needs diffgeo.

\underline{Corollary}: If \(K\) is a compact connected (matrix) lie group, the exponential map \(\exp: \mathfrak{k} \to K\) is onto.

\begin{proof}
    Pick \(x\in K\). Choose a max torus \(T_x\) containing \(x\).

    \(\exp: \mathfrak{t}_x \to T_x\) is onto.
    
    So, \(x=\exp(X), X\in \mathfrak{t}_x \subset \mathfrak{k}\) 

    So, onto.

\end{proof}

We move onto Weyl group.

\section*{Weyl Group (11.2, 11.7)}

Let \(T \subset K\) be a max torus with lie algebra \(\mathfrak{t}\).  

The \underline{normalizer} of \(T\) is:

\[
    N(T)=\{ x\in K ; x T x ^{-1} \subset T \} 
\]

Then \(T\) is a normal subgroup of \(N(T)\) 

\begin{definition}
    The quotient group \(W = N(T) / T\) is called the \underline{Weyl Group} of \(K\)  
\end{definition}

This is `same' as the group generated by reflections \(s_\alpha, \alpha \in R\).

If \(x\in N(T)\) acts on \(T\) by \(x\cdot t = x t x ^{-1}\) where \(t\in T\)

We get a map \(N(T)\to Aut(T)\).

Also, \(T \subset \ker\) of this map, since commutative.

So, \(W = N(T) / T\) acts on \(T\) ``by conjugation''

This induces action of \(W\) on \(\mathfrak{t}\) by orthogonal linear transformation.

\(w\cdot H = Ad_x (H)\) where \(H\in \mathfrak{t}\) and \(x\in N(T)\) is any representative of \(w\in W\).

\(W = N(T) / T \to O(\mathfrak{t})\) 

At this point we do not know yet that this map is 1-1.

\begin{definition}
    An element \(\alpha \in \mathfrak{t}\) is a \underline{real root} of \(\mathfrak{g} = (\mathfrak{k})_\mathbb{C}\) with respect to \(\mathfrak{t}\) if \(\alpha \in 0\) and \(\exists X\in \mathfrak{g}, X \neq 0\) such that \([H,X]=i \langle \alpha , H \rangle X\) for all \(H\in \mathfrak{t}\).
    
    For each real root \(\alpha\) define associative \underline{real coroot}: \(H_\alpha = 2 \frac{\alpha}{\langle \alpha , \alpha  \rangle }\in \mathfrak{t}\) 

    If \(K\) is simply connected, \(\mathfrak{g}=\mathfrak{k}_\mathbb{C}\) is semisimple (proposition 7.7).

    In general, \(\mathfrak{k} = \mathfrak{z} \oplus \mathfrak{k}_{s s}\) where \(\mathfrak{z}\) is the center of \(\mathfrak{k}\) and \((\mathfrak{k}_{s s})_\mathbb{C}\) is semisimple.   
    
\end{definition}

The results of chapter 7 apply to real roots except one exception.

Real roots \(\alpha\) do not span \(\mathfrak{t}\) when \(\mathfrak{z} \neq \{ 0 \}\).

Instead, \(\mathfrak{t} = \mathfrak{z} \oplus \mathbb{R}\)-span of roots.

\begin{proposition}
    For each real root \(\alpha \in R\) there is an \(x\in N(T)\) such that \(Ad_x(H_\alpha)=-H_\alpha\) and \(Ad_x(H)=H\) for all \(H\in \mathfrak{t}\) such that \(\langle \alpha ,H \rangle = 0 \).
    
    Thus, the adjoint action of \(x\) on \(\mathfrak{t}\) is the reflection \(s_\alpha\).  
\end{proposition}

\underline{Motivation for the Proof}

Suppose \(K=SU(2)\)

\(\mathfrak{t} = \begin{pmatrix}
    it &  0 \\
    0 &  -it \\
\end{pmatrix}\)

\(x = \begin{pmatrix}
    0 &  1 \\
    -1 &  0 \\
\end{pmatrix}\in SU(2)\)

\(x \begin{pmatrix}
    it &  0 \\
    0 &  -it \\
\end{pmatrix}x ^{-1} = \begin{pmatrix}
    -it &  0 \\
    0 &  it \\
\end{pmatrix}\)

\(\exp \begin{pmatrix}
    0 &  s \\
    -s &  0 \\
\end{pmatrix}=\begin{pmatrix}
    \cos s &  \sin s \\
    -\sin s &  \cos s \\
\end{pmatrix}\)

\(\begin{pmatrix}
    0 &  s \\
    -s &  0 \\
\end{pmatrix}\in \mathfrak{su}(2), \begin{pmatrix}
    0 &  s \\
    -s &  0 \\
\end{pmatrix}^2 = \begin{pmatrix}
    -s^2 &  0 \\
    0 &  -s^2 \\
\end{pmatrix}\)

\(s = \frac{\pi}{2}: \exp \begin{pmatrix}
    0 &  \frac{\pi}{2} \\
    -\frac{\pi}{2} & 0  \\
\end{pmatrix}=x\) 

\(-x\) works too.

\(\exp \begin{pmatrix}
    0 &  \frac{\pi}{2} \\
    -\frac{\pi}{2} & 0  \\
\end{pmatrix}\) also works

For \(K = SU(2) / \{ \pm I \} \) 

Now, for the proof.

\begin{proof}
    Choose \(X_\alpha , Y_\alpha\) as in theorem 7.19 with \(Y_\alpha = X_\alpha ^{\ast}\) (So that \(X_{\alpha}, Y_\alpha, [X_\alpha, Y_\alpha]=-i H_\alpha \)) is a lie algebra isomorphic to \(\mathfrak{sl}(2,\mathbb{C})\).
    
    Then \((X_\alpha - Y_\alpha)^{\ast} =-(X_\alpha - Y_\alpha )\implies X_\alpha - Y_\alpha \in \mathfrak{k} \)
    
    Define \(x\in K\) by \(x = \exp (\frac{\pi}{2}(X_\alpha - Y_\alpha))\)
    
    Then \(Ad_x(H)=\exp(\frac{\pi}{2}(ad_{X_\alpha}-ad_{Y_\alpha}))(H)\) for all \(H\in \mathfrak{t}\).
    
    If \(\langle \alpha , H \rangle = 0 \) then \((ad_{X_\alpha}-ad_{Y_\alpha})(H)=0\) and \(Ad_x(H)=H\) 
    
    \(ad_{X_\alpha}-ad_{Y_\alpha}\) has eigenvectors \(X_\alpha - Y_\alpha, X_\alpha + Y_\alpha - H_\alpha , X_\alpha + Y_\alpha + H_\alpha\)
    
    with eigenvalues \(0, 2i, -2i\).

    Textbook is wrong here. Source of problem is using the same notation for real coroot.

    \(Ad_x (H_\alpha)= \exp(\frac{\pi}{2}(ad_{X_\alpha}-ad_{Y_\alpha}))(\frac{1}{2}(X_\alpha + Y_\alpha + H_\alpha)-\frac{1}{2}(X_\alpha + Y_\alpha -H_\alpha))\)
    
    \(= \frac{1}{2}e^{-\pi i}(X_\alpha + Y_\alpha + H_\alpha)- \frac{1}{2}e^{-\pi i}(X_\alpha + Y_\alpha - H_\alpha )= -\frac{1}{2}(\cdots)+\frac{1}{2}(\cdots)=-H_\alpha\) 
\end{proof}

Centralizer of \(T\):

\(Z(T)=\{ x\in K : xt = tx  \text{ for all } t\in T \} \) 

\begin{theorem}
    If \(T\) is a max torus in \(K\) and \(K\) is connected then \(Z(T)=T\) 
\end{theorem}

\(N(T) / T \to O(\mathfrak{t})\) 

this theorem tells us that this map is 1-1.

\hrulefill

Class 27: 04/16

Recap:

\begin{theorem}
    [Torus Theorem] If \(K\) is connected compact (matrix) lie group,

    \begin{enumerate}
        \item If \(S\) and \(T\) are maximal tori in \(K\) there exists \(x\in K\) such that \(T = x S x ^{-1}\) 
        \item Every element of \(K\) is contained in some maximal torus.
    \end{enumerate}
\end{theorem}

\underline{Weyl Group} 

\(N(T)=\{ x\in K ; x T x ^{-1} \subset T \} \)

normalizer of \(T\) in \(K\) 

\(T\)-normal subgroup of \(N(T)\) 

\(W = N(T) / T\) acts on \(T\) by conjugation

Acts on \(\mathfrak{t}\) by \(\Ad\)

Centralizer of \(T\):

\(Z(T) = \{ x\in K ; xt = tx \forall t \in T \}\) 

\begin{theorem}
    If \(K\) is compact, connected, \(T\)-max torus in \(K\) then \(Z(T)=T\) 
\end{theorem}

This theorem follows from lemma with \(S = T\):

\underline{Lemma}: Suppose \(S\) is a connected commutative subgroup of \(K\). If \(x\in Z(S)\), then there exists max torus \(S^{\prime}\) containing \(S\) and \(x\) 

HW Review:

1: \(\mathfrak{g}\)-reductive, \underline{not} semisimple.

Want a representation \(\mathfrak{g}\) that is not completely reducible.

Consider first \(\mathfrak{g} = \mathbb{C} = (\operatorname{Lie}(S^1))_\mathbb{C}\) 

representation of \(\mathbb{C}\): \(V\), choice linear transformation \(A: V \to V\)

\(z_0\in\mathbb{C}\), \(z_0\neq 0\)  acts on \(V\) by \(A\)  

Then each \(z z_0\in\mathbb{C}\) acts on \(V\) by \(z A\) 

Take \(V = \mathbb{C}^2\) and \(A\) by matrix with nontrivial Jordan block, \(A = \begin{pmatrix}
    1 &  1 \\
    0 &  1 \\
\end{pmatrix}\).

\(z\in\mathbb{C}\) acts on \(\mathbb{C}^2\) by multiplication by \(\begin{pmatrix}
    z &  z \\
    0 &  z \\
\end{pmatrix}\)   

Since this is not diagonalizeable this representation is not completely reducible.

The only invariant \(1\)-dim subspace is \(\{ \binom{\ast}{0} \} \) 

This takes care of the case of one dimensional.

For general case, \(\mathfrak{g}\) is not semisimple so it must have non-trivial center.

So, \(\mathfrak{g} = \mathfrak{z} \oplus \mathfrak{g}_{s s} = \mathbb{C} \oplus \mathfrak{g}^{\prime}\) 

Note, \(\mathfrak{z}\) might have dimension \(> 1\). But \(\mathfrak{z}\) must be non-trivial, so we can take a one dimensional subspace out of \(\mathfrak{z}\)  

Now set \(\mathfrak{g}^{\prime}\) acts on \(\mathbb{C}^2\) by \(0\) 

10: Let \((\pi, V)\) be finite dimensional representation. Let \((\pi^{\ast} , V^{\ast} )\) be the dual.

Let \(\{ v_1,\cdots,v_k \} \)-basis of \(V\) consisting of weight vectors.

Let \(\{ v_1^{\ast} ,\cdots,v_n^{\ast} \} \) dual basis of \(V^{\ast}\) 

So \(v_i^{\ast} (v_j)=\delta_{ij}\)

If \(v_i\) has weight \(\lambda_i\) then \(v_i ^{\ast}\) has weight \(- \lambda_i\)

\((\pi^{\ast}(X)v^{\ast})(v)\overset{\operatorname{def}}{=} - v^{\ast}(\pi(X)v)\) 

Last part: \(V^{\ast} \simeq V\) under certain conditions.

\(V = \bigoplus V_i, V:\)-irreducible

\(V^{\ast} = \bigoplus V_i ^{\ast} = \bigoplus V_i\)

Inner products do not work well!

Inner product gives us \(V \simeq V^{\ast}\) by \(v \mapsto \langle v, - \rangle \) 

But this is not \underline{complex-linear}, it is \underline{conjugate-linear}!

Sometimes we can get away with conjugate linear maps. If we combine two conjugate linear maps then we get a general linear map.

Back to class:

\(N(T)\) is the normalizer of the maximal torus.

Since \(T\) is normal subgroup of \(N(T)\) we can construct \(W = N(T) / T\). We call this the weyl group.

We prove that this is the same as the Weyl group generated by reflection.

Note that \(W\) acts on \(T\) by conjugation and thus acts on \(\mathfrak{t}\) by \(\operatorname{Ad}\)  

Note that \(N(T) / T \to O(\mathfrak{t})\) 

We claim that the elements that cause the trivial transformations of \(\mathfrak{t}\) come from \(Z(T)\).

Once we establish \(Z(T)=T\) we know that \(N(T) / T \to O(\mathfrak{t})\) is one to one.

That would allow us to embed \(N(T) / T\) to \(O(\mathfrak{t})\) 

Torus theorem is key here.

For that reason, all results today requires us to have \(K\) connected compact.

We can construct counterexamples by product of connected compact and finite groups.

Recall:

\underline{Lemma}: Suppose \(S\) is a connected commutative subgroup of \(K\). If \(x\in Z(S)\), then there exists max torus \(S^{\prime}\) containing \(S\) and \(x\) 

\begin{proof}
    Let \(x\in Z(S), B \subset K\) be subgroup generated by \(S\) and \(x\).
    
    Let \(\overline{B} \subset K\) be closure of \(B\) in \(K\)
    
    Will show that there exists \(b\in \overline{B}\) such that the subgroup generated by \(b\) is dense in \(\overline{B}\).
    
    Then torus theorem \(\implies\) \(b\in S^1\)-max torus. Then
    
    \(S^{\prime} \supset \overline{B} \supset S,x \). This requires \(K\) to be connected.

    Now we prove that \(b\) exists.

    Let \(\overline{B}_0\) be the identity component of \(\overline{B}\)
    
    \(\overline{B} ,\overline{B}_0\) are both compact and commutative.
    
    Theorem 11.2 implies \(\overline{B}_0\) is a torus.
    
    \(\overline{B}\) has finitely many connected components
    
    Thus the quotient group \(\overline{B} / \overline{B}_0\) is finite
    
    Each \(y\in \overline{B}\) is limit of some sequence ofform \(x^{n_k}s_k\) where \(s_k\in S, n_k\in\mathbb{Z}\) 

    For \(k\) large enough \(x^{n_k}s_k\) will be in the same connected component of \(\overline{B}\)  as \(y\) 
    
    \(S\) connected means \(x^{n_k}\) is in the same connected component

    Conclude: coset \([y]=[x^{n_k}]\) in \(\overline{B} / \overline{B_0}\) are the same
    
    Thus, \([x]\) generates \(\overline{B} / \overline{B_0}\)
    
    Which means \(\overline{B} / \overline{B}_0\) is finite cyclic and thus \(\approx \mathbb{Z} / m \mathbb{Z}\)  

    Then \(x^m\in \overline{B}_0\)
    
    Choose element \(t\in \overline{B}_0\) such that the subgroup generated by \(t\) is dense in \(\overline{B}_0\)
    
    Such an element exists by proposition 11.4.

    Choose \(g\in \overline{B}_0\) so that \(g^m = x^{-m}t\) 
    
    We can choose since \(\exp\) map on  \(\overline{B}_0\) isonto.
    
    So, \(x^{-m}t=\exp(?)\)
    
    Take \(g = \exp (? / m)\) 

    Set \(b=gx\) so that \(b\) is in the same component of \(B\) as \(x\) 
    
    Since \(\overline{B}\) is commutative,
    
    \(b^m = g^m x^m = t\)
    
    elements of the form \(b^{nm}=t^n\) are dense in \(\overline{B}_0\)
    
    \(\overline{B} / \overline{B}_0\) ae cyclic with generator \([x]\) 
    
    Thus each component of \(\overline{B}\) can be realized as \(x^k \overline{B}_0\) for some power \(k\)
    
    Thus, elements of the form \(b^{nm+k}=x^k g^k t^n\) are in \(x^k \overline{B}_0\)  

    Conclude: subgroup generated by \(b\) is dense in \(\overline{B} \)  

\end{proof}

\underline{Lemma} Suppose \(C\) = fundamental Weyl Chamber relative to \(\Delta\)
    
Then \(C = \{ H\in \mathbb{R} \text{-span of } R; \langle \alpha , H \rangle > 0 \forall \alpha \in \Delta  \} \)

And suppose \(x\in N(T)\) such that \(\Ad_x (C) \subset C\). Then \(x\in T\) 

\begin{proof}
    The action of \(\Ad_x\) on \(\mathfrak{t}\) is trivial on the center of \(\mathfrak{k}\) and permutes the roots.
    
    Thus, \(\Ad_x\) as a linear transformation is uniquely determined by its effect on \(R\).

    Thus, the group of transformation generated by \(\Ad_x\) is finite.

    Pick any \(H_0\in C\)

    \(H = \frac{1}{\text{order of } \Ad_x}\sum_{k=1}^{\text{order of } \Ad_x} (\Ad_x)^k (H_0) \) 

    Then \(H\) is \(\Ad_x\) invariant

    Also, \(H\in C\) because it is convex.

    Then \(x\) commutes with every member of \(S = \{ \exp(tH); t\in\mathbb{R}\} \)

    Lemma 11.37 implies \(\exists\) max torus \(S^{\prime}\) containing \(x\) and \(S\) 
    
    if \(x \notin T, S^{\prime} \neq T\)
    
    So, \(\exists X\in \mathfrak{k}\) such that \(X\in \mathfrak{s}^{\prime}\), lie algebra of \(S^{\prime}\)   

    and \(X \notin \mathfrak{t}\) 

    We pause here. Continue Next lecture.

\end{proof}

\hrulefill

\underline{Lemma} Suppose \(C\) is the fundamental Weyl chamber with respect to \(\Delta\) 

\[
    C = \{ H\in \mathbb{R} \text{-span of } \mathbb{R} ; \langle \alpha , H \rangle > 0 \forall \alpha \in \Delta \} 
\]

And suppose \(x\in N(T)\) is such that \(\Ad_x(C) \subset C\)  

Then \(x\in T\) 

\begin{proof}
    Last time: Found and \(\Ad_x\) invariant element \(H\in C\) 

    Then \(x\) commute with every member of

    \(S = \{ \exp(tH) ; t\in \mathbb{R} \} \)
    
    By lemma 11.37, there exists max torus \(S^{\prime} \) containing \(x\) and \(S\) 

    If \(x\notin T, S^{\prime} \neq T, \exists X\in \mathfrak{k}\) such that \(X\in \mathfrak{s}^{\prime}\) - lie algebra of \(S^{\prime}\) and \(X\notin \mathfrak{t}\)
    
    Then \(X\) commutes with every element of \(S\) 

    Thus \(X\) commutes with \(H\) 

    \(\mathfrak{g} = \mathfrak{h} \oplus \bigoplus\) root spaces.
    
    Decompose \(X\in \mathfrak{k} \subset \mathfrak{g}\) accordingly.
    
    We have \([H,X] = 0\)
    
    \(\langle \alpha , H \rangle \neq 0 \forall \alpha \in R \)
    
    because \(H\in C\) 

    Thus, \(X\in \mathfrak{k} \cap \mathfrak{h} = \mathfrak{t} \)
    
    Contradicts \(X\notin \mathfrak{t}\)
    
\end{proof}

\begin{theorem}
    \(K\) -connected, the quotient group \(N(T) / \) acts on \(\mathfrak{t}\) effectively and this action is generated by the reflections \(s_\alpha , \alpha \in R\) 
    
    Thus the two descroptions of the Weyl group \(W\) lead to isomorphic groups.
\end{theorem}

\begin{proof}
    \(Z(T)=T\) implies:

    \(\Ad : N(T) / T \to O(\mathfrak{t})\) is 1-1.
    
    Then, we can think of \(N(T) / T\) as a subgroup of \(O(\mathfrak{t})\)
    
    Let \(W^{\prime} \subset N(T) / T\) be the subgroup generated by reflections \(s_\alpha\), \(\alpha \in R\)
    
    Let \(w\in N(T) / T, C\)-fundamental Weyl chamber
    
    \(w(C)\)- another weyl chamber

    by proposition 8.23 \(\exists w^{\prime} \in W^{\prime}\) such that \(w^{\prime} w(c)=C\)
    
    by lemma \(w^{\prime} w=1\)
    
    so \(w = (w^{\prime})^{-1} = W^{\prime}\)
    
    So \(W = W^{\prime}\) 
\end{proof}

\begin{theorem}
    If \(s,t\in T, x\in K\) such that \(t = x s x ^{-1}\) then \(\exists y\in N(T)\) such that \(t = y s y ^{-1}\)   
\end{theorem}

\underline{Corollary} if \(f\) is a cont Weyl-invariant function on \(T\) then \(f\) extends uniquely to a continuous class function on \(K\) 

\(K\)-connected

So,

\{continuous \(W\)-invariant functions on \(T\)\} = \{continuous class fuctions on \(K\) \}

for \(\leftarrow\)  restriction to \(T\) 

Gap in the book proof:

\(\{ x_n \} \to x\) 

\(\lim_{n \to \infty} F(x_n) \overset{?}{=} F(x)\) Does this limit exist???

book: each sequence \(\{ x_n \}\) has a subsequence \(x_{n_k}\) such that \(\lim_{k \to \infty} F(x_{n_k}) \to F(x)\)  

\(\lim_{n \to \infty} F(x_n)\) exists \(\iff \overline{\lim_{n \to \infty}} F(x_n) = \underline{\lim_{n \to \infty}} F(x_n) \)  

\section*{Bits of ch12, ch13}

\(K\)-compact lie group

\(T\)-max torus

\begin{definition}
    Let \(\Gamma \) be the subset of \(\mathfrak{t}\) given by \(\Gamma = \{ H \in \mathfrak{t} ; e^{2\pi H} = I \} \)
    
    the \underline{kernel of the exponential map} for \(\mathfrak{t}\)
\end{definition}

Recall real roots.

An element \(\alpha \in \mathfrak{t}\) is a \underline{real root} of \(\mathfrak{k}_\mathbb{C}\) with respect to \(\mathfrak{t}\) if \(\alpha \neq 0\) and \(\exists X\in \mathfrak{k}_\mathbb{C},X \neq 0\) such that \([H,X] = i \langle \alpha , H \rangle X \forall H\in \mathfrak{t} \)      

\underline{Real coroot}: \(H_\alpha = 2 \frac{\alpha}{\langle \alpha ,\alpha \rangle }\in \mathfrak{t}\) 

\begin{definition}
    The \underline{coroot lattice} denoted \(I\) is the set of all \(\mathbb{Z}\)-linear combination of real coroots \(H_\alpha, \in R\)   
\end{definition}

\(\Gamma\) is a lattice, and \(I\) is a sublattice.

\underline{Corollary 13.18} The fundamental group of \(K\) is isomorphic to the quotient group \(\Gamma / I\)

In particular, the fundamental group of \(K\) is always abelian.

\begin{definition}
    Let \((\Pi, V)\) be a finite dim representation of \(K\) and \(\pi\) be the associated representation of \(\mathfrak{k}_\mathbb{C}\). Element \(\lambda \in \mathfrak{t}\) is called a \underline{real weight} of \(V\) if \(\exists v\in V, v\neq 0\) such that \(\pi(H)v = i \langle \lambda , H \rangle v \forall H\in \mathfrak{t} \) \((\ast)\)
    
    The \underline{weight space} with weight \(\lambda\) is the set of all \(v\in V\) satisfying \((\ast)\) 
    
    and the multiplicity of \(\lambda\) is the dim of corresponding weight space. 
\end{definition}

\begin{proposition}
    If \((\Pi, V)\) is a finite-dim representation of \(K\) and \(V = \bigoplus\) weight spaces with real weights \(\lambda\)
    
    The real weights for \(\Pi\) and their multiplicites are invariant under the action of the Weyl group.
\end{proposition}

Recall: we hae two lattices.

\(I = \{ \mathbb{Z} \text{-comb of } H_\alpha \} \) 

and \(I \subset \Gamma =\) kernel of exp.

Introduce \underline{dual lattices}:

\(\Gamma ^\vee \subset I ^ \vee\) 

\begin{definition}
    An element \(\lambda \in \mathfrak{t}\) is \underline{analytically integral element} if \(\langle \lambda , H \rangle \in \mathbb{Z} \forall H\in \Gamma \) (dual to \(\Gamma\) )  

    AN element \(\lambda \in \mathfrak{t}\) is \underline{algebraically integral element} if \(\langle \lambda , H \rangle = 2 \frac{\langle \lambda , \alpha \rangle }{\langle \alpha , \alpha \rangle } \in \mathbb{Z} \) for all real roots \(\alpha\) (dual to \(I\) ) 
    
    An element \(\lambda \in \mathfrak{t}\) is \underline{dominant} if \(\langle \lambda , \alpha \rangle \geq 0 \forall \alpha \in \Delta\).
    
    Write \(\Delta = \{ \alpha_1, \cdots, \alpha_r \} \) and let \(\lambda , \mu \in \mathfrak{t}\). We say that \underline{\(\mu\) is higher than \(\lambda\)} if \(\mu - \lambda = c_1 \alpha_1 + \cdots + c_r \alpha_r\) and each \(c_j \geq 0\)
    
    We write \(\mu \succeq \lambda\) and \(\lambda \preceq \mu\) 
\end{definition}

\begin{proposition}
    [12.5] Let \((\Sigma , V)\) be a representation of \(K\) and let \(\sigma\) be the representation of \(\mathfrak{k}\). If \(\lambda \in \mathfrak{t}\) is a real weight of \(\sigma\) then \(\lambda\) is an analytically integral element.      
\end{proposition}

\begin{proof}
    If \(v\) is a weight vector with weight \(\lambda\) and \(H \in \Gamma, \Sigma(e^{2\pi H})v = \Sigma (I)v = v\)
    
    \(\Sigma (e^{2\pi H})=e^{2\pi \sigma(H)}v = e^{2\pi i \langle \lambda , H \rangle }v\) 

    So \(e^{2 \pi i \langle \lambda , H \rangle } = 1\) and \(\langle \lambda , H \rangle \in \mathbb{Z}\)  
\end{proof}

\begin{proposition}
    [12.7]

    i: The lattice of analytically integral elements is invariant under the action of the Weyl group.

    ii: Every analytically integral element is algebraic integral.

    iii: Every real root is analytically integral
\end{proposition}

\begin{theorem}
    (12.6) (Theorem of highest weight)

    If \(K\) is connected compact (matrix) lie group and \(T\) is a max torus in \(K\) then:
    
    i: Every irreducible representation of \(K\) has a highest weight.

    ii: Two irreducible representation of \(K\) with the same highest weight are isomorphic

    iii: The highest weight of each irreducible representation of \(K\) is dominant and analtically integral.

    iv: Of \(\mu\) is dominant analytic integral element there exists an irreducible representation of \(K\) with highest weight \(\mu\) 
\end{theorem}

\underline{Corollary} If \(K\)-connected comp Lie group such that \(\mathfrak{k}\) is trivial (so \(\mathfrak{g} = \mathfrak{k}_\mathbb{C}\) is semisimple) \(T\)-max torus, then a finite dim representation \((\pi,V)\) of \(\mathfrak{k}_\mathbb{C}\) lift to a rep of \(K\) if and only if all weights of \(V\) are analytically integral.

\(K\) is simply connected if and only if \(\Gamma = I\) if and only if \(\Gamma ^\vee = I ^ \vee\)

If and only if each algebriacally integral element is analytically integral.

\hrulefill

Class 29: 04/23

Classifying all irreducible representation of \(\mathfrak{sl}(2,\mathbb{C})\) 

Let \(\mathfrak{g} = \mathfrak{sl} (2,\mathbb{C})\) 

\(H = \begin{pmatrix}
    1 &  0 \\
    0 &  -1 \\
\end{pmatrix}, E = \begin{pmatrix}
    0 &  1 \\
    0 &  0 \\
\end{pmatrix}, F = \begin{pmatrix}
    0 &  0 \\
    1 &  0 \\
\end{pmatrix}\)

\([H,E]=2E, [H,F]=-2F, [E,F]=H\) 

Goal: classify all irreducible representation \((\pi, V)\) of \(\mathfrak{sl}(2,\mathbb{C})\) [including \(\infty\)-dim] subject to the following assumption:

There exists \(\lambda \in \mathbb{C}\) such that the \(\lambda\)-eigenspace for \(H\) in \(V\) has \(\dim 1\)

\(\pi(H)v_0 = \lambda v_0\) for some \(v_0 \in V, v_0 \neq 0, \lambda \in \mathbb{C}\)

and \(\forall v\in V, \pi(H)v = \lambda v\) means \(v\) is a scalar multiple of \(v_0\) 

\underline{Lemma}: Let \(v\in V\) be an eigenvector for \(H\) with eigenvalue \(\lambda\).

Then, \(\pi(E)v = \in V\) is \(0\) or egv with egv \(\lambda + 2\)

\(\pi(F)v\in V\) is \(0\) or egv with egv \(\lambda - 2\)

\(v_0\) egv with egv \(\lambda\)

Define \(v_k = (\pi(E))^k v_0, k = 1,2,3,\cdots\)

\(v_{-k}=(\pi(F))^k v_0\)

\underline{Corollary}: Each \(v_k\in V\) is either \(0\) or eigenvector with eigenvalue \(\lambda + 2k\) 

Let \(\Omega\) be the casimir element.

\(\Omega = H^2 + 2EF + 2FE \in \mathcal{Z}(\mathcal{U}(\mathfrak{sl}(2,\mathbb{C})))\) - center of univ. enveloping algeba.

\underline{Lemma}: Let \(v_0\in V\) be an eigenvector for \(H\) with eigenvalue \(\lambda\) such that \(\dim \ker (\pi(H)-\lambda)=1\).

Then \(\pi(\Omega)v_0 = \mu v_0\) for some \(\mu \in \mathbb{C}\)  

\begin{proof}
    \(\pi(\Omega)v_0 = \pi(H)^2 v_0 + 2\pi(E)\pi(F)v_0 + 2\pi(F)\pi(E)v_0\) 

    Note that \(E\) shifts up by \(2\) and \(F\) shifts down by \(2\)

    So, the two second terms have eigenvalue \(\lambda\) or are \(0\) 
    
    \(\pi(H)^2 v_0 = \lambda^2 v_0\)

    All must be proportional to \(v_0\) 

    So, it is indeed \(\mu v_0\) 
    
\end{proof}

\underline{Corollary}: Under the assumption of the lemma, \(\pi(\Omega)v_k = \mu v_k \forall k\in \mathbb{Z}\) 

\begin{proof}
    \(\pi(\Omega)v_k = \pi(\Omega)\pi(E)^k v_0\) if \(k \geq 0\)
    
    \(= \pi(E)^k \pi(\Omega)v_0\)
    
    \(= \mu \pi(E)^k v_0 = \mu v_k\) 

    Same for \(k \leq 0\) 
\end{proof}

Wants to show: \(V = \bigoplus_{k\in\mathbb{Z}}\mathbb{C} v_k\) 

Since \(V\) is irreducible it is sufficient to show:

\(V_0 \overset{def}{=} \bigoplus_{k\in\mathbb{Z}} \mathbb{C} v_k\) is \(\mathfrak{sl}(2,\mathbb{C})\) invariant

Obvious: \(V_0\) is \(\pi(H)\)-invariant.

\underline{Lemma}: Let \(v_0\) be an eigenvector for \(H\) with eigenvalue \(\lambda\).

Define \(V_0 = \bigoplus_{k\in\mathbb{Z}}\mathbb{C} v_k, supp \pi(\Omega)v_0 = \mu v_0\) for some \(\mu \in \mathbb{C} \)

Then the set of non-zero \(v_k\)'s form a basis for \(V_0\) with the following relations:

\(\pi(H)v_k = (\lambda + 2k)v_k\), \(k\in\mathbb{Z}\) 

\(\pi(E)v_k = v_{k+1}\) if \(k \geq 0\)

\(\pi(F)v_k = v_{k-1}\) if \(k \leq 0\)  

\(\pi(E)v_k = \frac{1}{4} \left( \mu - (\lambda + 2k + 2)^2 + 2(\lambda + 2k + 2) \right) v_{k+1}, k < 0 \) 

\(\pi(F)v_k = \frac{1}{4} \left( \mu - (\lambda + 2k - 2)^2 - 2(\lambda + 2k - 2) \right) v_{k-1} \) if \(k>0\) 

In part, \(V_0\) is \(\mathfrak{sl}(2,\mathbb{C})\)-invariant and all non-trivial \(H\)-eigenspaces of \(V_0\) are \(1\)-dim

\begin{proof}
    \(\pi(\Omega)v = \mu v \, \forall v\in V_0\) 

    \(\Omega = H^2 + 2EF + 2FE = H^2 + 2H + 4FE = H^2 - 2H + 4EF\) 

    Suppose \(u_\nu \in V_0\) is such that \(\pi(H) u_\nu = \nu u_\nu\)
    
    \(\pi (E) \pi (F) u_\nu = \frac{1}{4} (\pi(\Omega) - \pi(H)^2 + 2\pi(H))u_\nu = \frac{1}{4}(\mu - \nu ^2 + 2\nu)\nu\)
    
    \(\pi(F)\pi(E)u_\nu = \frac{1}{4}(\pi(\Omega)-\pi(H)^2 - 2\pi(H))u_\nu = \frac{1}{4}(\mu - \nu^2 - 2\nu )u_\nu\)
    
    If \(k < 0\) take \(u_\nu = v_{k+1}, \nu = \lambda + 2k + 2\)
    
    If \(k > 0\) take \(u_\nu = v_{k-1}, \nu = \lambda + 2k - 2\)

\end{proof}

\underline{Corollary}: Let \(V\) be an irreducible \(\mathfrak{sl}(2,\mathbb{C})\)-mod. Then TFAE:

i: \(\exists \lambda \in \mathbb{C}\) such that \(\dim \ker (\pi(H)-\lambda)=1\)

ii: \(\exists v_0\in V, v_0\neq 0\) such that \(\pi(H)v_0 = \lambda v_0, \pi(\Omega)v_0 = \mu v_0\) for some \(\lambda , \mu \in \mathbb{C}\)

\begin{proposition}
    Let \(V\) be an irreducible \(\mathfrak{sl}(2,\mathbb{C})\)-mod such that one of the two equivalent condition of the above corollary is satisfied. Then \(V = \bigoplus\) weight spaces. All weight spaces of \(V\) are \(1\)-dim. The weights of \(V\) are of the form \(\lambda + 2k\) with \(k\) ranging over `an interval of integers'
    
    \(\mathbb{Z} \cap [a,b], \mathbb{Z}\cap [a,\infty), \mathbb{Z} \cap (-\infty, b], \mathbb{Z}\)
    
    Moreover, \(\exists \mu \in \mathbb{C}\) such that \(\pi(\Omega)v = \mu v\) for all \(v\in V\)   
\end{proposition}

\underline{Classification of \(\mathfrak{sl}(2,\mathbb{C})\)-modules}

Start with the \(\mathbb{Z} \cap [a,b]\) case.

\begin{proposition}
    Let \(V\) be an irreducible \(\mathfrak{sl}(2,\mathbb{C})\)-mod of dimension \(d+1\).

    Then \(V\) has basis \(\{ v_0, \cdots, v_d \} \) such that \(\pi(H)v_k = (-d + 2k)v_k, 0 \leq k \leq d\)
    
    \(\pi (E)v_k = v_{k+1}, 0 \leq k < d, \pi (E)v_d = 0\)
    
    \(\pi(F)v_k = k(d+1-k)v_{k-1}, 0 < k \leq d, \pi(F)v_0 = 0\)
    
    \(\pi(\Omega)v = \underbrace{d(d+2)}_{=\mu}v, \forall v\in V\)
    
    And \(V\) is determined upto isom by its dimension.

\end{proposition}

\begin{proof}
    \(\pi(\Omega)\) commutes with all \(\pi(X), X\in \mathfrak{sl}(2,\mathbb{C})\)
    
    Schur's lemma implies \(\pi(\Omega)\) acts by multiplication by a scalar \(\mu\in \mathbb{C}\)
    
    Note here we use finite dimensionality, otherwise Schur's lemma doesn't apply.

    In infinite dimension, we instead might use Dixmier's lemma.

    \(\pi(H)\) has at least one eigenvalue (because \(V\) is finite-dimensional).
    
    Let \(\lambda \in \mathbb{C}\) be an eigenvalue for \(\pi(H)\) with the least real part. Let \(v_0\) be an eigenvector. \(v_0 \neq 0, \pi(H)v_0 = \lambda v_0,\pi(F)v_0 = 0\).

    Let \(v_k = (\pi(E))^k v_0, k=1,2,3,\cdots\) form a basis for \(V\) 

    \(\dim V = d+1\), so \(\{ v_0, v_1,\cdots,v_d \} \) is a basis for \(V\) such that \(\pi (E)v_k = v_{k+1}, 0 \leq k < d\) and \(\pi(E)v_d = 0\)    

    \(\pi(H)v_k = (\lambda + 2k)v_k\), \(0 \leq k \leq d\) 
    
    \(\pi(F)v_0 = 0\)
    
    \(\mu v_0 = \pi(\Omega)v_0 = (\pi(H)^2 - 2\pi(H)+4\pi(E)\pi(F)v_0)=(\lambda^2 - 2\lambda)v_0 = \lambda(\lambda - 2)v_0\)
    
    \(\implies \mu = \lambda (\lambda -2)\)
    
    \(\pi(E)v_0 = 0 \implies \)
    
    \(\mu v_d = \pi(\Omega)v_d = (\pi(H)^2 + 2\pi(H) + 4\pi(F)\pi (E))v_d = ((\lambda+2d)^2 + 2(\lambda + 2d))v_d = (\lambda + 2d)(\lambda + 2d + 2)v_d\)
    
    \(\mu = \lambda (\lambda -2) = (\lambda + 2d)(\lambda + 2d + 2)\)
    
    \(\implies \lambda =-d, \mu = d(d+2)\)
    
    \(\pi(F)v_k = k(d+1-k)v_{k-1}, 0 < k \leq d\)
    
    follows from previous lemma.

    \(\frac{1}{4}(\mu - (\lambda + 2k - 2)^2 - 2(\lambda + 2k -2))\)
    
    indecomposable means not writable as \(\oplus\) two subrepresentations.
    
    Every finite-dim indecomposible representation of \(\mathfrak{sl}(2,\mathbb{C})\) is irreducible.
    
    

\end{proof}

\hrulefill

Class 30: 04/25

Last time:

An irreducible representation \((\pi, V)\) of \(\mathfrak{sl}(2,\mathbb{C})\) has weights \(\lambda + 2k\);

\(k \in \mathbb{Z} \cap [a,b]\) [finite dim V]

\(\begin{rcases}
    k\in \mathbb{Z} \cap [a,\infty), &\text{lowest weight mods}  \\
    k\in \mathbb{Z} \cap (-\infty, b], &\text{highest weight mods} 
\end{rcases} \text{Verma mods} \) 


\(k\in\mathbb{Z}\) irreducible principal series modules

\begin{proposition}
    Let \(V\) be an irreducible \(\infty\)-dim lowest weight \(\mathfrak{sl}(2,\mathbb{C})\) - mod.
    
    Then \(V\) has a basis of \(H\)-eigenvectors \(\{ v_0, v_1, v_2, \cdots \} \) and a \(\lambda \in \mathbb{C}, \lambda \neq 0, -1, -2, -3, \cdots\) such that:
    
    \(\pi(H)v_k = (\lambda + 2k)v_k \quad k \geq 0\)
    
    \(\pi(E) v_k = v_{k+1} \quad k \geq 0\)
    
    \(\pi(F)v_k = -k(\lambda + k - 1)v_{k-1} \quad k > 0\) 

    \(\pi (F) v_0 = 0\)
    
    Moreover, \(\pi(\Omega) = \lambda (\lambda - 2)v \forall v\in V\) and \(V\) is determined upto isomorphism by its lowest weight \(\lambda\)  
\end{proposition}

\begin{proof}
    Let \(\lambda \in \mathbb{C}\) be an eigenvalue for \(\pi(H)\) with the least real part, \(v_0 \in V, v_0 \neq 0\) corresponding eigenvector.
    
    Then \(\pi(F)v_0\)
    
    Define \(v_k = (\pi(E))^k v_0\) for \(k = 1,2,3,\cdots\)
    
    \(\{ v_0, v_1, v_2, \cdots \} \) - span invariant subspace of \(V\) 
    
    Since \(V\) is irreducible, they span \(V\) 

    different eigenvalues implies they are linearly independent.

    So, \(\{ v_0, v_1, v_2, \cdots \} \) is a basis.

    \(\pi(\Omega) v_0 = (\pi(H)^2 - 2\pi(H) + 4\pi(E)\pi(F))v_0 = (\lambda^2 - 2\lambda)v_0 = \underbrace{\lambda (\lambda - 2)}_{=\mu}v_0\)
    
    \(\pi(F)v_k = \frac{1}{4}(\mu - (\lambda + 2l - 2)^2 - 2(\lambda + 2k - 2)) = \cdots = -k (\lambda + k - 1)\) 

    Remains to show: \(\lambda \neq 0, -1, -2, \cdots\)
    
    If \(\lambda \in\mathbb{Z} , \lambda \leq 0\) then \(\lambda (F) v_k = 0\) for \(k = -\lambda + 1\) 
    
    Since if we write \(v_0, v_1, \cdots\) in a line, \(E\) jumps from \(v_j\) to \(v_{j+1} \) and \(F\) jumps from \(v_j\) to \(v_{j-1}\) but jumps to \(0\) for \(j = k\) 

    Then \(v_k, v_{k+1}, v_{k+1}, \cdots  \) span \(\mathfrak{sl}(2,\mathbb{C})\) invariant subspace which gives us a contradiction.  

    Note that \(\pi(H)\)-invariant subspace is a span of some \(v_k\)'s. 
\end{proof}

\begin{proposition}
    Let \(V\) be an irreducible \(\infty\)-dim highest weight \(\mathfrak{sl}(2,\mathbb{C})\)-module. Then \(V\) has a basis of \(H\)-eigenvectors:
    
    \(\{ \overline{v_0}, \overline{v_1}, \overline{v_2},\cdots \} \) and a \(\lambda \in \mathbb{C} , \lambda \neq 0,1,2,3,\cdots\) such that:

    \(\pi(H)\overline{v_k} = (\lambda - 2k)\overline{v_k}\) for \(k \geq 0\)   

    \(\pi(F)\overline{v_k} = \overline{v_{k+1}} \) for \(k \geq 0\) 
    
    \(\pi(E)\overline{v_k} = k(\lambda - k + 1)\overline{v_{k-1}} \) for \(k > 0\) and \(\pi(E)\overline{v_0} = 0\) 
\end{proposition}

\begin{proof}
    Essentially \(\overline{V_k} = V_{-k}\) so argument is the same.
    
    \(\pi(\Omega)v = \lambda ( \lambda + 2)v, \forall v\in V\)
    
    \(V\) is determined upto isomorphism by its highest weight \(\lambda\) 
\end{proof}

\begin{proposition}
    Let \(V\) be an irreducible \(\infty\)-dim rep of \(\mathfrak{sl}(2,\mathbb{C})\) that is neither highest nor lowest weight module. Let \(v_0\in V\) be an eigenvector for \(\pi(H)\) with eigenvalue \(\lambda\), then \(\exists \mu \in \mathbb{C}\) such that \(\pi(\Omega)v = \mu v \forall v\in V\) and V  as a basis of \(H\)-eigenvctors \(\{ \cdots, v_{-2}, v_{-1}, v_0, v_1, v_2,\cdots \} \) such that:
    
    \(\pi(H)v_k = (\lambda + 2k)v_k\)
    
    \(\pi(E)v_k = v_k\) if \(k \geq 0\) 

    \(\pi(F)v_k = v_{k-1}\) if \(k \leq 0\)
    
    \(\pi(E)v_k = \frac{1}{4}(\mu - (\lambda + 2k + 1)^2 + 1)v_{k+1}\) if \(k < 0\)
    
    \(\pi(F)v_k = \frac{1}{4}(\mu - (\lambda + 2k -1)^2 + 1)v_{k-1}\) if \(k > 0\) 
    
    The constants \(\lambda , \mu \in \mathbb{C}\) are subject to the constraint:
    
    \(\lambda \pm \sqrt{\mu + 1} \neq\) odd integer

    The constraint comes from the fact that we don't want the weird coefficients to be \(0\)
    
    Denote such \(\mathfrak{sl}(2,\mathbb{C})\)-mod \(P(\lambda , \mu )\). Then,
    
    \(P(\lambda , \mu) \simeq P(\lambda ^{\prime} , \mu ^{\prime} )\) if and only if:
    
    \(\mu = \mu ^{\prime} \) and \(\lambda^{\prime}  = \lambda + 2k\) for some \(k\in \mathbb{Z}\)   
\end{proposition}

\begin{proof}
    Note: \(V\)-irreducible if and only if:

    \(\mu - (\lambda + 2k- 1)^2 + 1\neq 0\) and \(\mu - (\lambda + 2k + 1)^2 +1 \neq 0\)
    
    Which gives us the constraint.
    
    If one of them are zero, then we get invariant subspace which contradicts irreducibility.

    And check everything else as well, like \([\pi(E), \pi(F)] \overset{?}{=} \pi(H)\)
\end{proof}

So we finally have all irreducible reps of \(\mathfrak{sl}(2,\mathbb{C})\) 

\section*{Dixmier's Lemma}

Schur's Lemma is heavily used in representation theory, but it relies on the vector space being finite dimensional, since we need eigenvalues and eigenvectors.

When we deal with infinite dimensional space and want to use Schur's Lemma, we have an alternative called Dixmier's Lemma.

Drawback is: Vector space needs to have countable dimension.

Lecture note ~ Page 100:

\underline{Lemma}: Let \(V\) be a countable-dimensional vector space \(/ \mathbb{C}\). If \(T\in \operatorname{End}(V)\) then \(\exists c\in \mathbb{C}\) such that \(T-c \operatorname{Id}_V\) is not invertible on \(V\).   

\begin{proof}
    Suppose such a \(c\) doesn't exist.

    Then \(T - cI\) is invertible for all \(c\in\mathbb{C}\).
    
    Then polynomial \(P(T)\)-invertible for all polynomials \(P\in\mathbb{C} [x], P\neq 0\) by FTA.
    
    If \(R = P / Q\) rational function with \(P,Q\in\mathbb{C}[x]\) we can define \(R(T) = P(T) \cdot (Q(T))^{-1}\) 

    This rule defines a map \(\mathbb{C}(x) \to \operatorname{End}(V)\)
    
    If \(v\in V, v\neq 0\) and \(R\in\mathbb{C}(x), R\neq 0\) then \(R(T)v \neq 0\)
    
    Thus, the map \(\underbrace{\mathbb{C}(x)}_{\text{Uncountable dim}} \to \underbrace{V}_{\text{Countable dim}}\) given by \(R \mapsto R(T)v\) is 1-1.
    
    Which is a contradiction.
    
\end{proof}

\underline{Example}:

Let \(V = \bigoplus \mathbb{C} e_n\)

Where \(\{ e_n \}_{n\in\mathbb{Z}}\) is basis. Direct sum of one dimensional subspace.

We are allowed to have only finitely many nonzero elements.

Define \(T:V \to V\) to be the shift operator: \(T(e_j) = e_{j+1}\) 

Then \(T - cI\) is \underline{not invertible} for all \(c\neq 0\) in \(\mathbb{C}\).

Note that \(\ker(T-cI) = 0\) so there are no eigenvectors. 

\(\operatorname{Im}(T - cI) \subsetneq V\) for all \(c\in \mathbb{C} ^\times \) 

For example, for \(c = 1\) we have:

\(\operatorname{Im}(T - I) = \{ v = \sum_{n} a_n v_n ; \sum_{n} a_n = 0 \} \) 

\begin{definition}
    Let \(V\) be a vector space over \(\mathbb{C}\) and \(S \subset \operatorname{End}(V)\) a subset.
    
    Then \(S\) acts \underline{irreducibly} on \(V\) if whenever \(W \subset V\) is a subspace such that \(SW \subset W\) then \(W = V\) or \(W = \{ 0 \}\)   
\end{definition}

\underline{Dixmier's Lemma}: Suppose \(V\) is a vector space \underline{over \(\mathbb{C}\)} of \underline{countable dimensions} and that \(S \subset \operatorname{End}(V)\) acts irreducibly. If \(T \subset \operatorname{End}(V)\) commutes with every element of \(S\), then \(T\) is a scalar multiple of the identity operator.

\begin{proof}
    \(\exists c\in\mathbb{C}\) such that \(T - cI\) is not invertible.
    
    Observe that \(\ker (T - cI)\) and \(\operatorname{Im}(T - cI) \) are \(S\)-invariant.

    Thus, each of these are \(\{ 0 \}\) or \(\{ V \}\) by irreducibility.
    
    If \(\ker = 0\) and \(\operatorname{Im}\) is \(V\) then \(T - cI\) is invertible. But this is impossible.
    
    Therefore, we must have \(\ker = V\). So, \(T - cI\) is identically \(0\) 
    
    Therefore, \(T = c \operatorname{Id}_v\) 
\end{proof}

\end{document}